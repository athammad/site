{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Machine Learning, Copula and Synthetic Data\"\n",
        "image: https://goodonyou.eco/wp-content/uploads/2019/01/SyntheticFabrics-1200x630.jpg\n",
        "code-fold: false\n",
        "execute:\n",
        "  eval: true\n",
        "fontsize: 1.5em\n",
        "---"
      ],
      "id": "1d0241ec"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: justify\n",
        "Copulas and synthetic data play pivotal roles in statistical modeling, offering innovative solutions for various challenges in Machine Learning. Here, I will focus into the use of copulas for synthetic data generation.\n",
        "\n",
        "Copulas are mathematical constructs used to model the dependence structure between random variables. Unlike traditional correlation measures, copulas separate the marginal distributions from the dependence structure, providing a more flexible and nuanced approach to capturing complex relationships. They are particularly useful in scenarios where traditional models might fail to capture the intricate dependencies between variables. In the specific case of syntetic data generation, what we need is to mimics the **statistical properties** of real-world data. But why do we need this \"fake\" data in the first place? Syntetic data are invaluable in scenarios where obtaining sufficient real data is challenging (small sample size) or when privacy concerns limit access to actual data. By creating synthetic datasets we can augment the available data, **facilitating better model generalization and robustness**.\n",
        "\n",
        "Going back on the **statistical properties** we mentioned earlier, we are interested in the parameters governing the distribution of each variable separately (the marginals) and the dependency structure between them (the copula). Once these are known, we can generate new data from the same distribution and with the same correlation.\n",
        "\n",
        "To give a simple example let's take few variables from the classic Cars Dataset.\n"
      ],
      "id": "d63733b6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from copulas.multivariate import GaussianMultivariate\n",
        "from copulas.univariate import ParametricType, Univariate\n",
        "\n",
        "df = sns.load_dataset(\"mpg\")\n",
        "df=df.drop(columns=['origin', 'name'])\n",
        "df=df.dropna()\n",
        "df.columns\n",
        "\n",
        "df=df[['horsepower', 'weight','acceleration','mpg']]\n",
        "df.describe()"
      ],
      "id": "dfcd457c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot the kernel density distribution of 3 variables, the scatter plot of each pair and the corresponding correlation.\n"
      ],
      "id": "c0792a5f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def corrdot(*args, **kwargs):\n",
        "    corr_r = args[0].corr(args[1], 'pearson')\n",
        "    corr_text = f\"{corr_r:2.2f}\".replace(\"0.\", \".\")\n",
        "    ax = plt.gca()\n",
        "    ax.set_axis_off()\n",
        "    marker_size = abs(corr_r) * 1000\n",
        "    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap=\"coolwarm\",\n",
        "               vmin=-1, vmax=1, transform=ax.transAxes)\n",
        "    font_size = abs(corr_r) * 10 + 5\n",
        "    ax.annotate(corr_text, [.5, .5,],  xycoords=\"axes fraction\",\n",
        "                ha='center', va='center', fontsize=font_size)\n",
        "\n",
        "sns.set(style='white', font_scale=1)\n",
        "g = sns.PairGrid(df[['horsepower', 'weight','acceleration']], aspect=1, diag_sharey=False)\n",
        "g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})\n",
        "g.map_diag(sns.distplot, kde_kws={'color': 'black'})\n",
        "g.map_upper(corrdot)\n",
        "plt.show()"
      ],
      "id": "c915b3a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see all sorts of things here. Aside from the strong correlation among some of the variables, we see that they have different distribution. For example, acceleration is very normal distributed but the same cannot be said about the other two variables.\n",
        "\n",
        "Before anything else, let's try a simple model to predict mpg.\n"
      ],
      "id": "bff78020"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y = df.pop('mpg')\n",
        "X = df\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,random_state=42)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(model.score(X_test, y_test))"
      ],
      "id": "38eecb11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, can we simulate something so similar to the actual data that we would get the same score? Yes, we can thanks to copulas!!! We ca generate a synthetic dataset with the same underlying structure.\n"
      ],
      "id": "d1b00597"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Select the best PARAMETRIC univariate (no KDE)\n",
        "univariate = Univariate(parametric=ParametricType.PARAMETRIC)\n",
        "\n",
        "\n",
        "def create_synthetic(X, y):\n",
        "    \"\"\"\n",
        "    This function combines X and y into a single dataset D, models it\n",
        "    using a Gaussian copula, and generates a synthetic dataset S. It\n",
        "    returns the new, synthetic versions of X and y.\n",
        "    \"\"\"\n",
        "    dataset = np.concatenate([X, np.expand_dims(y, 1)], axis=1)\n",
        "\n",
        "    distribs =  GaussianMultivariate(distribution=univariate)\n",
        "    distribs.fit(dataset)\n",
        "\n",
        "    synthetic = distribs.sample(len(dataset))\n",
        "\n",
        "    X = synthetic.values[:, :-1]\n",
        "    y = synthetic.values[:, -1]\n",
        "\n",
        "    return X, y, distribs\n",
        "\n",
        "X_synthetic, y_synthetic, dist= create_synthetic(X_train, y_train)"
      ],
      "id": "d3cf2e5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the individual distributions fitted by the algorithm.\n"
      ],
      "id": "6f0ae054"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "parameters = dist.to_dict()\n",
        "parameters['univariates']"
      ],
      "id": "9175adca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the distributions (Gamma and Beta), and their corresponding parameters like location and scale. We can also take a look at the correlation that defines the join distribution.\n"
      ],
      "id": "a55e6af8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "parameters['correlation']"
      ],
      "id": "658c5992",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now it is time to look at all the synthetic variables and compare them with the original one. Let's look at the same things. A summary of the dataset and the plot of the 3 variables.\n"
      ],
      "id": "0e055761"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "syntDF=pd.DataFrame(np.concatenate([X_synthetic, np.expand_dims(y_synthetic, 1)], axis=1),columns=['horsepower', 'weight', 'acceleration','mpg'])\n",
        "\n",
        "syntDF.describe()"
      ],
      "id": "0227ef7a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The descriptive statistics are remarkably similar, reflecting the statistical properties emphasized earlier. This holds significant importance in our analysis. However, a closer examination of individual variable distributions and their correlations reveals disparities. The Kernel distributions have evidently undergone changes, and although the correlation values remain within the same magnitude range and exhibit the same sign, they are not identical.\n"
      ],
      "id": "785e09e2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "g = sns.PairGrid(syntDF[['horsepower', 'weight','acceleration']], aspect=1, diag_sharey=False)\n",
        "g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})\n",
        "g.map_diag(sns.distplot, kde_kws={'color': 'black'})\n",
        "g.map_upper(corrdot)\n",
        "plt.show()"
      ],
      "id": "d3412f31",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have seen similarities and differences, let's try to run the same simple linear model on the synthetic data.\n"
      ],
      "id": "168e3efa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = LinearRegression()\n",
        "model.fit(X_synthetic, y_synthetic)\n",
        "\n",
        "print(model.score(X_test, y_test))"
      ],
      "id": "aef34b9b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Upon scrutinizing the results, it is evident that they are highly comparable, even with the constraint of limiting the possible distributions to the simplest univariate forms and utilizing only three variables. This implies that our Gaussian copula has effectively captured the critical statistical characteristics of the dataset essential for addressing the regression problem.\n",
        ":::"
      ],
      "id": "9bac8920"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}