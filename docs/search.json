[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "üõ∞ Research",
    "section": "",
    "text": "The majority of my current research revolves around the nexus between causal inference, machine learning and environmental analysis. In the last few years my focus has been on topics such as climate change and extreme events but I had the chance to explore other avenues such as optimal allocation of health care facilities and transport networks analysis.\nMy interest is in exploring the interplay between causal inference, machine learning, and environmental analysis. The urgency of addressing environmental challenges necessitates innovative approaches to understanding the complex relationships between human activities, environmental factors, and their consequences. By combining techniques from causal inference and machine learning, I aim to develop novel methodologies that can effectively analyse environmental data, identify causal relationships, and inform evidence-based decision-making for sustainable environmental management.\n\n\n\n\nThe advent of big data and the increasing availability of diverse environmental datasets have provided researchers with an unprecedented opportunity to investigate complex environmental systems. However, extracting meaningful insights from these datasets poses significant challenges due to confounding factors, selection biases, and non-randomness inherent in observational data. Traditional statistical methods often fall short in capturing the complex and non-linear relationships within environmental systems. Therefore, there is a pressing need to integrate causal inference and machine learning techniques to overcome these limitations and achieve a deeper understanding of environmental dynamics.\n\n\n\n\n\nMachine learning algorithms have shown remarkable success in pattern recognition, predictive modelling, and data-driven decision-making. The ability to automatically extract complex patterns and relationships from large-scale environmental datasets makes machine learning an invaluable tool for environmental analysis. By leveraging machine learning techniques, develop predictive models to forecast environmental outcomes and assess the potential impacts of policy interventions.. Furthermore, machine learning methods can aid in feature selection, data imputation, and anomaly detection, contributing to improved data quality and reliability for environmental analysis.\n\n\n\n\n\nCausal inference provides a powerful framework for identifying cause-and-effect relationships within complex systems. By applying causal inference methods, we can go beyond correlations and uncover the underlying causal mechanisms driving observed environmental phenomena. Approaches such as propensity score matching, instrumental variables, and difference-in-differences analysis allow us to address confounding biases and estimate causal effects more accurately. Incorporating these causal inference techniques into environmental analysis can enhance our ability to discern the impacts of specific environmental factors and human activities on various ecological systems.\n\n\n\n\n\nIntegrating causal inference with machine learning techniques presents exciting opportunities for advancing environmental analysis. By combining the strengths of both fields, we can develop robust methodologies that provide accurate causal estimates while harnessing the predictive power of machine learning algorithms. However, several challenges need to be addressed to achieve this integration successfully. These challenges include the treatment of unmeasured confounding, scalability of methods for large environmental datasets, interpretability of complex machine learning models, and the consideration of uncertainty in causal inference.\n\n\n\n\n\nIn my research, I aim to address these challenges and contribute to the nexus between causal inference, machine learning, and environmental analysis. Specifically, I intend to:\n\nDevelop novel methodologies that integrate causal inference and machine learning techniques to estimate causal effects and predict environmental outcomes accurately.\nInvestigate the use of deep learning architectures and explainable AI methods to enhance the interpretability of complex machine learning models for environmental analysis.\nExplore scalable algorithms and parallel computing techniques to handle the computational demands of large environmental datasets.\nInvestigate methods for handling unmeasured confounding and uncertainties inherent in causal inference within the context of environmental analysis.\nApply the developed methodologies to real-world environmental challenges, such as climate change and extreme event analysis. By pursuing these research objectives, I aspire to advance the understanding of causal relationships in environmental systems, contribute to evidence-based decision-making for environmental management, and ultimately support the development of sustainable practices for a better future."
  },
  {
    "objectID": "research.html#background",
    "href": "research.html#background",
    "title": "üõ∞ Research",
    "section": "",
    "text": "The advent of big data and the increasing availability of diverse environmental datasets have provided researchers with an unprecedented opportunity to investigate complex environmental systems. However, extracting meaningful insights from these datasets poses significant challenges due to confounding factors, selection biases, and non-randomness inherent in observational data. Traditional statistical methods often fall short in capturing the complex and non-linear relationships within environmental systems. Therefore, there is a pressing need to integrate causal inference and machine learning techniques to overcome these limitations and achieve a deeper understanding of environmental dynamics."
  },
  {
    "objectID": "research.html#machine-learning",
    "href": "research.html#machine-learning",
    "title": "üõ∞ Research",
    "section": "",
    "text": "Machine learning algorithms have shown remarkable success in pattern recognition, predictive modelling, and data-driven decision-making. The ability to automatically extract complex patterns and relationships from large-scale environmental datasets makes machine learning an invaluable tool for environmental analysis. By leveraging machine learning techniques, develop predictive models to forecast environmental outcomes and assess the potential impacts of policy interventions.. Furthermore, machine learning methods can aid in feature selection, data imputation, and anomaly detection, contributing to improved data quality and reliability for environmental analysis."
  },
  {
    "objectID": "research.html#causal-inference",
    "href": "research.html#causal-inference",
    "title": "üõ∞ Research",
    "section": "",
    "text": "Causal inference provides a powerful framework for identifying cause-and-effect relationships within complex systems. By applying causal inference methods, we can go beyond correlations and uncover the underlying causal mechanisms driving observed environmental phenomena. Approaches such as propensity score matching, instrumental variables, and difference-in-differences analysis allow us to address confounding biases and estimate causal effects more accurately. Incorporating these causal inference techniques into environmental analysis can enhance our ability to discern the impacts of specific environmental factors and human activities on various ecological systems."
  },
  {
    "objectID": "research.html#integration-and-challenges",
    "href": "research.html#integration-and-challenges",
    "title": "üõ∞ Research",
    "section": "",
    "text": "Integrating causal inference with machine learning techniques presents exciting opportunities for advancing environmental analysis. By combining the strengths of both fields, we can develop robust methodologies that provide accurate causal estimates while harnessing the predictive power of machine learning algorithms. However, several challenges need to be addressed to achieve this integration successfully. These challenges include the treatment of unmeasured confounding, scalability of methods for large environmental datasets, interpretability of complex machine learning models, and the consideration of uncertainty in causal inference."
  },
  {
    "objectID": "research.html#research-objectives",
    "href": "research.html#research-objectives",
    "title": "üõ∞ Research",
    "section": "",
    "text": "In my research, I aim to address these challenges and contribute to the nexus between causal inference, machine learning, and environmental analysis. Specifically, I intend to:\n\nDevelop novel methodologies that integrate causal inference and machine learning techniques to estimate causal effects and predict environmental outcomes accurately.\nInvestigate the use of deep learning architectures and explainable AI methods to enhance the interpretability of complex machine learning models for environmental analysis.\nExplore scalable algorithms and parallel computing techniques to handle the computational demands of large environmental datasets.\nInvestigate methods for handling unmeasured confounding and uncertainties inherent in causal inference within the context of environmental analysis.\nApply the developed methodologies to real-world environmental challenges, such as climate change and extreme event analysis. By pursuing these research objectives, I aspire to advance the understanding of causal relationships in environmental systems, contribute to evidence-based decision-making for environmental management, and ultimately support the development of sustainable practices for a better future."
  },
  {
    "objectID": "cv/cv.html",
    "href": "cv/cv.html",
    "title": "Ahmed T. Hammad",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "üßë‚Äçüè´ Teaching",
    "section": "",
    "text": "As a teacher, I aim to perpetuate knowledge and inspire learning.\n\n\n\n\nAs a data scientist and academic researcher, I introduce students to an array of fundamental concepts of our understanding of reality through data and always ask them to articulate their reactions and understanding of the topics by providing me, and their classmates with alternative examples and practical applications of the concepts and techniques we learned together.\nMy teaching approach engages students‚Äô natural creativity with a challenging mix of theory and practice. I teach theoretical concepts with a commitment to connecting these concepts with concrete real-life examples and by sharing the implications in business. For example, as the lead Data Science Instructor at Le Wagon Singapore, I taught several machine learning algorithms by explaining the implication of their use in different business and real-life contexts. I found that this approach helped students understand why, when and how to use a specific algorithm rather than another and how to articulate and justify their decision process.\n\n\n\n\n\nI seek a balance in my courses between lecturing to students and asking them to make discoveries. I encourage students to engage with the topic at hand, with me and with each other, in the belief that good teaching depends upon intellectual exchange. My end goal is to enable my students to envision and develop a ‚Äúproduct‚Äù around the concepts and techniques we learned in class.\nMy approach to student assessment reflects my two goals. First, the students are expected to master a body of knowledge by demonstrating the practical use of different algorithms and methodologies at the end of the lecture. Second, students are given the opportunity to reflect upon the material at greater leisure and detail on project assignments.\n\n\n\n\n\nWhile my standards are high, I do not hesitate to help my students to meet expectations by providing office hours, review sessions and private time to discuss their concerns, fears and dreams. My approach has been to always make myself readily available to students and make it clear that if they need something then I am available at any time and point in their life to support them in their journey.\nI also have a strong passion for helping students who are dedicated, but not necessarily performing well in class. In my experience, I have found some students performing far better in very applied tasks, shining in applied academic research or reaching success in business settings.\n\n\n\n\n\nMy teaching, research, and work experience has covered a wide variety of topics, including machine learning, causal machine learning, incremental machine learning, econometrics, causal inference, probabilistic modelling, time series modelling and network analysis. Given the need, I am qualified to and would readily commit the effort required to effectively teach undergraduate and graduated classes in these subjects. Because of the scope of my current research, I am specifically eager and excited to teach artificial intelligence, data science and statistics classes at both the undergraduate and graduate levels."
  },
  {
    "objectID": "teaching.html#teaching-approach",
    "href": "teaching.html#teaching-approach",
    "title": "üßë‚Äçüè´ Teaching",
    "section": "",
    "text": "As a data scientist and academic researcher, I introduce students to an array of fundamental concepts of our understanding of reality through data and always ask them to articulate their reactions and understanding of the topics by providing me, and their classmates with alternative examples and practical applications of the concepts and techniques we learned together.\nMy teaching approach engages students‚Äô natural creativity with a challenging mix of theory and practice. I teach theoretical concepts with a commitment to connecting these concepts with concrete real-life examples and by sharing the implications in business. For example, as the lead Data Science Instructor at Le Wagon Singapore, I taught several machine learning algorithms by explaining the implication of their use in different business and real-life contexts. I found that this approach helped students understand why, when and how to use a specific algorithm rather than another and how to articulate and justify their decision process."
  },
  {
    "objectID": "teaching.html#lecturing-approach",
    "href": "teaching.html#lecturing-approach",
    "title": "üßë‚Äçüè´ Teaching",
    "section": "",
    "text": "I seek a balance in my courses between lecturing to students and asking them to make discoveries. I encourage students to engage with the topic at hand, with me and with each other, in the belief that good teaching depends upon intellectual exchange. My end goal is to enable my students to envision and develop a ‚Äúproduct‚Äù around the concepts and techniques we learned in class.\nMy approach to student assessment reflects my two goals. First, the students are expected to master a body of knowledge by demonstrating the practical use of different algorithms and methodologies at the end of the lecture. Second, students are given the opportunity to reflect upon the material at greater leisure and detail on project assignments."
  },
  {
    "objectID": "teaching.html#advising-approach",
    "href": "teaching.html#advising-approach",
    "title": "üßë‚Äçüè´ Teaching",
    "section": "",
    "text": "While my standards are high, I do not hesitate to help my students to meet expectations by providing office hours, review sessions and private time to discuss their concerns, fears and dreams. My approach has been to always make myself readily available to students and make it clear that if they need something then I am available at any time and point in their life to support them in their journey.\nI also have a strong passion for helping students who are dedicated, but not necessarily performing well in class. In my experience, I have found some students performing far better in very applied tasks, shining in applied academic research or reaching success in business settings."
  },
  {
    "objectID": "teaching.html#teaching-interests",
    "href": "teaching.html#teaching-interests",
    "title": "üßë‚Äçüè´ Teaching",
    "section": "",
    "text": "My teaching, research, and work experience has covered a wide variety of topics, including machine learning, causal machine learning, incremental machine learning, econometrics, causal inference, probabilistic modelling, time series modelling and network analysis. Given the need, I am qualified to and would readily commit the effort required to effectively teach undergraduate and graduated classes in these subjects. Because of the scope of my current research, I am specifically eager and excited to teach artificial intelligence, data science and statistics classes at both the undergraduate and graduate levels."
  },
  {
    "objectID": "students.html",
    "href": "students.html",
    "title": "üßë‚Äçüéì Students",
    "section": "",
    "text": "What my students say about me\n\n\nI was very fortunate to get Ahmed as my instructor during my data science boot camp. Overall, Ahmed was very passionate in teaching, his lessons were entertaining and enjoyable, and he was also able to explain complicated concepts in a clean and simple manner. What I liked most was that he often imparted little tips and advice on how to do well in data science, and also shared his personal experiences and challenges he faced in his own projects and how he managed to resolve them. I couldn‚Äôt have asked for a better instructor to kickstart my data science journey. Marcus Tan\n\n\nAhmed was my teacher during the 9 weeks data science bootcamp. He is an exceptional and passionate teacher. He teaches with great enthusiasm and patience. Additionally, he goes beyond to provide excellent support and guidance. Speaking for all my classmates, we were truly grateful to have him as a teacher. George Lee\n\n\nI highly recommend Ahmed for the position of university lecturer in data science. I had the pleasure of having him as my teacher in the Le Wagon data science bootcamp. He is a talented data scientist with a passion for teaching. He is also a highly motivated and hardworking individual. I am confident that he would be a positive addition to any company that is willing to hired.\nHere are some of Ahmed‚Äôs strengths as a data science educator:\nDeep knowledge of the subject matter\nAbility to communicate complex concepts in a clear and engaging way\nPatient and helpful in answering student questions\nPassionate about data science and eager to share his knowledge with others. Zheng YongShun\n\n\nAhmed, thank you for being an amazing teacher. Your passion for the subject and the way you create a welcoming classroom environment have made learning so much more enjoyable. You have a talent for making complicated things easy to understand, and I appreciate how you always go the extra mile to help us succeed. I feel lucky to be your student. Yong Sin Tan\n\n\nDr.¬†Ahmed is someone who can break down complicated mathematical concepts into easily understandable components. Not only that, he demonstrates genuine care for his students‚Äô progress.\nEven until now, long after our classes with him, he is still always available when we need help or clarification in data science/career progression topics. I‚Äôm very fortunate to have the opportunity to be taught by this outstanding and passionate teacher. Yong Chew\n\n\nI had the privilege of attending Ahmed‚Äôs lectures for Le Wagon‚Äôs full time data science program that spanned nine weeks.\n\nAhmed is an exceptional educator with a profound expertise in various domains of data science. His proficiency in statistical modeling, data analytics, SQL and Python programming, project management, machine learning, and deep learning is truly impressive. Throughout the bootcamp, Ahmed demonstrated a deep understanding of these subjects, and his ability to effectively convey complex concepts made the learning experience incredibly rewarding.\n\nOne aspect that truly stood out during the bootcamp was the final project on machine learning. We took on the task to predict possible side effects when two drugs are taken together, otherwise known as drug-drug interaction. Ahmed‚Äôs knowledge in statistics played a crucial role in guiding us through the project. His guidance and facilitation during the sessions ensured that we had a comprehensive understanding of each phase of the project. Under his supervision, we were able to complete the project from scratch within a remarkably short timeframe of just 2-3 weeks.\n\nHowever, what truly sets Ahmed apart is his dedication to ensuring that every student grasps the topics taught during the bootcamp. He goes above and beyond to make sure that even those without a computer science or statistics background can understand the material. Ahmed‚Äôs ability to blend well with us students and convey the teachings in a relatable manner is exceptional. He takes pride in his work and is a responsible lecturer who is always available to provide guidance and support.\n\nAhmed‚Äôs teaching style is highly engaging and interactive. He fosters an inclusive learning environment, encouraging students to ask questions, collaborate, and explore practical applications of the concepts learned. His dedication to our success was evident in the time and effort he invested in each student, providing valuable feedback and guidance to help us reach our full potential.\n\nI highly recommend Ahmed as a data science lecturer and researcher. His exceptional knowledge, dedication, and teaching abilities make him an invaluable asset to any learning environment. It was a privilege to learn from him, and I am confident that he will continue to inspire and shape the careers of aspiring data scientists. Xin Er Chong"
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "‚úç Papers",
    "section": "",
    "text": "Publications\n\n\nPrioritizing COVID-19 vaccine allocation in resource poor settings: Towards an Artificial Intelligence-enabled and Geospatial-assisted decision support framework\nShayegh S.,[...], Hammad, A.T., et al., (2023), PLOS ONE\nProbabilistic forecasting of remotely sensed cropland vegetation health and its relevance for food security\nAT Hammad, G Falchetta .Science of the Total Environment 838, 156157\nBack to the fields? Increased agricultural land greenness after a COVID-19 lockdown\nAT Hammad, G Falchetta, IBM Wirawan. Environmental Research Communications 3 (5), 051007\nComparing paratransit in seven major African cities: an accessibility and network analysis\nG Falchetta, M Noussan, AT Hammad. Journal of Transport Geography 94, 103131\nPlanning universal accessibility to public health care in sub-Saharan Africa\nG Falchetta, AT Hammad, S Shayegh. Proceedings of the National Academy of Sciences 117 (50), 31760-31769\n\n\n\n\nCurrent work\n\n\nTracking global urban green space trends\nG Falchetta, AT Hammad. EGU23\n\n\n\n\nForthcoming\n\n\nA note on vulnerability\nProbabilistic reinforcement learning for evaluating single-subject experiments"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ahmed T. Hammad",
    "section": "",
    "text": "Hello, my name is Ahmed, and I am a Data Scientist, Quantitative Researcher, and Lecturer. I have a PhD in Causal Machine Learning and over 10 years of experience working with data in both academic and private sectors. I have extensive knowledge of programming languages such as R and Python, which I use daily to conduct my research and analysis.\nAs a passionate learner, I always strive to stay up-to-date with the latest developments in Machine Learning. That‚Äôs why I‚Äôm still involved in research and regularly publish my analysis on a variety of topics, including Machine Learning and Econometrics applied to environmental analysis and public health.\nOne of the most fulfilling aspects of my career is sharing my knowledge of statistics and machine learning with others. Teaching the younger generation and professionals how to ‚Äúget things done‚Äù and handle real-world data problems is something that brings me so much joy. It‚Äôs an opportunity for me to share my years of experience in the field and help others succeed.\n\n\n\n\n\n\n\n\nMachine Learning\nEconometrics\nIncremental Machine Learning\nCausal Inference\n\n\n\nTime Series\nProbabilistic Machine Learning\nEnvironmental Analysis\nSensor & Satellite Data"
  },
  {
    "objectID": "index.html#my-fields-of-interest",
    "href": "index.html#my-fields-of-interest",
    "title": "Ahmed T. Hammad",
    "section": "",
    "text": "Machine Learning\nEconometrics\nIncremental Machine Learning\nCausal Inference\n\n\n\nTime Series\nProbabilistic Machine Learning\nEnvironmental Analysis\nSensor & Satellite Data"
  },
  {
    "objectID": "blog/Logistic Regression and Marginal Effects.html",
    "href": "blog/Logistic Regression and Marginal Effects.html",
    "title": "Logistic Regression and Marginal Effects",
    "section": "",
    "text": "Logistic regression is a widely used statistical technique for modeling the relationship between a binary outcome and one or more predictor variables. It is commonly employed in various fields, such as healthcare, finance, and social sciences, to predict the probability of an event occurring. While the coefficients in logistic regression provide valuable insights, they are difficult to interpret. In this blog post, we will explore the use of marginal effects in logistic regression, unraveling their significance and practical applications.\n\n\nIn logistic regression, we model the log-odds of the event of interest occurring as a linear combination of predictor variables. The logistic function (sigmoid function) is then applied to this linear combination to convert it into a probability value between 0 and 1.\nThe logistic regression equation is given by:\n\\[\\text{logit}(P(Y=1)) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p\\]\nWhere:\n\n\\(P(Y=1)\\) is the probability of the event of interest (e.g., success) occurring.\n\\(logit (P(Y=1))\\) is the log-odds (logit) of the event occurring.\n\\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_p\\) are the coefficients of the model.\n\\(X_1, X_2, ..., X_p\\) are the predictor variables.\n\n\n\nThe coefficients \\(\\beta_1, \\beta_2, ..., \\beta_p\\) in the logistic regression equation hold crucial information about the impact of each predictor variable on the log-odds of the event occurring. To interpret these coefficients, we need to consider their signs, magnitude, and statistical significance.\n\nSign of Coefficients:\n\nThe sign of a coefficient reveals the direction of the relationship between the predictor variable and the log-odds of the event occurring. A positive coefficient (\\(\\beta &gt; 0\\)) indicates that an increase in the predictor variable leads to an increase in the log-odds (probability) of the event happening. Conversely, a negative coefficient (\\(\\beta &lt; 0\\)) suggests that an increase in the predictor variable is associated with a decrease in the log-odds (probability) of the event occurring.\n\nMagnitude of Coefficients:\n\nThe magnitude of a coefficient provides information about the strength of the relationship between the predictor variable and the log-odds of the event. Larger absolute values of coefficients indicate stronger influences on the probability.\n\nStatistical Significance:\n\nEvaluating the statistical significance of coefficients is essential to determine if the relationship between a predictor variable and the event probability is meaningful or just due to random chance. Researchers often use hypothesis testing or confidence intervals to assess the statistical significance of coefficients.\n\n\nLet‚Äôs consider an example to illustrate the interpretation of coefficients in logistic regression. Suppose we are studying the factors that influence whether a customer will make a purchase on an e-commerce website. Our logistic regression model includes two predictor variables: ‚ÄúTime Spent on Website‚Äù and ‚ÄúNumber of Items Added to Cart.‚Äù The estimated coefficients are as follows:\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\)\n\\(\\beta_{\\text{Number of Items}} = 0.2\\)\n\n\n\n\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\): For each additional minute a customer spends on the website, the log-odds (probability) of making a purchase increases by 0.05 units. This means that spending more time on the website is associated with a higher likelihood of making a purchase.\n\\(\\beta_{\\text{Number of Items}} = 0.2\\): For each additional item a customer adds to the cart, the log-odds (probability) of making a purchase increases by 0.2 units. This suggests that adding more items to the cart is associated with a higher probability of making a purchase.\n\n\n\n\nIdeally, we want to understand what the model is telling us on the probability scale and not in the odds scale, much less in the estimation scale, the log-odds.\nWe might be tempted to convert log-odds to odds and then to probability and interpret everything as usual. But this would be very wrong!\nWhy?\nThe issue here is that on the probability scale the function is non-linear!\n\n\n\n\nMarginal effects can be described as the change in outcome as a function of the change in the independent variable of interest holding all other variables in the model constant.\n\nNOTE: In linear regression, the estimated regression coefficients are marginal effects and are more easily interpreted.*\n\nOne measure of change in a system is that the rate of change of the system is non-zero.\nIf we had a simple linear regression model for a trend, then the estimated rate of change would be the slope of the line. Technically, this is the instantaneous rate of change of the function that defines the line.\nThe idea can be extended to any function, even one as potentially complex non-linear function.\nThe problem we have is that in general we don‚Äôt have an equation for the non-linear from which we can derive the derivatives.\nSo how do we estimate the derivative of a non-linear function ?\nOne solution is to use the method of finite differences but to understand it properly we need to introduce first the basic concept of differential calculus.\n\n\n\n\nDifferential calculus is a branch of calculus that focuses on the study of rates of change and slopes of curves. It deals with the concept of derivatives, which represent the rate of change of a function at a specific point. Derivatives allow us to understand how a function is changing at a particular location and provide valuable insights into the behavior of functions.\nThe derivative of a function \\(f(x)\\) with respect to \\(x\\) is denoted as \\(f'(x)\\) or \\(\\frac{df(x)}{dx}\\). It represents the slope of the tangent line to the graph of the function at a given point \\(x\\). The derivative can be interpreted as the instantaneous rate of change of \\(f(x)\\) with respect to \\(x\\).\nFor example, consider the function \\(f(x) = x^2\\). The derivative of \\(f(x)\\) is \\(f'(x) = 2x\\). This means that the slope of the tangent line to the graph of \\(f(x)\\) at any point \\(x\\) is twice the value of \\(x\\) at that point.\nDifferential calculus has wide-ranging applications in various fields, including physics, engineering, economics, and more. It is used to model and analyze phenomena that involve continuous changes, such as motion, growth, and optimization.\nFinite differences are a numerical technique used to approximate derivatives of functions. The method involves computing the difference in function values at two nearby points and then dividing by the difference in their corresponding independent variable values. By using a small interval between the two points, we can get an approximation of the derivative at a specific point.\nThe finite difference formula for approximating the first derivative of a function \\(f(x)\\) at a point \\(x\\) is given by:\n\\[ f‚Äô(x) \\approx \\frac{f(x + h) - f(x)}{h} \\]\nWhere:\n\n\\(h\\) is a small value representing the step size or interval between the points.\n\nSimilarly, we can use finite differences to approximate higher-order derivatives, such as the second derivative:\n\\[ f‚Äô‚Äô(x) \\approx \\frac{f(x + h) - 2f(x) + f(x - h)}{h^2} \\]\nFinite differences are particularly useful when the analytical expression for the derivative is difficult to obtain or when dealing with discrete data points instead of continuous functions.\nDifferential calculus and finite differences are both essential tools in mathematics and scientific disciplines. Differential calculus provides a rigorous framework for studying rates of change and slopes of curves through derivatives. On the other hand, finite differences offer a practical and numerical approach to approximate derivatives when the analytical solution is not readily available or when dealing with discrete data. Together, these concepts enable us to better understand the behavior of functions and analyze real-world phenomena involving continuous and discrete changes.\n\n\n\nIn logistic regression, the estimated coefficients (log odds) indicate how the log odds of the outcome change with a one-unit increase in the predictor variable while holding other variables constant. However, interpreting these coefficients can be challenging, especially for non-statisticians. This is where marginal effects come to the rescue.\nMarginal effects provide a more straightforward interpretation by measuring the impact of a one-unit change in the predictor variable on the probability of the event occurring (i.e., the probability of success). They express the change in the probability as a result of the predictor variable‚Äôs change, allowing us to understand the practical implications of the model."
  },
  {
    "objectID": "blog/Unraveling the Power of Causal Machine Learning.html",
    "href": "blog/Unraveling the Power of Causal Machine Learning.html",
    "title": "Unraveling the Power of Causal Machine Learning",
    "section": "",
    "text": "Hey fellow data enthusiasts! Today, let‚Äôs embark on a journey to explore the captivating world of causal machine learning. üöÄ\nüîç Understanding Causality:\nIn the realm of traditional machine learning, we primarily focus on identifying patterns and correlations in data to make accurate predictions. While this approach is incredibly useful, it often fails to decipher the cause-and-effect relationships that underpin these patterns. This limitation can lead to misguided conclusions and ineffective decision-making.\nCausality seeks to answer ‚Äúwhy‚Äù questions, revealing the cause behind an observed effect. Imagine a scenario where we want to understand if a certain medication truly improves patient outcomes or if it‚Äôs just correlated with better health. Causal inference allows us to make more meaningful inferences, beyond mere correlations.\nüß© The Challenge of Causal Inference:\nEstablishing causality is not a straightforward task. The fundamental problem lies in the fact that we cannot simultaneously observe both the treatment group (those who receive a particular intervention) and the control group (those who do not). Traditional observational data often suffers from confounding variables, making it challenging to disentangle cause and effect.\nüí° Enter Causal Machine Learning:\nCausal Machine Learning, a fascinating interdisciplinary field, bridges the gap between traditional machine learning and causal inference. Its primary goal is to leverage data to identify causal relationships and infer the effects of interventions accurately.\nüéØ Causal Inference Methods:\n1. Randomized Control Trials (RCTs): Considered the gold standard for establishing causality, RCTs involve randomly assigning subjects to either the treatment or control group. By eliminating confounding factors, RCTs provide strong causal evidence. However, they may not always be practical or ethical.\n2. Propensity Score Matching (PSM): When RCTs are not feasible, PSM is a popular method. It attempts to create a ‚Äúquasi-experimental‚Äù design by matching treated and control units based on their propensity scores, which represent the likelihood of receiving the treatment. This helps mitigate confounding effects.\n3. Instrumental Variables (IV): IV analysis relies on instrumental variables that are correlated with the treatment but have no direct effect on the outcome. By utilizing these instruments, researchers can uncover causal relationships even in the presence of unobserved confounders.\n4. Synthetic Control (SC): The process of creating a synthetic counterfactual involves using historical data from similar individuals or groups that did not receive the treatment to construct a ‚Äúsynthetic‚Äù control group. This control group is designed to closely match the characteristics of the treated group before the intervention. By doing so, researchers aim to create a plausible estimate of what would have happened to the treated group had they not received the treatment.\nüöÄ Real-World Applications:\nCausal Machine Learning has found applications in various domains, including public health, economics, social sciences, and marketing. It enables policymakers to make informed decisions, businesses to optimize interventions, and researchers to uncover hidden causal mechanisms.\nüåü Embracing Causal Machine Learning:\nAs the world becomes more data-driven, understanding causality becomes paramount. Embracing causal machine learning techniques empowers us to make smarter decisions, avoid biases, and design better interventions that have a genuine impact on our lives.\nAn excellent example application of Causal Machine Learning is in the field of healthcare for personalized treatment recommendations or to estimate the impact of marketing campaigns. Beyonf the classic RCT or A/B testing, in both cases the use of Reinforcement Learning is a very interesting application that we will discuss in the next blog post."
  },
  {
    "objectID": "blog/Quantile Random Forest.html",
    "href": "blog/Quantile Random Forest.html",
    "title": "Quantile Random Forest",
    "section": "",
    "text": "Quantile Random Forest (QRF) is an extension of the traditional Random Forest algorithm that estimates not only the mean but also the entire conditional distribution of the target variable. The distribution is represented by its quantiles, which are specific points that divide the data into segments, such as quartiles (0.25, 0.50, 0.75). QRF is particularly useful when dealing with heteroscedastic data or when we need to assess the uncertainty in our predictions beyond just the central tendency.\nIn QRF, an ensemble of decision trees is constructed. Each decision tree is built by bootstrapping the training data, which means sampling the data points with replacement, and selecting a random subset of features at each split. This creates diversity among the trees in the ensemble.\nThe key difference between Random Forest and QRF lies in the prediction phase. In Random Forest, the predicted value for a new data point is the average (mean) of the individual tree predictions. However, in QRF, we aim to estimate multiple quantiles of the target variable‚Äôs distribution for a given input.\nWhen growing a tree in QRF, instead of splitting the data based solely on minimizing the variance or mean squared error, QRF performs splits to optimize quantile-specific criteria. Each tree node is split to minimize the quantile loss function, which reflects the error in predicting the specific quantile. This means that the tree seeks to minimize the difference between the true quantile value and the estimated quantile value for the data points that fall into each node.\nOnce the tree is fully grown, the terminal nodes (leaf nodes) contain specific quantile values rather than just the average value. Each terminal node in the QRF stores a quantile-specific estimate based on the data points that fall into that node. This means that each tree provides quantile-specific predictions.\nTo make predictions for a new data point, QRF evaluates the input through each decision tree, resulting in a set of quantile-specific predictions. The user can then choose the desired quantiles (e.g., 0.25, 0.50, 0.75) to obtain the corresponding quantile predictions. For example, if we want to estimate the 0.75 quantile, we collect the 0.75 quantile values from each tree, and these values form the quantile prediction for that data point.\nOne of the advantages of QRF is that it provides a measure of uncertainty in the predictions. Given the estimated quantiles, QRF can construct a prediction interval for each data point. The prediction interval provides a range of possible values, accounting for the uncertainty in the prediction due to the variability of the underlying data.\nAdvantages of Quantile Random Forest:\n\nRobustness to outliers: QRF can handle outliers effectively as it estimates the entire distribution, including extreme quantiles.\nProvides uncertainty information: By estimating quantiles, QRF offers a measure of uncertainty in the predictions, which is valuable in decision-making.\nVersatility: QRF can be applied to regression and quantile regression problems, and it can be extended to handle multivariate quantile estimation.\n\nApplications of Quantile Random Forest:\n\nFinancial forecasting: QRF can be used to predict quantiles of financial variables, such as stock prices or asset returns, to assess the risk of investment decisions.\nMedical applications: In healthcare, QRF can estimate quantiles of patient response times to treatments, enabling doctors to tailor treatments accordingly.\nEnvironmental modeling: QRF can estimate quantiles of environmental variables like pollutant concentrations, assisting in environmental risk assessments.\n\nIn summary, Quantile Random Forest is a powerful extension of the Random Forest algorithm that estimates the entire distribution of the target variable through quantile-specific predictions. It is useful when capturing the entire distribution or quantifying uncertainty is essential for making informed decisions in various applications such as finance, healthcare, and environmental modeling."
  },
  {
    "objectID": "blog/Embracing Change: Incremental vs. Batch Machine Learning.html",
    "href": "blog/Embracing Change: Incremental vs. Batch Machine Learning.html",
    "title": "Embracing Change: Incremental vs.¬†Batch Machine Learning",
    "section": "",
    "text": "Title: Embracing Change: Incremental vs.¬†Batch Machine Learning\nIntroduction\nMachine learning has revolutionized the way we interact with data, allowing us to uncover valuable insights and make informed decisions. Within the realm of machine learning, there are two fundamental approaches: incremental learning and batch learning. These methods serve distinct purposes and cater to diverse scenarios, offering unique advantages and challenges. In this blog post, we will embark on an exciting journey to understand the differences between incremental and batch machine learning, providing you with the knowledge to embrace these powerful techniques.\nIncremental Machine Learning: An Ever-Evolving Journey\nImagine a learning process that continuously adapts to new information, always striving to stay up-to-date with the latest trends. That‚Äôs incremental machine learning for you! Also known as online learning or streaming learning, this approach updates the model in real-time as fresh data arrives. Unlike batch learning, which requires processing the entire dataset at once, incremental learning embraces change, incorporating new observations into the existing model as they arrive.\nCharacteristics of Incremental Machine Learning:\n1. Real-Time Adaptation: The beauty of incremental learning lies in its ability to quickly respond to shifts in data distribution. This makes it ideal for applications dealing with rapidly evolving data, like online advertising or fraud detection.\n2. Low Memory Requirements: Incremental learning processes data in small batches or individual data points, reducing memory usage compared to batch learning, where the entire dataset needs to be stored.\n3. Constant Learning: With incremental learning, models can continuously learn and improve over time without the need for retraining on the entire dataset. It‚Äôs like a lifelong learning journey!\nAdvantages of Incremental Machine Learning:\n1. Efficiency: Incremental learning is computationally efficient since it updates the model incrementally, without the need to retrain on the entire dataset each time.\n2. Scalability: This approach is well-suited for large-scale applications where processing the entire dataset at once would be overwhelming.\n3. Dynamic Adaptation: Incremental learning is ideal for applications where data distribution changes frequently, such as natural language processing or recommendation systems. It embraces the unexpected!\nDisadvantages of Incremental Machine Learning:\n1. Forgetting Old Data: Just like we can‚Äôt remember everything we‚Äôve learned in the past, incremental learning models may forget information from the past, leading to potential performance degradation on older data points.\n2. Model Drift: In dynamic environments, incremental learning models may struggle to adapt to sudden shifts in data patterns, leading to inaccuracies. Sometimes, change can be overwhelming!\nBatch Machine Learning: A Journey with Stability\nNow, let‚Äôs switch gears and explore batch machine learning. Picture a learning process that takes time to analyze and understand the whole story before making decisions. That‚Äôs batch machine learning for you! This approach involves training the model on the entire dataset and updating it periodically or when significant amounts of new data accumulate.\nCharacteristics of Batch Machine Learning:\n1. Periodic Updates: Batch learning involves training models from scratch on a fixed dataset, leveraging powerful hardware resources and sophisticated algorithms. It‚Äôs like taking a step back to see the bigger picture!\n2. Complete Data Usage: Unlike incremental learning, which processes data in smaller chunks, batch learning uses the entire dataset for training, ensuring no data points are left behind.\nAdvantages of Batch Machine Learning:\n1. Strong Convergence: By training on the entire dataset, batch learning often achieves more stable and reliable results, especially with sufficient computational resources. It‚Äôs like taking the time to contemplate before making decisions.\n2. Accuracy: Batch learning models tend to be more accurate due to their exhaustive training process. Sometimes, thoroughness pays off!\nDisadvantages of Batch Machine Learning:\n1. High Resource Demands: Batch learning requires substantial computational power and memory, making it less scalable for real-time applications or large datasets.\n2. Time Lag in Updates: Models may not adapt quickly to changes in data distribution since they are updated only after accumulating new data. Patience is key!\nUse Cases for Incremental and Batch Machine Learning\nIncremental Learning:\n- Real-time fraud detection in financial transactions.\n- Adaptive recommendation systems that learn from user interactions in real-time.\n- Sentiment analysis on streaming social media data.\nBatch Learning:\n- Image classification in computer vision tasks.\n- Natural language processing applications like language translation.\n- Training large-scale recommendation systems periodically.\nConclusion\nAs we bid farewell to this exploration, we have gained an understanding of the intriguing differences between incremental and batch machine learning. Incremental learning embraces change and adapts to evolving data, making it perfect for real-time applications. On the other hand, batch learning takes a more measured approach, achieving strong convergence and accuracy with thorough analysis. Both methods have their unique strengths, empowering data scientists and developers to make informed choices based on the demands of their applications. So, whether you‚Äôre venturing on a dynamic journey with incremental learning or taking a stable path with batch learning, remember to embrace the wonders of machine learning and let it lead you to exciting discoveries!"
  },
  {
    "objectID": "blog/Logistic Regression and Marginal Effects.html#interpretation-issues",
    "href": "blog/Logistic Regression and Marginal Effects.html#interpretation-issues",
    "title": "Logistic Regression and Marginal Effects",
    "section": "Interpretation Issues",
    "text": "Interpretation Issues\nIdeally, we want to understand what the model is telling us on the probability scale and not in the odds scale, much less in the estimation scale, the log-odds.\nWe might be tempted to convert log-odds to odds and then to probability and interpret everything as usual. But this would be very wrong!\nWhy?\nThe issue here is that on the probability scale the function is non-linear!"
  },
  {
    "objectID": "blog/Logistic Regression and Marginal Effects.html#logistic-regression",
    "href": "blog/Logistic Regression and Marginal Effects.html#logistic-regression",
    "title": "Logistic Regression and Marginal Effects",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nIn logistic regression, we model the log-odds of the event of interest occurring as a linear combination of predictor variables. The logistic function (sigmoid function) is then applied to this linear combination to convert it into a probability value between 0 and 1.\nThe logistic regression equation is given by:\n\\[\\text{logit}(P(Y=1)) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p\\]\nWhere:\n\n\\(P(Y=1)\\) is the probability of the event of interest (e.g., success) occurring.\n\\(logit (P(Y=1))\\) is the log-odds (logit) of the event occurring.\n\\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_p\\) are the coefficients of the model.\n\\(X_1, X_2, ..., X_p\\) are the predictor variables.\n\nInterpreting Coefficients in Logistic Regression\nThe coefficients \\(\\beta_1, \\beta_2, ..., \\beta_p\\) in the logistic regression equation hold crucial information about the impact of each predictor variable on the log-odds of the event occurring. To interpret these coefficients, we need to consider their signs, magnitude, and statistical significance.\n\nSign of Coefficients:\n\nThe sign of a coefficient reveals the direction of the relationship between the predictor variable and the log-odds of the event occurring. A positive coefficient (\\(\\beta &gt; 0\\)) indicates that an increase in the predictor variable leads to an increase in the log-odds (probability) of the event happening. Conversely, a negative coefficient (\\(\\beta &lt; 0\\)) suggests that an increase in the predictor variable is associated with a decrease in the log-odds (probability) of the event occurring.\n\nMagnitude of Coefficients:\n\nThe magnitude of a coefficient provides information about the strength of the relationship between the predictor variable and the log-odds of the event. Larger absolute values of coefficients indicate stronger influences on the probability.\n\nStatistical Significance:\n\nEvaluating the statistical significance of coefficients is essential to determine if the relationship between a predictor variable and the event probability is meaningful or just due to random chance. Researchers often use hypothesis testing or confidence intervals to assess the statistical significance of coefficients.\nPractical Interpretation Example\nLet‚Äôs consider an example to illustrate the interpretation of coefficients in logistic regression. Suppose we are studying the factors that influence whether a customer will make a purchase on an e-commerce website. Our logistic regression model includes two predictor variables: ‚ÄúTime Spent on Website‚Äù and ‚ÄúNumber of Items Added to Cart.‚Äù The estimated coefficients are as follows:\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\)\n\\(\\beta_{\\text{Number of Items}} = 0.2\\)"
  },
  {
    "objectID": "blog/Logistic Regression and Marginal Effects.html#interpreting-coefficients-in-logistic-regression",
    "href": "blog/Logistic Regression and Marginal Effects.html#interpreting-coefficients-in-logistic-regression",
    "title": "Logistic Regression and Marginal Effects",
    "section": "",
    "text": "The coefficients \\(\\beta_1, \\beta_2, ..., \\beta_p\\) in the logistic regression equation hold crucial information about the impact of each predictor variable on the log-odds of the event occurring. To interpret these coefficients, we need to consider their signs, magnitude, and statistical significance.\n\nSign of Coefficients:\n\nThe sign of a coefficient reveals the direction of the relationship between the predictor variable and the log-odds of the event occurring. A positive coefficient (\\(\\beta &gt; 0\\)) indicates that an increase in the predictor variable leads to an increase in the log-odds (probability) of the event happening. Conversely, a negative coefficient (\\(\\beta &lt; 0\\)) suggests that an increase in the predictor variable is associated with a decrease in the log-odds (probability) of the event occurring.\n\nMagnitude of Coefficients:\n\nThe magnitude of a coefficient provides information about the strength of the relationship between the predictor variable and the log-odds of the event. Larger absolute values of coefficients indicate stronger influences on the probability.\n\nStatistical Significance:\n\nEvaluating the statistical significance of coefficients is essential to determine if the relationship between a predictor variable and the event probability is meaningful or just due to random chance. Researchers often use hypothesis testing or confidence intervals to assess the statistical significance of coefficients.\n\n\nLet‚Äôs consider an example to illustrate the interpretation of coefficients in logistic regression. Suppose we are studying the factors that influence whether a customer will make a purchase on an e-commerce website. Our logistic regression model includes two predictor variables: ‚ÄúTime Spent on Website‚Äù and ‚ÄúNumber of Items Added to Cart.‚Äù The estimated coefficients are as follows:\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\)\n\\(\\beta_{\\text{Number of Items}} = 0.2\\)\n\n\n\n\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\): For each additional minute a customer spends on the website, the log-odds (probability) of making a purchase increases by 0.05 units. This means that spending more time on the website is associated with a higher likelihood of making a purchase.\n\\(\\beta_{\\text{Number of Items}} = 0.2\\): For each additional item a customer adds to the cart, the log-odds (probability) of making a purchase increases by 0.2 units. This suggests that adding more items to the cart is associated with a higher probability of making a purchase.\n\n\n\n\nIdeally, we want to understand what the model is telling us on the probability scale and not in the odds scale, much less in the estimation scale, the log-odds.\nWe might be tempted to convert log-odds to odds and then to probability and interpret everything as usual. But this would be very wrong!\nWhy?\nThe issue here is that on the probability scale the function is non-linear!"
  },
  {
    "objectID": "blog/Logistic Regression and Marginal Effects.html#interpretation",
    "href": "blog/Logistic Regression and Marginal Effects.html#interpretation",
    "title": "Logistic Regression and Marginal Effects",
    "section": "Interpretation:",
    "text": "Interpretation:\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\): For each additional minute a customer spends on the website, the log-odds (probability) of making a purchase increases by 0.05 units. This means that spending more time on the website is associated with a higher likelihood of making a purchase.\n\\(\\beta_{\\text{Number of Items}} = 0.2\\): For each additional item a customer adds to the cart, the log-odds (probability) of making a purchase increases by 0.2 units. This suggests that adding more items to the cart is associated with a higher probability of making a purchase.\n\n\nInterpretation Issues\nIdeally, we want to understand what the model is telling us on the probability scale and not in the odds scale, much less in the estimation scale, the log-odds.\nWe might be tempted to convert log-odds to odds and then to probability and interpret everything as usual. But this would be very wrong!\nWhy?\nThe issue here is that on the probability scale the function is non-linear!"
  },
  {
    "objectID": "blog/Logistic Regression and Marginal Effects.html#marginal-effects",
    "href": "blog/Logistic Regression and Marginal Effects.html#marginal-effects",
    "title": "Logistic Regression and Marginal Effects",
    "section": "",
    "text": "Marginal effects can be described as the change in outcome as a function of the change in the independent variable of interest holding all other variables in the model constant.\n\nNOTE: In linear regression, the estimated regression coefficients are marginal effects and are more easily interpreted.*\n\nOne measure of change in a system is that the rate of change of the system is non-zero.\nIf we had a simple linear regression model for a trend, then the estimated rate of change would be the slope of the line. Technically, this is the instantaneous rate of change of the function that defines the line.\nThe idea can be extended to any function, even one as potentially complex non-linear function.\nThe problem we have is that in general we don‚Äôt have an equation for the non-linear from which we can derive the derivatives.\nSo how do we estimate the derivative of a non-linear function ?\nOne solution is to use the method of finite differences but to understand it properly we need to introduce first the basic concept of differential calculus."
  },
  {
    "objectID": "posts/Unraveling the Power of Causal Machine Learning.html",
    "href": "posts/Unraveling the Power of Causal Machine Learning.html",
    "title": "Unraveling the Power of Causal Machine Learning",
    "section": "",
    "text": "In the realm of traditional machine learning, we primarily focus on identifying patterns and correlations in data to make accurate predictions. While this approach is incredibly useful, it often fails to decipher the cause-and-effect relationships that underpin these patterns. This limitation can lead to misguided conclusions and ineffective decision-making.\nCausality seeks to answer ‚Äúwhy‚Äù questions, revealing the cause behind an observed effect. Imagine a scenario where we want to understand if a certain medication truly improves patient outcomes or if it‚Äôs just correlated with better health. Causal inference allows us to make more meaningful inferences, beyond mere correlations.\n\n\nEstablishing causality is not a straightforward task. The fundamental problem lies in the fact that we cannot simultaneously observe both the treatment group (those who receive a particular intervention) and the control group (those who do not). Traditional observational data often suffers from confounding variables, making it challenging to disentangle cause and effect.\n\n\n\nCausal Machine Learning, a fascinating interdisciplinary field, bridges the gap between traditional machine learning and causal inference. Its primary goal is to leverage data to identify causal relationships and infer the effects of interventions accurately.\n\n\n\n\nRandomized Control Trials (RCTs): Considered the gold standard for establishing causality, RCTs involve randomly assigning subjects to either the treatment or control group. By eliminating confounding factors, RCTs provide strong causal evidence. However, they may not always be practical or ethical.\nPropensity Score Matching (PSM): When RCTs are not feasible, PSM is a popular method. It attempts to create a ‚Äúquasi-experimental‚Äù design by matching treated and control units based on their propensity scores, which represent the likelihood of receiving the treatment. This helps mitigate confounding effects.\nInstrumental Variables (IV): IV analysis relies on instrumental variables that are correlated with the treatment but have no direct effect on the outcome. By utilizing these instruments, researchers can uncover causal relationships even in the presence of unobserved confounders.\nSynthetic Control (SC): The process of creating a synthetic counterfactual involves using historical data from similar individuals or groups that did not receive the treatment to construct a ‚Äúsynthetic‚Äù control group. This control group is designed to closely match the characteristics of the treated group before the intervention. By doing so, researchers aim to create a plausible estimate of what would have happened to the treated group had they not received the treatment.\n\n\n\n\nCausal Machine Learning has found applications in various domains, including public health, economics, social sciences, and marketing. It enables policymakers to make informed decisions, businesses to optimize interventions, and researchers to uncover hidden causal mechanisms.\n\n\n\nAs the world becomes more data-driven, understanding causality becomes paramount. Embracing causal machine learning techniques empowers us to make smarter decisions, avoid biases, and design better interventions that have a genuine impact on our lives.\nAn excellent example application of Causal Machine Learning is in the field of healthcare for personalized treatment recommendations or to estimate the impact of marketing campaigns. Beyond the classic RCT or A/B testing, in both cases the use of Reinforcement Learning is a very interesting application that we will discuss in the next blog post."
  },
  {
    "objectID": "posts/Quantile Random Forest.html",
    "href": "posts/Quantile Random Forest.html",
    "title": "Quantile Random Forest",
    "section": "",
    "text": "Quantile Random Forest (QRF) is an extension of the traditional Random Forest algorithm that estimates not only the mean but also the entire conditional distribution of the target variable. The distribution is represented by its quantiles, which are specific points that divide the data into segments, such as quartiles (0.25, 0.50, 0.75). QRF is particularly useful when dealing with heteroscedastic data or when we need to assess the uncertainty in our predictions beyond just the central tendency.\nIn QRF, an ensemble of decision trees is constructed. Each decision tree is built by bootstrapping the training data, which means sampling the data points with replacement, and selecting a random subset of features at each split. This creates diversity among the trees in the ensemble."
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html",
    "title": "Embracing Change: Incremental vs.¬†Batch Machine Learning",
    "section": "",
    "text": "Machine learning has revolutionized the way we interact with data, allowing us to uncover valuable insights and make informed decisions. Within the realm of machine learning, there are two fundamental approaches: incremental learning and batch learning. These methods serve distinct purposes and cater to diverse scenarios, offering unique advantages and challenges. In this blog post, we will embark on an exciting journey to understand the differences between incremental and batch machine learning, providing you with the knowledge to embrace these powerful techniques.\n\n\nImagine a learning process that continuously adapts to new information, always striving to stay up-to-date with the latest trends. That‚Äôs incremental machine learning for you! Also known as online learning or streaming learning, this approach updates the model in real-time as fresh data arrives. Unlike batch learning, which requires processing the entire dataset at once, incremental learning embraces change, incorporating new observations into the existing model as they arrive.\n\n\n\nReal-Time Adaptation: The beauty of incremental learning lies in its ability to quickly respond to shifts in data distribution. This makes it ideal for applications dealing with rapidly evolving data, like online advertising or fraud detection.\nLow Memory Requirements: Incremental learning processes data in small batches or individual data points, reducing memory usage compared to batch learning, where the entire dataset needs to be stored.\nConstant Learning: With incremental learning, models can continuously learn and improve over time without the need for retraining on the entire dataset. It‚Äôs like a lifelong learning journey!\n\nAdvantages of Incremental Machine Learning\n\nEfficiency: Incremental learning is computationally efficient since it updates the model incrementally, without the need to retrain on the entire dataset each time.\nScalability: This approach is well-suited for large-scale applications where processing the entire dataset at once would be overwhelming.\nDynamic Adaptation: Incremental learning is ideal for applications where data distribution changes frequently, such as natural language processing or recommendation systems. It embraces the unexpected!\n\nDisadvantages of Incremental Machine Learning\n\nForgetting Old Data: Just like we can‚Äôt remember everything we‚Äôve learned in the past, incremental learning models may forget information from the past, leading to potential performance degradation on older data points.\nModel Drift: In dynamic environments, incremental learning models may struggle to adapt to sudden shifts in data patterns, leading to inaccuracies. Sometimes, change can be overwhelming!\n\n\n\n\n\nNow, let‚Äôs switch gears and explore batch machine learning. Picture a learning process that takes time to analyze and understand the whole story before making decisions. That‚Äôs batch machine learning for you! This approach involves training the model on the entire dataset and updating it periodically or when significant amounts of new data accumulate.\n\n\n\nPeriodic Updates: Batch learning involves training models from scratch on a fixed dataset, leveraging powerful hardware resources and sophisticated algorithms. It‚Äôs like taking a step back to see the bigger picture!\nComplete Data Usage: Unlike incremental learning, which processes data in smaller chunks, batch learning uses the entire dataset for training, ensuring no data points are left behind.\n\nAdvantages of Batch Machine Learning\n\nStrong Convergence: By training on the entire dataset, batch learning often achieves more stable and reliable results, especially with sufficient computational resources. It‚Äôs like taking the time to contemplate before making decisions.\nAccuracy: Batch learning models tend to be more accurate due to their exhaustive training process. Sometimes, thoroughness pays off!\n\nDisadvantages of Batch Machine Learning\n\nHigh Resource Demands: Batch learning requires substantial computational power and memory, making it less scalable for real-time applications or large datasets.\nTime Lag in Updates: Models may not adapt quickly to changes in data distribution since they are updated only after accumulating new data. Patience is key!\n\n\n\n\n\nAs we bid farewell to this exploration, we have gained an understanding of the intriguing differences between incremental and batch machine learning. Incremental learning embraces change and adapts to evolving data, making it perfect for real-time applications. On the other hand, batch learning takes a more measured approach, achieving strong convergence and accuracy with thorough analysis. Both methods have their unique strengths, empowering data scientists and developers to make informed choices based on the demands of their applications. So, whether you‚Äôre venturing on a dynamic journey with incremental learning or taking a stable path with batch learning, remember to embrace the wonders of machine learning and let it lead you to exciting discoveries!"
  },
  {
    "objectID": "posts/Logistic Regression and Marginal Effects.html",
    "href": "posts/Logistic Regression and Marginal Effects.html",
    "title": "Logistic Regression and Marginal Effects",
    "section": "",
    "text": "Logistic regression is a widely used statistical technique for modeling the relationship between a binary outcome and one or more predictor variables. It is commonly employed in various fields, such as healthcare, finance, and social sciences, to predict the probability of an event occurring. While the coefficients in logistic regression provide valuable insights, they are difficult to interpret. In this blog post, we will explore the use of marginal effects in logistic regression, unraveling their significance and practical applications."
  },
  {
    "objectID": "posts/Logistic Regression and Marginal Effects.html#interpreting-coefficients-in-logistic-regression",
    "href": "posts/Logistic Regression and Marginal Effects.html#interpreting-coefficients-in-logistic-regression",
    "title": "Logistic Regression and Marginal Effects",
    "section": "Interpreting Coefficients in Logistic Regression",
    "text": "Interpreting Coefficients in Logistic Regression\nThe coefficients \\(\\beta_1, \\beta_2, ..., \\beta_p\\) in the logistic regression equation hold crucial information about the impact of each predictor variable on the log-odds of the event occurring. To interpret these coefficients, we need to consider their signs, magnitude, and statistical significance.\n\nSign of Coefficients:\n\nThe sign of a coefficient reveals the direction of the relationship between the predictor variable and the log-odds of the event occurring. A positive coefficient (\\(\\beta &gt; 0\\)) indicates that an increase in the predictor variable leads to an increase in the log-odds (probability) of the event happening. Conversely, a negative coefficient (\\(\\beta &lt; 0\\)) suggests that an increase in the predictor variable is associated with a decrease in the log-odds (probability) of the event occurring.\n\nMagnitude of Coefficients:\n\nThe magnitude of a coefficient provides information about the strength of the relationship between the predictor variable and the log-odds of the event. Larger absolute values of coefficients indicate stronger influences on the probability.\n\nStatistical Significance:\n\nEvaluating the statistical significance of coefficients is essential to determine if the relationship between a predictor variable and the event probability is meaningful or just due to random chance. Researchers often use hypothesis testing or confidence intervals to assess the statistical significance of coefficients."
  },
  {
    "objectID": "posts/Logistic Regression and Marginal Effects.html#marginal-effects",
    "href": "posts/Logistic Regression and Marginal Effects.html#marginal-effects",
    "title": "Logistic Regression and Marginal Effects",
    "section": "",
    "text": "Marginal effects can be described as the change in outcome as a function of the change in the independent variable of interest holding all other variables in the model constant.\n\nNOTE: In linear regression, the estimated regression coefficients are marginal effects and are more easily interpreted.*\n\nOne measure of change in a system is that the rate of change of the system is non-zero.\nIf we had a simple linear regression model for a trend, then the estimated rate of change would be the slope of the line. Technically, this is the instantaneous rate of change of the function that defines the line.\nThe idea can be extended to any function, even one as potentially complex non-linear function.\nThe problem we have is that in general we don‚Äôt have an equation for the non-linear from which we can derive the derivatives.\nSo how do we estimate the derivative of a non-linear function ?\nOne solution is to use the method of finite differences but to understand it properly we need to introduce first the basic concept of differential calculus."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "üéôÔ∏èBlog",
    "section": "",
    "text": "Unraveling the Power of Causal Machine Learning\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nQuantile Random Forest\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nEmbracing Change: Incremental vs.¬†Batch Machine Learning\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nLogistic Regression and Marginal Effects\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nContextual Multi-Armed Bandit: Maximizing Rewards with Intelligent Decision-Making\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWhen in doubt, just model it. Modelling uncertainty\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nDeveloping in a Docker container\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nData Science Books\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nData Science project Boilerplate\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBayesian updating\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#incremental-machine-learning-an-ever-evolving-journey",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#incremental-machine-learning-an-ever-evolving-journey",
    "title": "Embracing Change: Incremental vs.¬†Batch Machine Learning",
    "section": "",
    "text": "Imagine a learning process that continuously adapts to new information, always striving to stay up-to-date with the latest trends. That‚Äôs incremental machine learning for you! Also known as online learning or streaming learning, this approach updates the model in real-time as fresh data arrives. Unlike batch learning, which requires processing the entire dataset at once, incremental learning embraces change, incorporating new observations into the existing model as they arrive.\n\n\n\nReal-Time Adaptation: The beauty of incremental learning lies in its ability to quickly respond to shifts in data distribution. This makes it ideal for applications dealing with rapidly evolving data, like online advertising or fraud detection.\nLow Memory Requirements: Incremental learning processes data in small batches or individual data points, reducing memory usage compared to batch learning, where the entire dataset needs to be stored.\nConstant Learning: With incremental learning, models can continuously learn and improve over time without the need for retraining on the entire dataset. It‚Äôs like a lifelong learning journey!\n\nAdvantages of Incremental Machine Learning\n\nEfficiency: Incremental learning is computationally efficient since it updates the model incrementally, without the need to retrain on the entire dataset each time.\nScalability: This approach is well-suited for large-scale applications where processing the entire dataset at once would be overwhelming.\nDynamic Adaptation: Incremental learning is ideal for applications where data distribution changes frequently, such as natural language processing or recommendation systems. It embraces the unexpected!\n\nDisadvantages of Incremental Machine Learning\n\nForgetting Old Data: Just like we can‚Äôt remember everything we‚Äôve learned in the past, incremental learning models may forget information from the past, leading to potential performance degradation on older data points.\nModel Drift: In dynamic environments, incremental learning models may struggle to adapt to sudden shifts in data patterns, leading to inaccuracies. Sometimes, change can be overwhelming!"
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#batch-machine-learning-a-journey-with-stability",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#batch-machine-learning-a-journey-with-stability",
    "title": "Embracing Change: Incremental vs.¬†Batch Machine Learning",
    "section": "",
    "text": "Now, let‚Äôs switch gears and explore batch machine learning. Picture a learning process that takes time to analyze and understand the whole story before making decisions. That‚Äôs batch machine learning for you! This approach involves training the model on the entire dataset and updating it periodically or when significant amounts of new data accumulate.\n\n\n\nPeriodic Updates: Batch learning involves training models from scratch on a fixed dataset, leveraging powerful hardware resources and sophisticated algorithms. It‚Äôs like taking a step back to see the bigger picture!\nComplete Data Usage: Unlike incremental learning, which processes data in smaller chunks, batch learning uses the entire dataset for training, ensuring no data points are left behind.\n\nAdvantages of Batch Machine Learning\n\nStrong Convergence: By training on the entire dataset, batch learning often achieves more stable and reliable results, especially with sufficient computational resources. It‚Äôs like taking the time to contemplate before making decisions.\nAccuracy: Batch learning models tend to be more accurate due to their exhaustive training process. Sometimes, thoroughness pays off!\n\nDisadvantages of Batch Machine Learning\n\nHigh Resource Demands: Batch learning requires substantial computational power and memory, making it less scalable for real-time applications or large datasets.\nTime Lag in Updates: Models may not adapt quickly to changes in data distribution since they are updated only after accumulating new data. Patience is key!"
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#advantages-of-batch-machine-learning",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#advantages-of-batch-machine-learning",
    "title": "Embracing Change: Incremental vs.¬†Batch Machine Learning",
    "section": "",
    "text": "Strong Convergence: By training on the entire dataset, batch learning often achieves more stable and reliable results, especially with sufficient computational resources. It‚Äôs like taking the time to contemplate before making decisions.\nAccuracy: Batch learning models tend to be more accurate due to their exhaustive training process. Sometimes, thoroughness pays off!"
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#disadvantages-of-batch-machine-learning",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#disadvantages-of-batch-machine-learning",
    "title": "Embracing Change: Incremental vs.¬†Batch Machine Learning",
    "section": "",
    "text": "High Resource Demands: Batch learning requires substantial computational power and memory, making it less scalable for real-time applications or large datasets.\nTime Lag in Updates: Models may not adapt quickly to changes in data distribution since they are updated only after accumulating new data. Patience is key!\n\nUse Cases for Incremental and Batch Machine Learning"
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#incremental-learning",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#incremental-learning",
    "title": "Embracing Change: Incremental vs.¬†Batch Machine Learning",
    "section": "",
    "text": "Real-time fraud detection in financial transactions.\nAdaptive recommendation systems that learn from user interactions in real-time.\nSentiment analysis on streaming social media data.\n\nBatch Learning:\n\nImage classification in computer vision tasks.\nNatural language processing applications like language translation.\nTraining large-scale recommendation systems periodically."
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#conclusion",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#conclusion",
    "title": "Embracing Change: Incremental vs.¬†Batch Machine Learning",
    "section": "",
    "text": "As we bid farewell to this exploration, we have gained an understanding of the intriguing differences between incremental and batch machine learning. Incremental learning embraces change and adapts to evolving data, making it perfect for real-time applications. On the other hand, batch learning takes a more measured approach, achieving strong convergence and accuracy with thorough analysis. Both methods have their unique strengths, empowering data scientists and developers to make informed choices based on the demands of their applications. So, whether you‚Äôre venturing on a dynamic journey with incremental learning or taking a stable path with batch learning, remember to embrace the wonders of machine learning and let it lead you to exciting discoveries!"
  },
  {
    "objectID": "posts/Unraveling the Power of Causal Machine Learning.html#the-challenge-of-causal-inference",
    "href": "posts/Unraveling the Power of Causal Machine Learning.html#the-challenge-of-causal-inference",
    "title": "Unraveling the Power of Causal Machine Learning",
    "section": "",
    "text": "Establishing causality is not a straightforward task. The fundamental problem lies in the fact that we cannot simultaneously observe both the treatment group (those who receive a particular intervention) and the control group (those who do not). Traditional observational data often suffers from confounding variables, making it challenging to disentangle cause and effect."
  },
  {
    "objectID": "posts/Unraveling the Power of Causal Machine Learning.html#enter-causal-machine-learning",
    "href": "posts/Unraveling the Power of Causal Machine Learning.html#enter-causal-machine-learning",
    "title": "Unraveling the Power of Causal Machine Learning",
    "section": "",
    "text": "Causal Machine Learning, a fascinating interdisciplinary field, bridges the gap between traditional machine learning and causal inference. Its primary goal is to leverage data to identify causal relationships and infer the effects of interventions accurately."
  },
  {
    "objectID": "posts/Unraveling the Power of Causal Machine Learning.html#causal-inference-methods",
    "href": "posts/Unraveling the Power of Causal Machine Learning.html#causal-inference-methods",
    "title": "Unraveling the Power of Causal Machine Learning",
    "section": "",
    "text": "Randomized Control Trials (RCTs): Considered the gold standard for establishing causality, RCTs involve randomly assigning subjects to either the treatment or control group. By eliminating confounding factors, RCTs provide strong causal evidence. However, they may not always be practical or ethical.\nPropensity Score Matching (PSM): When RCTs are not feasible, PSM is a popular method. It attempts to create a ‚Äúquasi-experimental‚Äù design by matching treated and control units based on their propensity scores, which represent the likelihood of receiving the treatment. This helps mitigate confounding effects.\nInstrumental Variables (IV): IV analysis relies on instrumental variables that are correlated with the treatment but have no direct effect on the outcome. By utilizing these instruments, researchers can uncover causal relationships even in the presence of unobserved confounders.\nSynthetic Control (SC): The process of creating a synthetic counterfactual involves using historical data from similar individuals or groups that did not receive the treatment to construct a ‚Äúsynthetic‚Äù control group. This control group is designed to closely match the characteristics of the treated group before the intervention. By doing so, researchers aim to create a plausible estimate of what would have happened to the treated group had they not received the treatment."
  },
  {
    "objectID": "posts/Unraveling the Power of Causal Machine Learning.html#real-world-applications",
    "href": "posts/Unraveling the Power of Causal Machine Learning.html#real-world-applications",
    "title": "Unraveling the Power of Causal Machine Learning",
    "section": "",
    "text": "Causal Machine Learning has found applications in various domains, including public health, economics, social sciences, and marketing. It enables policymakers to make informed decisions, businesses to optimize interventions, and researchers to uncover hidden causal mechanisms."
  },
  {
    "objectID": "posts/Unraveling the Power of Causal Machine Learning.html#embracing-causal-machine-learning",
    "href": "posts/Unraveling the Power of Causal Machine Learning.html#embracing-causal-machine-learning",
    "title": "Unraveling the Power of Causal Machine Learning",
    "section": "",
    "text": "As the world becomes more data-driven, understanding causality becomes paramount. Embracing causal machine learning techniques empowers us to make smarter decisions, avoid biases, and design better interventions that have a genuine impact on our lives.\nAn excellent example application of Causal Machine Learning is in the field of healthcare for personalized treatment recommendations or to estimate the impact of marketing campaigns. Beyond the classic RCT or A/B testing, in both cases the use of Reinforcement Learning is a very interesting application that we will discuss in the next blog post."
  },
  {
    "objectID": "posts/Quantile Random Forest.html#random-forest-vs-qrf",
    "href": "posts/Quantile Random Forest.html#random-forest-vs-qrf",
    "title": "Quantile Random Forest",
    "section": "",
    "text": "The key difference between Random Forest and QRF lies in the prediction phase. In Random Forest, the predicted value for a new data point is the average (mean) of the individual tree predictions. However, in QRF, we aim to estimate multiple quantiles of the target variable‚Äôs distribution for a given input.\nWhen growing a tree in QRF, instead of splitting the data based solely on minimizing the variance or mean squared error, QRF performs splits to optimize quantile-specific criteria. Each tree node is split to minimize the quantile loss function, which reflects the error in predicting the specific quantile. This means that the tree seeks to minimize the difference between the true quantile value and the estimated quantile value for the data points that fall into each node.\nOnce the tree is fully grown, the terminal nodes (leaf nodes) contain specific quantile values rather than just the average value. Each terminal node in the QRF stores a quantile-specific estimate based on the data points that fall into that node. This means that each tree provides quantile-specific predictions.\nTo make predictions for a new data point, QRF evaluates the input through each decision tree, resulting in a set of quantile-specific predictions. The user can then choose the desired quantiles (e.g., 0.25, 0.50, 0.75) to obtain the corresponding quantile predictions. For example, if we want to estimate the 0.75 quantile, we collect the 0.75 quantile values from each tree, and these values form the quantile prediction for that data point.\nOne of the advantages of QRF is that it provides a measure of uncertainty in the predictions. Given the estimated quantiles, QRF can construct a prediction interval for each data point. The prediction interval provides a range of possible values, accounting for the uncertainty in the prediction due to the variability of the underlying data."
  },
  {
    "objectID": "posts/Quantile Random Forest.html#advantages",
    "href": "posts/Quantile Random Forest.html#advantages",
    "title": "Quantile Random Forest",
    "section": "",
    "text": "One of the advantages of QRF is that it provides a measure of uncertainty in the predictions. Given the estimated quantiles, QRF can construct a prediction interval for each data point. The prediction interval provides a range of possible values, accounting for the uncertainty in the prediction due to the variability of the underlying data.\nAdvantages of Quantile Random Forest\n\nRobustness to outliers: QRF can handle outliers effectively as it estimates the entire distribution, including extreme quantiles.\nProvides uncertainty information: By estimating quantiles, QRF offers a measure of uncertainty in the predictions, which is valuable in decision-making.\nVersatility: QRF can be applied to regression and quantile regression problems, and it can be extended to handle multivariate quantile estimation.\n\nApplications of Quantile Random Forest\n\nFinancial forecasting: QRF can be used to predict quantiles of financial variables, such as stock prices or asset returns, to assess the risk of investment decisions.\nMedical applications: In healthcare, QRF can estimate quantiles of patient response times to treatments, enabling doctors to tailor treatments accordingly.\nEnvironmental modeling: QRF can estimate quantiles of environmental variables like pollutant concentrations, assisting in environmental risk assessments."
  },
  {
    "objectID": "posts/Contextual Multi-Armed Bandit.html",
    "href": "posts/Contextual Multi-Armed Bandit.html",
    "title": "Contextual Multi-Armed Bandit: Maximizing Rewards with Intelligent Decision-Making",
    "section": "",
    "text": "Imagine a gambler facing a row of slot machines (arms), each with an unknown payout probability. The gambler‚Äôs goal is to maximize their cumulative reward while exploring the machines‚Äô potential and exploiting the most promising ones. In recent years, the contextual multi-armed bandit (CMAB) has emerged as a powerful extension to this problem, incorporating context or additional information to make even more intelligent decisions. In this blog post, we will delve into the world of contextual multi-armed bandits, exploring their significance, strategies, and real-world applications.\n\n\nIn the classic multi-armed bandit problem, a gambler is faced with a set of arms (slot machines) that have different reward probabilities, but the gambler doesn‚Äôt know these probabilities initially. The gambler‚Äôs objective is to determine which arms to pull (explore) and which to favor (exploit) over time to maximize their cumulative reward. The challenge lies in striking a balance between exploring new arms to gather more information and exploiting the arms that seem to provide higher rewards.\n\n\n\nThe contextual multi-armed bandit takes the classic problem a step further by incorporating additional information or context. In CMAB, each arm is associated with a context, which can be thought of as features or attributes characterizing the current situation. This context provides valuable insights into the arms‚Äô potential rewards and helps the decision-maker make more informed choices.\nFor instance, imagine a digital advertising scenario where ads (arms) are shown to users, and each ad has associated user characteristics as context, such as age, location, and interests. By considering the context, the CMAB algorithm can optimize the selection of ads for each user to maximize clicks or conversions.\n\n\n\nLinUCB Algorithm:\n\nOne popular approach for solving CMAB problems is the LinUCB algorithm. It uses linear models to estimate the expected rewards for each arm given the context. The algorithm maintains a confidence interval for each arm‚Äôs expected reward and selects the arm with the highest upper confidence bound, balancing exploration and exploitation.\n\nThompson Sampling:\n\nThompson Sampling is a widely used Bayesian approach for CMAB. It models the reward distribution for each arm based on observed rewards and context. The algorithm then samples from these distributions and selects the arm with the highest sample. This method effectively leverages uncertainty to explore and exploit in a principled manner.\nReal-World Applications of Contextual Multi-Armed Bandits\n\nPersonalized Recommendations:\n\nCMAB algorithms can be applied to personalized recommendation systems, where the context represents user preferences, past behavior, and demographics. By dynamically selecting the most relevant content or products, these systems can improve user engagement and satisfaction.\n\nHealthcare Treatment Selection:\n\nIn healthcare, CMAB can help optimize treatment selection based on patient characteristics, medical history, and response to previous treatments. By considering the context, doctors can make more data-driven decisions to improve patient outcomes.\n\nOnline Advertising:\n\nIn digital advertising, CMAB can enhance ad placement by taking into account user profiles, browsing behavior, and real-time context. This enables advertisers to show the most relevant ads to users, increasing click-through rates and conversions."
  },
  {
    "objectID": "posts/Contextual Multi-Armed Bandit.html#strategies-for-contextual-multi-armed-bandits",
    "href": "posts/Contextual Multi-Armed Bandit.html#strategies-for-contextual-multi-armed-bandits",
    "title": "Contextual Multi-Armed Bandit: Maximizing Rewards with Intelligent Decision-Making",
    "section": "",
    "text": "LinUCB Algorithm:\n\nOne popular approach for solving CMAB problems is the LinUCB algorithm. It uses linear models to estimate the expected rewards for each arm given the context. The algorithm maintains a confidence interval for each arm‚Äôs expected reward and selects the arm with the highest upper confidence bound, balancing exploration and exploitation.\n\nThompson Sampling:\n\nThompson Sampling is a widely used Bayesian approach for CMAB. It models the reward distribution for each arm based on observed rewards and context. The algorithm then samples from these distributions and selects the arm with the highest sample. This method effectively leverages uncertainty to explore and exploit in a principled manner.\nReal-World Applications of Contextual Multi-Armed Bandits\n\nPersonalized Recommendations:\n\nCMAB algorithms can be applied to personalized recommendation systems, where the context represents user preferences, past behavior, and demographics. By dynamically selecting the most relevant content or products, these systems can improve user engagement and satisfaction.\n\nHealthcare Treatment Selection:\n\nIn healthcare, CMAB can help optimize treatment selection based on patient characteristics, medical history, and response to previous treatments. By considering the context, doctors can make more data-driven decisions to improve patient outcomes.\n\nOnline Advertising:\n\nIn digital advertising, CMAB can enhance ad placement by taking into account user profiles, browsing behavior, and real-time context. This enables advertisers to show the most relevant ads to users, increasing click-through rates and conversions."
  },
  {
    "objectID": "posts/When in doubt, just model it. Modelling uncertainty.html",
    "href": "posts/When in doubt, just model it. Modelling uncertainty.html",
    "title": "When in doubt, just model it. Modelling uncertainty",
    "section": "",
    "text": "Probabilistic modeling is a powerful approach in statistics and machine learning that enables us to quantify uncertainty in our predictions and decisions. It allows us to represent and reason about uncertain or incomplete information by incorporating probability distributions over the model parameters or the outcomes themselves. In this context, we can distinguish between two types of uncertainty: uncertainty and deep uncertainty."
  },
  {
    "objectID": "posts/When in doubt, just model it. Modelling uncertainty.html#uncertainty",
    "href": "posts/When in doubt, just model it. Modelling uncertainty.html#uncertainty",
    "title": "When in doubt, just model it. Modelling uncertainty",
    "section": "Uncertainty",
    "text": "Uncertainty\nUncertainty refers to the lack of perfect knowledge about a particular event or outcome. In probabilistic modeling, uncertainty is captured by probability distributions. These distributions represent our beliefs about the likelihood of different outcomes or the variability in model parameters. By using probability distributions, we can express the confidence or lack thereof in our predictions.\nFor example, in linear regression, instead of providing a single point estimate for the model coefficients, we can use a probability distribution to describe the uncertainty around these coefficients. This allows us to understand the range of plausible values and the level of confidence we have in our estimates.\nProbabilistic modeling allows us to make more robust decisions by considering the uncertainty and accounting for potential errors in our predictions. It is widely used in various applications, including finance, healthcare, weather forecasting, and natural language processing."
  },
  {
    "objectID": "posts/When in doubt, just model it. Modelling uncertainty.html#deep-uncertainty",
    "href": "posts/When in doubt, just model it. Modelling uncertainty.html#deep-uncertainty",
    "title": "When in doubt, just model it. Modelling uncertainty",
    "section": "Deep Uncertainty",
    "text": "Deep Uncertainty\nDeep uncertainty, on the other hand, goes beyond simple uncertainty and arises when we have limited knowledge about the underlying data-generating process or when there is ambiguity in the model assumptions. In such cases, traditional probabilistic models may not be sufficient to capture the full extent of uncertainty.\nDeep uncertainty is particularly prevalent in complex and chaotic systems, where there are many interacting variables and non-linear relationships that are difficult to model precisely. In these scenarios, traditional probabilistic models may yield unreliable estimates or may not provide meaningful uncertainty quantification.\nTo address deep uncertainty, researchers have developed alternative methods, such as robust optimization, scenario-based analysis, and Bayesian non-parametric models. These techniques are designed to handle situations where the underlying assumptions are uncertain or where we lack sufficient data to make accurate probabilistic estimates.\nIn recent years, the development of deep learning and Bayesian deep learning has also allowed us to tackle deep uncertainty in more complex models. Bayesian neural networks, for example, use Bayesian inference to represent uncertainty in neural network weights, enabling better uncertainty quantification in deep learning models."
  },
  {
    "objectID": "posts/Logistic Regression and Marginal Effects.html#logistic-regression",
    "href": "posts/Logistic Regression and Marginal Effects.html#logistic-regression",
    "title": "Logistic Regression and Marginal Effects",
    "section": "",
    "text": "In logistic regression, we model the log-odds of the event of interest occurring as a linear combination of predictor variables. The logistic function (sigmoid function) is then applied to this linear combination to convert it into a probability value between 0 and 1.\nThe logistic regression equation is given by:\n\\[\\text{logit}(P(Y=1)) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p\\]\nWhere:\n\n\\(P(Y=1)\\) is the probability of the event of interest (e.g., success) occurring.\n\\(logit (P(Y=1))\\) is the log-odds (logit) of the event occurring.\n\\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_p\\) are the coefficients of the model.\n\\(X_1, X_2, ..., X_p\\) are the predictor variables."
  },
  {
    "objectID": "posts/Logistic Regression and Marginal Effects.html#practical-example",
    "href": "posts/Logistic Regression and Marginal Effects.html#practical-example",
    "title": "Logistic Regression and Marginal Effects",
    "section": "Practical Example",
    "text": "Practical Example\nLet‚Äôs consider an example to illustrate the interpretation of coefficients in logistic regression. Suppose we are studying the factors that influence whether a customer will make a purchase on an e-commerce website. Our logistic regression model includes two predictor variables: ‚ÄúTime Spent on Website‚Äù and ‚ÄúNumber of Items Added to Cart.‚Äù The estimated coefficients are as follows:\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\)\n\\(\\beta_{\\text{Number of Items}} = 0.2\\)\n\n\nInterpretation\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\): For each additional minute a customer spends on the website, the log-odds (probability) of making a purchase increases by 0.05 units. This means that spending more time on the website is associated with a higher likelihood of making a purchase.\n\\(\\beta_{\\text{Number of Items}} = 0.2\\): For each additional item a customer adds to the cart, the log-odds (probability) of making a purchase increases by 0.2 units. This suggests that adding more items to the cart is associated with a higher probability of making a purchase."
  },
  {
    "objectID": "posts/Contextual Multi-Armed Bandit.html#the-classic-multi-armed-bandit-problem",
    "href": "posts/Contextual Multi-Armed Bandit.html#the-classic-multi-armed-bandit-problem",
    "title": "Contextual Multi-Armed Bandit: Maximizing Rewards with Intelligent Decision-Making",
    "section": "",
    "text": "In the classic multi-armed bandit problem, a gambler is faced with a set of arms (slot machines) that have different reward probabilities, but the gambler doesn‚Äôt know these probabilities initially. The gambler‚Äôs objective is to determine which arms to pull (explore) and which to favor (exploit) over time to maximize their cumulative reward. The challenge lies in striking a balance between exploring new arms to gather more information and exploiting the arms that seem to provide higher rewards."
  },
  {
    "objectID": "posts/Contextual Multi-Armed Bandit.html#introducing-contextual-multi-armed-bandit",
    "href": "posts/Contextual Multi-Armed Bandit.html#introducing-contextual-multi-armed-bandit",
    "title": "Contextual Multi-Armed Bandit: Maximizing Rewards with Intelligent Decision-Making",
    "section": "",
    "text": "The contextual multi-armed bandit takes the classic problem a step further by incorporating additional information or context. In CMAB, each arm is associated with a context, which can be thought of as features or attributes characterizing the current situation. This context provides valuable insights into the arms‚Äô potential rewards and helps the decision-maker make more informed choices.\nFor instance, imagine a digital advertising scenario where ads (arms) are shown to users, and each ad has associated user characteristics as context, such as age, location, and interests. By considering the context, the CMAB algorithm can optimize the selection of ads for each user to maximize clicks or conversions.\n\n\n\nLinUCB Algorithm:\n\nOne popular approach for solving CMAB problems is the LinUCB algorithm. It uses linear models to estimate the expected rewards for each arm given the context. The algorithm maintains a confidence interval for each arm‚Äôs expected reward and selects the arm with the highest upper confidence bound, balancing exploration and exploitation.\n\nThompson Sampling:\n\nThompson Sampling is a widely used Bayesian approach for CMAB. It models the reward distribution for each arm based on observed rewards and context. The algorithm then samples from these distributions and selects the arm with the highest sample. This method effectively leverages uncertainty to explore and exploit in a principled manner.\nReal-World Applications of Contextual Multi-Armed Bandits\n\nPersonalized Recommendations:\n\nCMAB algorithms can be applied to personalized recommendation systems, where the context represents user preferences, past behavior, and demographics. By dynamically selecting the most relevant content or products, these systems can improve user engagement and satisfaction.\n\nHealthcare Treatment Selection:\n\nIn healthcare, CMAB can help optimize treatment selection based on patient characteristics, medical history, and response to previous treatments. By considering the context, doctors can make more data-driven decisions to improve patient outcomes.\n\nOnline Advertising:\n\nIn digital advertising, CMAB can enhance ad placement by taking into account user profiles, browsing behavior, and real-time context. This enables advertisers to show the most relevant ads to users, increasing click-through rates and conversions."
  },
  {
    "objectID": "posts/Quantile Random Forest.html#conclusion",
    "href": "posts/Quantile Random Forest.html#conclusion",
    "title": "Quantile Random Forest",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, Quantile Random Forest is a powerful extension of the Random Forest algorithm that estimates the entire distribution of the target variable through quantile-specific predictions. It is useful when capturing the entire distribution or quantifying uncertainty is essential for making informed decisions in various applications such as finance, healthcare, and environmental modeling."
  },
  {
    "objectID": "posts/Quantile Random Forest.html#advantages-of-quantile-random-forest",
    "href": "posts/Quantile Random Forest.html#advantages-of-quantile-random-forest",
    "title": "Quantile Random Forest",
    "section": "Advantages of Quantile Random Forest",
    "text": "Advantages of Quantile Random Forest\n\nRobustness to outliers: QRF can handle outliers effectively as it estimates the entire distribution, including extreme quantiles.\nProvides uncertainty information: By estimating quantiles, QRF offers a measure of uncertainty in the predictions, which is valuable in decision-making.\nVersatility: QRF can be applied to regression and quantile regression problems, and it can be extended to handle multivariate quantile estimation."
  },
  {
    "objectID": "posts/Quantile Random Forest.html#applications-of-quantile-random-forest",
    "href": "posts/Quantile Random Forest.html#applications-of-quantile-random-forest",
    "title": "Quantile Random Forest",
    "section": "Applications of Quantile Random Forest",
    "text": "Applications of Quantile Random Forest\n\nFinancial forecasting: QRF can be used to predict quantiles of financial variables, such as stock prices or asset returns, to assess the risk of investment decisions.\nMedical applications: In healthcare, QRF can estimate quantiles of patient response times to treatments, enabling doctors to tailor treatments accordingly.\nEnvironmental modeling: QRF can estimate quantiles of environmental variables like pollutant concentrations, assisting in environmental risk assessments."
  },
  {
    "objectID": "posts/Logistic Regression and Marginal Effects.html#interpretation",
    "href": "posts/Logistic Regression and Marginal Effects.html#interpretation",
    "title": "Logistic Regression and Marginal Effects",
    "section": "Interpretation",
    "text": "Interpretation\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\): For each additional minute a customer spends on the website, the log-odds (probability) of making a purchase increases by 0.05 units. This means that spending more time on the website is associated with a higher likelihood of making a purchase.\n\\(\\beta_{\\text{Number of Items}} = 0.2\\): For each additional item a customer adds to the cart, the log-odds (probability) of making a purchase increases by 0.2 units. This suggests that adding more items to the cart is associated with a higher probability of making a purchase."
  },
  {
    "objectID": "posts/Logistic Regression and Marginal Effects.html#interpretation-issues",
    "href": "posts/Logistic Regression and Marginal Effects.html#interpretation-issues",
    "title": "Logistic Regression and Marginal Effects",
    "section": "Interpretation Issues",
    "text": "Interpretation Issues\nIdeally, we want to understand what the model is telling us on the probability scale and not in the odds scale, much less in the estimation scale, the log-odds.\nWe might be tempted to convert log-odds to odds and then to probability and interpret everything as usual. But this would be very wrong!\nWhy?\nThe issue here is that on the probability scale the function is non-linear!"
  },
  {
    "objectID": "posts/Logistic Regression and Marginal Effects.html#finite-differences",
    "href": "posts/Logistic Regression and Marginal Effects.html#finite-differences",
    "title": "Logistic Regression and Marginal Effects",
    "section": "Finite differences",
    "text": "Finite differences\nDifferential calculus is a branch of calculus that focuses on the study of rates of change and slopes of curves. It deals with the concept of derivatives, which represent the rate of change of a function at a specific point. Derivatives allow us to understand how a function is changing at a particular location and provide valuable insights into the behavior of functions.\nThe derivative of a function \\(f(x)\\) with respect to \\(x\\) is denoted as \\(f'(x)\\) or \\(\\frac{df(x)}{dx}\\). It represents the slope of the tangent line to the graph of the function at a given point \\(x\\). The derivative can be interpreted as the instantaneous rate of change of \\(f(x)\\) with respect to \\(x\\).\nFor example, consider the function \\(f(x) = x^2\\). The derivative of \\(f(x)\\) is \\(f'(x) = 2x\\). This means that the slope of the tangent line to the graph of \\(f(x)\\) at any point \\(x\\) is twice the value of \\(x\\) at that point.\nDifferential calculus has wide-ranging applications in various fields, including physics, engineering, economics, and more. It is used to model and analyze phenomena that involve continuous changes, such as motion, growth, and optimization.\nFinite differences are a numerical technique used to approximate derivatives of functions. The method involves computing the difference in function values at two nearby points and then dividing by the difference in their corresponding independent variable values. By using a small interval between the two points, we can get an approximation of the derivative at a specific point.\nThe finite difference formula for approximating the first derivative of a function \\(f(x)\\) at a point \\(x\\) is given by:\n\\[ f‚Äô(x) \\approx \\frac{f(x + h) - f(x)}{h} \\]\nWhere:\n\n\\(h\\) is a small value representing the step size or interval between the points.\n\nSimilarly, we can use finite differences to approximate higher-order derivatives, such as the second derivative:\n\\[ f‚Äô‚Äô(x) \\approx \\frac{f(x + h) - 2f(x) + f(x - h)}{h^2} \\]\nFinite differences are particularly useful when the analytical expression for the derivative is difficult to obtain or when dealing with discrete data points instead of continuous functions.\nDifferential calculus and finite differences are both essential tools in mathematics and scientific disciplines. Differential calculus provides a rigorous framework for studying rates of change and slopes of curves through derivatives. On the other hand, finite differences offer a practical and numerical approach to approximate derivatives when the analytical solution is not readily available or when dealing with discrete data. Together, these concepts enable us to better understand the behavior of functions and analyze real-world phenomena involving continuous and discrete changes."
  },
  {
    "objectID": "posts/Developing in a Docker container.html",
    "href": "posts/Developing in a Docker container.html",
    "title": "Developing in a Docker container",
    "section": "",
    "text": "Developing within a Docker container has become a common practice in the software development workflow. It offers various benefits, such as ensuring consistent environments, reproducibility, and isolation.\nUsing docker means not having to have complex software server applications installed locally. Instead, they are run in ‚Äòsealed‚Äô containers, held discrete from the local computer. In an example Use Case of developing code in Python, for example, a docker container holding the latest version of Python, plus the Python script can be built and run. However, in this use case, to edit the Python script and rerun it, the image container needs to be rebuilt in docker each time there are changes made to the code. Docker can use ‚Äòbind mounts‚Äô to circumvent this for development purposes. The folder holding the source code is ‚Äòmounted‚Äô into the container image, meaning that the source code can then be edited and run immediately in the image without needing to be rebuilt.\nCombining Docker with a bind mount in VScode makes life so much easier. Here‚Äôs a step-by-step guide to get started with developing in a Docker container:\n\nInstall Docker: Make sure you have Docker installed on your system. You can download and install it from the official Docker website based on your operating system. 2. Create a Dockerfile: A Dockerfile is a text file that contains instructions for building a Docker image. Create a new file named Dockerfile in your project‚Äôs root directory. Here‚Äôs a basic example of a Python project:\n\n# Use an official Python runtime as the base image\nFROM python:3.10.6-buster\n\nCOPY requirements.txt /requirements.txt\n\nRUN pip install --upgrade pip RUN pip install -r requirements.txt\n\n# Set the working directory\n\nWORKDIR /app\n\n# Define the command to run when the container starts\n\nCMD \\[\"bash\"\\]\n\nBuild the Docker Image: Open a terminal, navigate to your project‚Äôs directory containing the Dockerfile, and run the following command to build the Docker image:\n\ndocker build -t myapp .\nHere, myapp is the name you‚Äôre giving to your Docker image, and the dot . indicates the build context (current directory).\n\nRun the Docker Container: After building the image, you can run a container from it using the following command:\n\ndocker run -d -it --hostname yourname -v /path/to/folder/holding/the/source/code:/app myapp\nHere, -d runs the container in detached mode, -it allocates a pseudo-TTY and keeps STDIN open even if not attached, -- hostname simply indicates the container‚Äôs hostname , finally -v will bind mount a volume following given its path.\n\nConnect with VScode: Now that you have your development environment inside a Docker container, you can edit your code directly inside the container using VScode. Changes you make to your code will be reflected within the container. This way, you can edit code on your host machine and see the changes in the container immediately.\nTo do so, you will need to install the docker extension as shown below.\n\n\nOnce you have installed the extension, go on the docker icon in the left side bar and this is what you will see.\n\nAnd here is the app folder inside the container.\n\nFinally you just need to right click on the container and you will see th option to Attach Visual Studio Code. This will open a new VScode window. Now you are inside the container you can start to develop. All changes you will made to the files in the attached folder will affect the local folder as well."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "üìΩÔ∏èÔ∏èSlides",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/Data Science Books.html",
    "href": "posts/Data Science Books.html",
    "title": "Data Science Books",
    "section": "",
    "text": "Whether you‚Äôre a seasoned data scientist looking to expand your knowledge or a newcomer eager to dive into this exciting field, I made this curated list of must-read books that will support you throughout your career or journey with data, statistics, and machine learning.\n\n\n\n\n\n\n\n‚ÄúIntroductory Econometrics: A Modern Approach‚Äù by Jeffrey M. Wooldridge is a widely acclaimed textbook that serves as a comprehensive introduction to the field of econometrics. This book is a valuable resource for students, researchers, and practitioners interested in using statistical methods to analyze economic data. Wooldridge‚Äôs approach is modern and intuitive, making complex econometric concepts accessible to readers at various levels of expertise.\nThe book covers key topics such as regression analysis, hypothesis testing and more advanced teniques such as instrumental variables, panel data analysis, and time series analysis. What sets it apart is its focus on real-world applications, using examples and datasets from various fields of economics to illustrate the concepts discussed. This practical approach helps readers bridge the gap between theory and practice, preparing them to conduct meaningful empirical research in economics.\nFurthermore, Wooldridge‚Äôs writing style is clear and engaging, making it easier for readers to grasp challenging econometric concepts. With its emphasis on modern techniques and hands-on applications, ‚ÄúIntroductory Econometrics: A Modern Approach‚Äù has become a trusted resource in the field, helping students and researchers develop the skills needed to analyze and interpret economic data effectively.\n\n\n\n\n\n\n\n\n‚ÄúIntroduction to Machine Learning with Python: A Guide for Data Scientists‚Äù by Andreas C. M√ºller and Sarah Guido is a highly regarded and practical book that provides an excellent introduction to machine learning concepts and their implementation using Python. This book is particularly beneficial for data scientists, engineers, and anyone interested in learning the fundamentals of machine learning in a hands-on manner.\nThe book covers a wide range of machine learning topics, including supervised learning, unsupervised learning, and deep learning. It also introduces essential libraries and tools for machine learning in Python, such as scikit-learn and TensorFlow. One of the strengths of this book is its focus on practical examples and code snippets, which allow readers to apply what they learn immediately.\nOverall, ‚ÄúIntroduction to Machine Learning with Python‚Äù is a valuable resource for those looking to gain a solid foundation in machine learning while leveraging the power of the Python programming language. It‚Äôs suitable for both beginners and intermediate learners and provides a practical and hands-on approach to mastering the essentials of machine learning for data science.\n\n\n\n\n\n\n\n\n‚ÄúHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems‚Äù by Aur√©lien G√©ron is a popular and comprehensive book that provides a practical guide to building machine learning models using three widely-used libraries: scikit-learn, Keras, and TensorFlow. The book is designed for both beginners and experienced practitioners in the field of machine learning and artificial intelligence.\nThe book is known for its clarity, practicality, and hands-on approach. It has been praised for its ability to take readers from the fundamentals of machine learning to building and deploying sophisticated machine learning models. Whether you‚Äôre a beginner looking to learn the basics or an experienced data scientist seeking to expand your knowledge, ‚ÄúHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow‚Äù is a valuable resource for anyone interested in the field of machine learning.\n\n\n\n\n\n\n\n\n‚ÄúDeep Learning with Python‚Äù by Fran√ßois Chollet is a highly regarded book that serves as a practical guide to understanding and implementing deep learning techniques using the Python programming language and the Keras deep learning framework. Fran√ßois Chollet is the creator of Keras, which has become a popular tool for building deep neural networks due to its simplicity and flexibility.\nThis must read book is known for its accessibility and practical approach. It‚Äôs suitable for readers with varying levels of experience in deep learning, from beginners looking to get started to experienced practitioners seeking to deepen their knowledge. The book‚Äôs focus on Keras and Python makes it an excellent choice for those interested in hands-on deep learning projects, and Chollet‚Äôs expertise as the creator of Keras ensures that readers receive valuable insights into the field of deep learning.\n\n\n\n\n\n\n\n\nThe Incerto book series, written by Nassim Nicholas Taleb, is a collection of five interconnected books that delve into the themes of uncertainty, probability, risk, and decision-making under conditions of unpredictability. Nassim Nicholas Taleb is a renowned author, philosopher, and former Wall Street trader who has become well-known for his ideas on risk and randomness.\nThe series is known for its challenging and unconventional ideas, and Taleb‚Äôs writing style combines philosophy, economics, and practical insights to encourage readers to think critically about risk, uncertainty, and decision-making in a complex world. The series has gained a dedicated following and has had a significant impact on discussions around risk management, finance, and decision science. It is a fundamental and enjoyably read (no code) to understand how reality works and how our own intuitions are sometimes completely misleading."
  },
  {
    "objectID": "posts/Data Science project Boilerplate.html",
    "href": "posts/Data Science project Boilerplate.html",
    "title": "Data Science project Boilerplate",
    "section": "",
    "text": "A data science boilerplate, in the context of software development and data science projects, refers to a standardized and reusable set of code, templates, libraries, and best practices that are pre-defined and organized to kickstart a data science project. It serves as a foundation or starting point for data scientists and analysts, helping them save time and effort when beginning a new project or analysis. Here‚Äôs an ideal data science boilerplate that I recommend for basically any project.\n\nThe folder structure\n.\n‚îú‚îÄ‚îÄ data\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ data.csv\n‚îú‚îÄ‚îÄ Dockerfile\n‚îú‚îÄ‚îÄ Makefile\n‚îú‚îÄ‚îÄ .env\n‚îú‚îÄ‚îÄ.envrc\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ notebooks\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ datascientist_deliverable.ipynb\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ requirements.txt\n‚îú‚îÄ‚îÄ scripts\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ script.py\n‚îú‚îÄ‚îÄ setup.py\n‚îî‚îÄ‚îÄ thepkg\n    ‚îú‚îÄ‚îÄ __init__.py\n    ‚îú‚îÄ‚îÄ interface\n    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ __init__.py\n    ‚îú‚îÄ‚îÄ ml_logic\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data.py\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ model.py\n    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ preprocessor.py\n    ‚îú‚îÄ‚îÄ params.py\n    ‚îî‚îÄ‚îÄ utils.py\n\nThis folder structure represents a typical directory layout for a data science project. Each folder and file serves a specific purpose in organizing and managing the project‚Äôs code, data, documentation, and other resources. I will now give an explanation of each item in the structure:\n\ndata: This folder contains the project‚Äôs data files. In this case, there is a single CSV file named data.csv, but you can add more data files as needed.\nDockerfile: This file is used to define the instructions for creating a Docker container for your project. Docker allows you to encapsulate your project environment and dependencies for consistency and portability. This is a key element for most of my projects since I love to delevope inside a Docker Container (see my other blog post on how to use Docker with ‚Äòbind mounts‚Äô.)\nMakefile: A Makefile contains a set of rules and commands for building, testing, and running various aspects of your project. It can automate common development tasks.\n.env: This file is often used to store environment variables specific to your project. These variables can include API keys, database connection strings, or other sensitive information.\n.envrc: This file is typically used in conjunction with a tool like direnv to manage environment variables for your project, ensuring that the correct environment is set up when you enter the project directory.\n.gitignore: This file specifies files and folders that should be ignored by Git when tracking changes. It helps avoid including sensitive or unnecessary files in version control.\nnotebooks: This folder is meant for Jupyter notebooks used for data exploration, analysis, and documentation. In this case, there‚Äôs a single notebook file named datascientist_deliverable.ipynb.\nREADME.md: This Markdown file is used to provide an overview and documentation of the project. It typically includes project goals, setup instructions, usage examples, and other relevant information.\nrequirements.txt: This file lists the Python packages and their versions required for the project. You can use it to recreate the project‚Äôs environment on another system.\nscripts: This folder is intended for Python scripts that are part of your project. In this example, there‚Äôs a single script file named script.py.\nsetup.py: This is a Python script used for packaging and distributing your project as a Python package and is directly related with the folder `thepkg`. It‚Äôs often used when you want to share your code with others or publish it on platforms like PyPI.\nthepkg: This folder represents the main Python package of your project. In development phase we would install the package using the classic pip install . e (editablemode) to updated function without having to reinstalling it over and over again. The folder is organized in a way that follows Python package conventions:\n\ninit.py: These files indicate that the directories are Python packages and can be imported as modules.\ninterface: This subpackage appears to be a module for defining an interface or API for your project.\nml_logic: This subpackage seems to contain modules related to machine learning logic, including data processing (data.py and preprocessor.py) and modeling (model.py).\nparams.py: This file could contain project-specific configuration parameters or settings.\nutils.py: This file likely contains utility functions or helper code used throughout the project.\n\n\nOverall, this folder structure provides a well-organized framework for a data science project, making it easier to collaborate, manage dependencies, and maintain consistency in your work.\nRemember that I boilerplate is just a template. Feel free to use this structure as a first step into your new Data Science project."
  },
  {
    "objectID": "posts/Data Science Books.html#introductory-econometrics-a-modern-approach-by-jeffrey-m.-wooldridge",
    "href": "posts/Data Science Books.html#introductory-econometrics-a-modern-approach-by-jeffrey-m.-wooldridge",
    "title": "Data Science Books",
    "section": "",
    "text": "‚ÄúIntroductory Econometrics: A Modern Approach‚Äù by Jeffrey M. Wooldridge is a widely acclaimed textbook that serves as a comprehensive introduction to the field of econometrics. This book is a valuable resource for students, researchers, and practitioners interested in using statistical methods to analyze economic data. Wooldridge‚Äôs approach is modern and intuitive, making complex econometric concepts accessible to readers at various levels of expertise.\nThe book covers key topics such as regression analysis, hypothesis testing and more advanced teniques such as instrumental variables, panel data analysis, and time series analysis. What sets it apart is its focus on real-world applications, using examples and datasets from various fields of economics to illustrate the concepts discussed. This practical approach helps readers bridge the gap between theory and practice, preparing them to conduct meaningful empirical research in economics.\nFurthermore, Wooldridge‚Äôs writing style is clear and engaging, making it easier for readers to grasp challenging econometric concepts. With its emphasis on modern techniques and hands-on applications, ‚ÄúIntroductory Econometrics: A Modern Approach‚Äù has become a trusted resource in the field, helping students and researchers develop the skills needed to analyze and interpret economic data effectively."
  },
  {
    "objectID": "posts/Data Science Books.html#introduction-to-machine-learning-with-python-a-guide-for-data-scientists-1st-edition-by-andreas-m√ºller-sarah-guido",
    "href": "posts/Data Science Books.html#introduction-to-machine-learning-with-python-a-guide-for-data-scientists-1st-edition-by-andreas-m√ºller-sarah-guido",
    "title": "Data Science Books",
    "section": "",
    "text": "‚ÄúIntroduction to Machine Learning with Python: A Guide for Data Scientists‚Äù by Andreas C. M√ºller and Sarah Guido is a highly regarded and practical book that provides an excellent introduction to machine learning concepts and their implementation using Python. This book is particularly beneficial for data scientists, engineers, and anyone interested in learning the fundamentals of machine learning in a hands-on manner.\nThe book covers a wide range of machine learning topics, including supervised learning, unsupervised learning, and deep learning. It also introduces essential libraries and tools for machine learning in Python, such as scikit-learn and TensorFlow. One of the strengths of this book is its focus on practical examples and code snippets, which allow readers to apply what they learn immediately.\nOverall, ‚ÄúIntroduction to Machine Learning with Python‚Äù is a valuable resource for those looking to gain a solid foundation in machine learning while leveraging the power of the Python programming language. It‚Äôs suitable for both beginners and intermediate learners and provides a practical and hands-on approach to mastering the essentials of machine learning for data science."
  },
  {
    "objectID": "posts/Data Science Books.html#hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow-concepts-tools-and-techniques-to-build-intelligent-systems-by-aur√©lien-g√©ron",
    "href": "posts/Data Science Books.html#hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow-concepts-tools-and-techniques-to-build-intelligent-systems-by-aur√©lien-g√©ron",
    "title": "Data Science Books",
    "section": "",
    "text": "‚ÄúHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems‚Äù by Aur√©lien G√©ron is a popular and comprehensive book that provides a practical guide to building machine learning models using three widely-used libraries: scikit-learn, Keras, and TensorFlow. The book is designed for both beginners and experienced practitioners in the field of machine learning and artificial intelligence.\nThe book is known for its clarity, practicality, and hands-on approach. It has been praised for its ability to take readers from the fundamentals of machine learning to building and deploying sophisticated machine learning models. Whether you‚Äôre a beginner looking to learn the basics or an experienced data scientist seeking to expand your knowledge, ‚ÄúHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow‚Äù is a valuable resource for anyone interested in the field of machine learning."
  },
  {
    "objectID": "posts/Data Science Books.html#deep-learning-with-python-by-francois-chollet",
    "href": "posts/Data Science Books.html#deep-learning-with-python-by-francois-chollet",
    "title": "Data Science Books",
    "section": "",
    "text": "‚ÄúDeep Learning with Python‚Äù by Fran√ßois Chollet is a highly regarded book that serves as a practical guide to understanding and implementing deep learning techniques using the Python programming language and the Keras deep learning framework. Fran√ßois Chollet is the creator of Keras, which has become a popular tool for building deep neural networks due to its simplicity and flexibility.\nThis must read book is known for its accessibility and practical approach. It‚Äôs suitable for readers with varying levels of experience in deep learning, from beginners looking to get started to experienced practitioners seeking to deepen their knowledge. The book‚Äôs focus on Keras and Python makes it an excellent choice for those interested in hands-on deep learning projects, and Chollet‚Äôs expertise as the creator of Keras ensures that readers receive valuable insights into the field of deep learning."
  },
  {
    "objectID": "posts/Data Science Books.html#incerto-book-series-by-nassim-nicholas-taleb",
    "href": "posts/Data Science Books.html#incerto-book-series-by-nassim-nicholas-taleb",
    "title": "Data Science Books",
    "section": "",
    "text": "The Incerto book series, written by Nassim Nicholas Taleb, is a collection of five interconnected books that delve into the themes of uncertainty, probability, risk, and decision-making under conditions of unpredictability. Nassim Nicholas Taleb is a renowned author, philosopher, and former Wall Street trader who has become well-known for his ideas on risk and randomness.\nThe series is known for its challenging and unconventional ideas, and Taleb‚Äôs writing style combines philosophy, economics, and practical insights to encourage readers to think critically about risk, uncertainty, and decision-making in a complex world. The series has gained a dedicated following and has had a significant impact on discussions around risk management, finance, and decision science. It is a fundamental and enjoyably read (no code) to understand how reality works and how our own intuitions are sometimes completely misleading."
  },
  {
    "objectID": "posts/Bayesian updating.html",
    "href": "posts/Bayesian updating.html",
    "title": "Bayesian updating",
    "section": "",
    "text": "To elucidate the workings of Bayesian updating, I often find it helpful to draw parallels from everyday experiences. Previously, I‚Äôve used examples like my dog and my first date with my girlfriend. This time, let‚Äôs delve into the concept using a job interview scenario.\n\n\nImagine you‚Äôve landed a job interview ‚Äì you‚Äôre simultaneously excited and apprehensive. As you stand at the threshold, you‚Äôre completely uncertain about what awaits you inside. You don‚Äôt know who the interviewers are, what to expect, or how the interview will unfold. The odds of everything going either terribly wrong or wonderfully right seem equally plausible.\nIn the realm of probability, we can liken your state of mind to a uniform distribution, where all outcomes are equally probable. This state of not knowing what will happen or how the interview will conclude is what Bayesian terminology dubs the ‚Äúprior belief‚Äù ‚Äì it‚Äôs the starting point for your assessment.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\n#make this example reproducible.\nnp.random.seed(1)\ndataUni = np.random.uniform(size=1000)\nsns.kdeplot(dataUni)\n# Adjust x-axis limits to show the tails\nplt.xlim(-0.1, 1.1)\nplt.title('Prior Belief with no idea')\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nInterestingly, we still call it a ‚Äúprior belief‚Äù even if you harbor some inkling about how the interview might unfold. For instance, let‚Äôs say you had an unpleasant preliminary conversation with the company, and as you enter the room, you‚Äôre not filled with optimism, estimating a 40% chance of success. In Bayesian terms, this, too, can be translated into probabilities.\n\n\nCode\n#make this example reproducible.\nnp.random.seed(1)\n# Define the parameters for the prior belief and evidence\nprior_prob = 0.4\n# Create a range of values for the probability (x-axis)\nx = np.linspace(0, 1, 1000)\n# Calculate the prior probability distribution (prior belief)\nprior_distribution = stats.beta.pdf(x, 2, 3)  # Beta distribution parameters (2, 3)\nsns.lineplot(x=x, y=prior_distribution,color=\"blue\")\nplt.title('Prior Belief with some idea')\n# Show the plot\nplt.show()\n\n\n\n\n\nKeep in mind, though, that this assessment is occurring just as you‚Äôre about to step through the door.\n\n\n\nNow, envision yourself crossing that threshold, and the interviewers pose their initial question. Your response is swift, and you notice a confident smile on the interviewer‚Äôs face. This moment represents a pivotal piece of new information drawn from your real-time experience, and naturally, you‚Äôll use it to revise your initial belief about the interview‚Äôs outcome. We call this new information and ‚Äúevidence‚Äù.\nWhat‚Äôs happening inside your mind, that surge of heightened confidence, can once again be translated into numerical probabilities to update your initial belief. This is precisely where the Bayes comes into play.\nLet‚Äôs show how the update takes place. Let‚Äôs assign to that smile, the evidence, a probability of 80% to pass the interview.\n\n\nCode\nevidence_prob = 0.8\n# Create a range of values for the probability (x-axis)\nx = np.linspace(0, 1, 1000)\n\nevidence_distribution = stats.beta.pdf(x, 4, 2)  # Beta distribution parameters (4, 2)\nsns.lineplot(x=x, y=evidence_distribution, color='green')\nplt.title('Evidence')\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nNow, let‚Äôs plug in all the information we have in the Bayesian therm we all know. We started with a not so confident prior belief (40% chance of success), then we got a new smiling evidence (80% chance of success). From here we can combine these two information and calculate our updated beliefs or as we call them ‚Äúposterior beliefs‚Äù.\n\n\nCode\n# Define the parameters for the prior belief and evidence\nprior_prob = 0.4\nevidence_prob = 0.8\n\n# Create a range of values for the probability (x-axis)\nx = np.linspace(0, 1, 1000)\n\n# Calculate the prior probability distribution (prior belief)\nprior_distribution = stats.beta.pdf(x, 2, 3)  # Beta distribution parameters (2, 3)\n\n# Calculate the evidence (as a Beta distribution with parameters based on evidence probability)\nevidence_distribution = stats.beta.pdf(x, 4, 2)  # Beta distribution parameters (4, 2)\n\n# Calculate the posterior probability distribution (updated belief)\nposterior_distribution = prior_distribution * evidence_distribution\nposterior_distribution = posterior_distribution\n\n# Create a Seaborn plot\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=x, y=prior_distribution, label='Prior Belief (40%)', color='blue')\nsns.lineplot(x=x, y=evidence_distribution, label='Evidence (80%)', color='green')\nsns.lineplot(x=x, y=posterior_distribution, label='Posterior Belief', color='red')\n\n# Customize the plot\nplt.title('Bayes update Representation')\nplt.xlabel('Probability')\nplt.ylabel('Density')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nAs you can see, now you have around 60% of passing the interview. These process can keep on going with every new information you collect during the interview as in this gif.\n\n\n\n\n\nIn essence, this is the core of Bayesian updating. It allows you to adapt your beliefs based on new information as you progress through an uncertain situation, much like navigating the twists and turns of a job interview."
  },
  {
    "objectID": "posts/Bayesian updating.html#the-job-interview",
    "href": "posts/Bayesian updating.html#the-job-interview",
    "title": "Bayesian updating",
    "section": "",
    "text": "Imagine you‚Äôve landed a job interview ‚Äì you‚Äôre simultaneously excited and apprehensive. As you stand at the threshold, you‚Äôre completely uncertain about what awaits you inside. You don‚Äôt know who the interviewers are, what to expect, or how the interview will unfold. The odds of everything going either terribly wrong or wonderfully right seem equally plausible.\nIn the realm of probability, we can liken your state of mind to a uniform distribution, where all outcomes are equally probable. This state of not knowing what will happen or how the interview will conclude is what Bayesian terminology dubs the ‚Äúprior belief‚Äù ‚Äì it‚Äôs the starting point for your assessment.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\n#make this example reproducible.\nnp.random.seed(1)\ndataUni = np.random.uniform(size=1000)\nsns.kdeplot(dataUni)\n# Adjust x-axis limits to show the tails\nplt.xlim(-0.1, 1.1)\nplt.title('Prior Belief with no idea')\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "posts/Bayesian updating.html#beliefs",
    "href": "posts/Bayesian updating.html#beliefs",
    "title": "Bayesian updating",
    "section": "",
    "text": "Interestingly, we still call it a ‚Äúprior belief‚Äù even if you harbor some inkling about how the interview might unfold. For instance, let‚Äôs say you had an unpleasant preliminary conversation with the company, and as you enter the room, you‚Äôre not filled with optimism, estimating a 40% chance of success. In Bayesian terms, this, too, can be translated into probabilities.\n\n\nCode\n#make this example reproducible.\nnp.random.seed(1)\n# Define the parameters for the prior belief and evidence\nprior_prob = 0.4\n# Create a range of values for the probability (x-axis)\nx = np.linspace(0, 1, 1000)\n# Calculate the prior probability distribution (prior belief)\nprior_distribution = stats.beta.pdf(x, 2, 3)  # Beta distribution parameters (2, 3)\nsns.lineplot(x=x, y=prior_distribution,color=\"blue\")\nplt.title('Prior Belief with some idea')\n# Show the plot\nplt.show()\n\n\n\n\n\nKeep in mind, though, that this assessment is occurring just as you‚Äôre about to step through the door."
  },
  {
    "objectID": "posts/Bayesian updating.html#smile",
    "href": "posts/Bayesian updating.html#smile",
    "title": "Bayesian updating",
    "section": "",
    "text": "Now, envision yourself crossing that threshold, and the interviewers pose their initial question. Your response is swift, and you notice a confident smile on the interviewer‚Äôs face. This moment represents a pivotal piece of new information drawn from your real-time experience, and naturally, you‚Äôll use it to revise your initial belief about the interview‚Äôs outcome. We call this new information and ‚Äúevidence‚Äù.\nWhat‚Äôs happening inside your mind, that surge of heightened confidence, can once again be translated into numerical probabilities to update your initial belief. This is precisely where the Bayes comes into play.\nLet‚Äôs show how the update takes place. Let‚Äôs assign to that smile, the evidence, a probability of 80% to pass the interview.\n\n\nCode\nevidence_prob = 0.8\n# Create a range of values for the probability (x-axis)\nx = np.linspace(0, 1, 1000)\n\nevidence_distribution = stats.beta.pdf(x, 4, 2)  # Beta distribution parameters (4, 2)\nsns.lineplot(x=x, y=evidence_distribution, color='green')\nplt.title('Evidence')\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "posts/Bayesian updating.html#update",
    "href": "posts/Bayesian updating.html#update",
    "title": "Bayesian updating",
    "section": "",
    "text": "Now, let‚Äôs plug in all the information we have in the Bayesian therm we all know. We started with a not so confident prior belief (40% chance of success), then we got a new smiling evidence (80% chance of success). From here we can combine these two information and calculate our updated beliefs or as we call them ‚Äúposterior beliefs‚Äù.\n\n\nCode\n# Define the parameters for the prior belief and evidence\nprior_prob = 0.4\nevidence_prob = 0.8\n\n# Create a range of values for the probability (x-axis)\nx = np.linspace(0, 1, 1000)\n\n# Calculate the prior probability distribution (prior belief)\nprior_distribution = stats.beta.pdf(x, 2, 3)  # Beta distribution parameters (2, 3)\n\n# Calculate the evidence (as a Beta distribution with parameters based on evidence probability)\nevidence_distribution = stats.beta.pdf(x, 4, 2)  # Beta distribution parameters (4, 2)\n\n# Calculate the posterior probability distribution (updated belief)\nposterior_distribution = prior_distribution * evidence_distribution\nposterior_distribution = posterior_distribution\n\n# Create a Seaborn plot\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=x, y=prior_distribution, label='Prior Belief (40%)', color='blue')\nsns.lineplot(x=x, y=evidence_distribution, label='Evidence (80%)', color='green')\nsns.lineplot(x=x, y=posterior_distribution, label='Posterior Belief', color='red')\n\n# Customize the plot\nplt.title('Bayes update Representation')\nplt.xlabel('Probability')\nplt.ylabel('Density')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nAs you can see, now you have around 60% of passing the interview. These process can keep on going with every new information you collect during the interview as in this gif.\n\n\n\n\n\nIn essence, this is the core of Bayesian updating. It allows you to adapt your beliefs based on new information as you progress through an uncertain situation, much like navigating the twists and turns of a job interview."
  }
]