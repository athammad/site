[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "🛰 Research",
    "section": "",
    "text": "The majority of my current research revolves around the nexus between causal inference, machine learning and environmental analysis. In the last few years my focus has been on topics such as climate change and extreme events but I had the chance to explore other avenues such as optimal allocation of health care facilities and transport networks analysis.\nMy interest is in exploring the interplay between causal inference, machine learning, and environmental analysis. The urgency of addressing environmental challenges necessitates innovative approaches to understanding the complex relationships between human activities, environmental factors, and their consequences. By combining techniques from causal inference and machine learning, I aim to develop novel methodologies that can effectively analyse environmental data, identify causal relationships, and inform evidence-based decision-making for sustainable environmental management.\n\n\n\n\nThe advent of big data and the increasing availability of diverse environmental datasets have provided researchers with an unprecedented opportunity to investigate complex environmental systems. However, extracting meaningful insights from these datasets poses significant challenges due to confounding factors, selection biases, and non-randomness inherent in observational data. Traditional statistical methods often fall short in capturing the complex and non-linear relationships within environmental systems. Therefore, there is a pressing need to integrate causal inference and machine learning techniques to overcome these limitations and achieve a deeper understanding of environmental dynamics.\n\n\n\n\n\nMachine learning algorithms have shown remarkable success in pattern recognition, predictive modelling, and data-driven decision-making. The ability to automatically extract complex patterns and relationships from large-scale environmental datasets makes machine learning an invaluable tool for environmental analysis. By leveraging machine learning techniques, develop predictive models to forecast environmental outcomes and assess the potential impacts of policy interventions.. Furthermore, machine learning methods can aid in feature selection, data imputation, and anomaly detection, contributing to improved data quality and reliability for environmental analysis.\n\n\n\n\n\nCausal inference provides a powerful framework for identifying cause-and-effect relationships within complex systems. By applying causal inference methods, we can go beyond correlations and uncover the underlying causal mechanisms driving observed environmental phenomena. Approaches such as propensity score matching, instrumental variables, and difference-in-differences analysis allow us to address confounding biases and estimate causal effects more accurately. Incorporating these causal inference techniques into environmental analysis can enhance our ability to discern the impacts of specific environmental factors and human activities on various ecological systems.\n\n\n\n\n\nIntegrating causal inference with machine learning techniques presents exciting opportunities for advancing environmental analysis. By combining the strengths of both fields, we can develop robust methodologies that provide accurate causal estimates while harnessing the predictive power of machine learning algorithms. However, several challenges need to be addressed to achieve this integration successfully. These challenges include the treatment of unmeasured confounding, scalability of methods for large environmental datasets, interpretability of complex machine learning models, and the consideration of uncertainty in causal inference.\n\n\n\n\n\nIn my research, I aim to address these challenges and contribute to the nexus between causal inference, machine learning, and environmental analysis. Specifically, I intend to:\n\nDevelop novel methodologies that integrate causal inference and machine learning techniques to estimate causal effects and predict environmental outcomes accurately.\nInvestigate the use of deep learning architectures and explainable AI methods to enhance the interpretability of complex machine learning models for environmental analysis.\nExplore scalable algorithms and parallel computing techniques to handle the computational demands of large environmental datasets.\nInvestigate methods for handling unmeasured confounding and uncertainties inherent in causal inference within the context of environmental analysis.\nApply the developed methodologies to real-world environmental challenges, such as climate change and extreme event analysis. By pursuing these research objectives, I aspire to advance the understanding of causal relationships in environmental systems, contribute to evidence-based decision-making for environmental management, and ultimately support the development of sustainable practices for a better future."
  },
  {
    "objectID": "research.html#background",
    "href": "research.html#background",
    "title": "🛰 Research",
    "section": "",
    "text": "The advent of big data and the increasing availability of diverse environmental datasets have provided researchers with an unprecedented opportunity to investigate complex environmental systems. However, extracting meaningful insights from these datasets poses significant challenges due to confounding factors, selection biases, and non-randomness inherent in observational data. Traditional statistical methods often fall short in capturing the complex and non-linear relationships within environmental systems. Therefore, there is a pressing need to integrate causal inference and machine learning techniques to overcome these limitations and achieve a deeper understanding of environmental dynamics."
  },
  {
    "objectID": "research.html#machine-learning",
    "href": "research.html#machine-learning",
    "title": "🛰 Research",
    "section": "",
    "text": "Machine learning algorithms have shown remarkable success in pattern recognition, predictive modelling, and data-driven decision-making. The ability to automatically extract complex patterns and relationships from large-scale environmental datasets makes machine learning an invaluable tool for environmental analysis. By leveraging machine learning techniques, develop predictive models to forecast environmental outcomes and assess the potential impacts of policy interventions.. Furthermore, machine learning methods can aid in feature selection, data imputation, and anomaly detection, contributing to improved data quality and reliability for environmental analysis."
  },
  {
    "objectID": "research.html#causal-inference",
    "href": "research.html#causal-inference",
    "title": "🛰 Research",
    "section": "",
    "text": "Causal inference provides a powerful framework for identifying cause-and-effect relationships within complex systems. By applying causal inference methods, we can go beyond correlations and uncover the underlying causal mechanisms driving observed environmental phenomena. Approaches such as propensity score matching, instrumental variables, and difference-in-differences analysis allow us to address confounding biases and estimate causal effects more accurately. Incorporating these causal inference techniques into environmental analysis can enhance our ability to discern the impacts of specific environmental factors and human activities on various ecological systems."
  },
  {
    "objectID": "research.html#integration-and-challenges",
    "href": "research.html#integration-and-challenges",
    "title": "🛰 Research",
    "section": "",
    "text": "Integrating causal inference with machine learning techniques presents exciting opportunities for advancing environmental analysis. By combining the strengths of both fields, we can develop robust methodologies that provide accurate causal estimates while harnessing the predictive power of machine learning algorithms. However, several challenges need to be addressed to achieve this integration successfully. These challenges include the treatment of unmeasured confounding, scalability of methods for large environmental datasets, interpretability of complex machine learning models, and the consideration of uncertainty in causal inference."
  },
  {
    "objectID": "research.html#research-objectives",
    "href": "research.html#research-objectives",
    "title": "🛰 Research",
    "section": "",
    "text": "In my research, I aim to address these challenges and contribute to the nexus between causal inference, machine learning, and environmental analysis. Specifically, I intend to:\n\nDevelop novel methodologies that integrate causal inference and machine learning techniques to estimate causal effects and predict environmental outcomes accurately.\nInvestigate the use of deep learning architectures and explainable AI methods to enhance the interpretability of complex machine learning models for environmental analysis.\nExplore scalable algorithms and parallel computing techniques to handle the computational demands of large environmental datasets.\nInvestigate methods for handling unmeasured confounding and uncertainties inherent in causal inference within the context of environmental analysis.\nApply the developed methodologies to real-world environmental challenges, such as climate change and extreme event analysis. By pursuing these research objectives, I aspire to advance the understanding of causal relationships in environmental systems, contribute to evidence-based decision-making for environmental management, and ultimately support the development of sustainable practices for a better future."
  },
  {
    "objectID": "cv/cv.html",
    "href": "cv/cv.html",
    "title": "Ahmed T. Hammad",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "🧑‍🏫 Teaching",
    "section": "",
    "text": "As a teacher, I aim to perpetuate knowledge and inspire learning.\n\n\n\n\nAs a data scientist and academic researcher, I introduce students to an array of fundamental concepts of our understanding of reality through data and always ask them to articulate their reactions and understanding of the topics by providing me, and their classmates with alternative examples and practical applications of the concepts and techniques we learned together.\nMy teaching approach engages students’ natural creativity with a challenging mix of theory and practice. I teach theoretical concepts with a commitment to connecting these concepts with concrete real-life examples and by sharing the implications in business. For example, as the lead Data Science Instructor at Le Wagon Singapore, I taught several machine learning algorithms by explaining the implication of their use in different business and real-life contexts. I found that this approach helped students understand why, when and how to use a specific algorithm rather than another and how to articulate and justify their decision process.\n\n\n\n\n\nI seek a balance in my courses between lecturing to students and asking them to make discoveries. I encourage students to engage with the topic at hand, with me and with each other, in the belief that good teaching depends upon intellectual exchange. My end goal is to enable my students to envision and develop a “product” around the concepts and techniques we learned in class.\nMy approach to student assessment reflects my two goals. First, the students are expected to master a body of knowledge by demonstrating the practical use of different algorithms and methodologies at the end of the lecture. Second, students are given the opportunity to reflect upon the material at greater leisure and detail on project assignments.\n\n\n\n\n\nWhile my standards are high, I do not hesitate to help my students to meet expectations by providing office hours, review sessions and private time to discuss their concerns, fears and dreams. My approach has been to always make myself readily available to students and make it clear that if they need something then I am available at any time and point in their life to support them in their journey.\nI also have a strong passion for helping students who are dedicated, but not necessarily performing well in class. In my experience, I have found some students performing far better in very applied tasks, shining in applied academic research or reaching success in business settings.\n\n\n\n\n\nMy teaching, research, and work experience has covered a wide variety of topics, including machine learning, causal machine learning, incremental machine learning, econometrics, causal inference, probabilistic modelling, time series modelling and network analysis. Given the need, I am qualified to and would readily commit the effort required to effectively teach undergraduate and graduated classes in these subjects. Because of the scope of my current research, I am specifically eager and excited to teach artificial intelligence, data science and statistics classes at both the undergraduate and graduate levels."
  },
  {
    "objectID": "teaching.html#teaching-approach",
    "href": "teaching.html#teaching-approach",
    "title": "🧑‍🏫 Teaching",
    "section": "",
    "text": "As a data scientist and academic researcher, I introduce students to an array of fundamental concepts of our understanding of reality through data and always ask them to articulate their reactions and understanding of the topics by providing me, and their classmates with alternative examples and practical applications of the concepts and techniques we learned together.\nMy teaching approach engages students’ natural creativity with a challenging mix of theory and practice. I teach theoretical concepts with a commitment to connecting these concepts with concrete real-life examples and by sharing the implications in business. For example, as the lead Data Science Instructor at Le Wagon Singapore, I taught several machine learning algorithms by explaining the implication of their use in different business and real-life contexts. I found that this approach helped students understand why, when and how to use a specific algorithm rather than another and how to articulate and justify their decision process."
  },
  {
    "objectID": "teaching.html#lecturing-approach",
    "href": "teaching.html#lecturing-approach",
    "title": "🧑‍🏫 Teaching",
    "section": "",
    "text": "I seek a balance in my courses between lecturing to students and asking them to make discoveries. I encourage students to engage with the topic at hand, with me and with each other, in the belief that good teaching depends upon intellectual exchange. My end goal is to enable my students to envision and develop a “product” around the concepts and techniques we learned in class.\nMy approach to student assessment reflects my two goals. First, the students are expected to master a body of knowledge by demonstrating the practical use of different algorithms and methodologies at the end of the lecture. Second, students are given the opportunity to reflect upon the material at greater leisure and detail on project assignments."
  },
  {
    "objectID": "teaching.html#advising-approach",
    "href": "teaching.html#advising-approach",
    "title": "🧑‍🏫 Teaching",
    "section": "",
    "text": "While my standards are high, I do not hesitate to help my students to meet expectations by providing office hours, review sessions and private time to discuss their concerns, fears and dreams. My approach has been to always make myself readily available to students and make it clear that if they need something then I am available at any time and point in their life to support them in their journey.\nI also have a strong passion for helping students who are dedicated, but not necessarily performing well in class. In my experience, I have found some students performing far better in very applied tasks, shining in applied academic research or reaching success in business settings."
  },
  {
    "objectID": "teaching.html#teaching-interests",
    "href": "teaching.html#teaching-interests",
    "title": "🧑‍🏫 Teaching",
    "section": "",
    "text": "My teaching, research, and work experience has covered a wide variety of topics, including machine learning, causal machine learning, incremental machine learning, econometrics, causal inference, probabilistic modelling, time series modelling and network analysis. Given the need, I am qualified to and would readily commit the effort required to effectively teach undergraduate and graduated classes in these subjects. Because of the scope of my current research, I am specifically eager and excited to teach artificial intelligence, data science and statistics classes at both the undergraduate and graduate levels."
  },
  {
    "objectID": "students.html",
    "href": "students.html",
    "title": "🧑‍🎓 Students",
    "section": "",
    "text": "What my students say about me\n\n\nI was very fortunate to get Ahmed as my instructor during my data science boot camp. Overall, Ahmed was very passionate in teaching, his lessons were entertaining and enjoyable, and he was also able to explain complicated concepts in a clean and simple manner. What I liked most was that he often imparted little tips and advice on how to do well in data science, and also shared his personal experiences and challenges he faced in his own projects and how he managed to resolve them. I couldn’t have asked for a better instructor to kickstart my data science journey. Marcus Tan\n\n\nAhmed was my teacher during the 9 weeks data science bootcamp. He is an exceptional and passionate teacher. He teaches with great enthusiasm and patience. Additionally, he goes beyond to provide excellent support and guidance. Speaking for all my classmates, we were truly grateful to have him as a teacher. George Lee\n\n\nI highly recommend Ahmed for the position of university lecturer in data science. I had the pleasure of having him as my teacher in the Le Wagon data science bootcamp. He is a talented data scientist with a passion for teaching. He is also a highly motivated and hardworking individual. I am confident that he would be a positive addition to any company that is willing to hired.\nHere are some of Ahmed’s strengths as a data science educator:\nDeep knowledge of the subject matter\nAbility to communicate complex concepts in a clear and engaging way\nPatient and helpful in answering student questions\nPassionate about data science and eager to share his knowledge with others. Zheng YongShun\n\n\nAhmed, thank you for being an amazing teacher. Your passion for the subject and the way you create a welcoming classroom environment have made learning so much more enjoyable. You have a talent for making complicated things easy to understand, and I appreciate how you always go the extra mile to help us succeed. I feel lucky to be your student. Yong Sin Tan\n\n\nDr. Ahmed is someone who can break down complicated mathematical concepts into easily understandable components. Not only that, he demonstrates genuine care for his students’ progress.\nEven until now, long after our classes with him, he is still always available when we need help or clarification in data science/career progression topics. I’m very fortunate to have the opportunity to be taught by this outstanding and passionate teacher. Yong Chew\n\n\nI had the privilege of attending Ahmed’s lectures for Le Wagon’s full time data science program that spanned nine weeks.\n\nAhmed is an exceptional educator with a profound expertise in various domains of data science. His proficiency in statistical modeling, data analytics, SQL and Python programming, project management, machine learning, and deep learning is truly impressive. Throughout the bootcamp, Ahmed demonstrated a deep understanding of these subjects, and his ability to effectively convey complex concepts made the learning experience incredibly rewarding.\n\nOne aspect that truly stood out during the bootcamp was the final project on machine learning. We took on the task to predict possible side effects when two drugs are taken together, otherwise known as drug-drug interaction. Ahmed’s knowledge in statistics played a crucial role in guiding us through the project. His guidance and facilitation during the sessions ensured that we had a comprehensive understanding of each phase of the project. Under his supervision, we were able to complete the project from scratch within a remarkably short timeframe of just 2-3 weeks.\n\nHowever, what truly sets Ahmed apart is his dedication to ensuring that every student grasps the topics taught during the bootcamp. He goes above and beyond to make sure that even those without a computer science or statistics background can understand the material. Ahmed’s ability to blend well with us students and convey the teachings in a relatable manner is exceptional. He takes pride in his work and is a responsible lecturer who is always available to provide guidance and support.\n\nAhmed’s teaching style is highly engaging and interactive. He fosters an inclusive learning environment, encouraging students to ask questions, collaborate, and explore practical applications of the concepts learned. His dedication to our success was evident in the time and effort he invested in each student, providing valuable feedback and guidance to help us reach our full potential.\n\nI highly recommend Ahmed as a data science lecturer and researcher. His exceptional knowledge, dedication, and teaching abilities make him an invaluable asset to any learning environment. It was a privilege to learn from him, and I am confident that he will continue to inspire and shape the careers of aspiring data scientists. Xin Er Chong"
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "✍ Papers",
    "section": "",
    "text": "Pubblications\n\n\nPlanning universal accessibility to public health care in sub-Saharan Africa\nG Falchetta, AT Hammad, S Shayegh. Proceedings of the National Academy of Sciences 117 (50), 31760-31769\nComparing paratransit in seven major African cities: an accessibility and network analysis\nG Falchetta, M Noussan, AT Hammad. Journal of Transport Geography 94, 103131\nBack to the fields? Increased agricultural land greenness after a COVID-19 lockdown\nAT Hammad, G Falchetta, IBM Wirawan. Environmental Research Communications 3 (5), 051007\nProbabilistic forecasting of remotely sensed cropland vegetation health and its relevance for food security\nAT Hammad, G Falchetta .Science of the Total Environment 838, 156157\n\n\n\n\nCurrent work\n\n\nTracking global urban green space trends\nG Falchetta, AT Hammad. EGU23\n\n\n\n\nForthcoming\n\n\nA note on vulnerability\nProbabilistic reinforcement learning for evaluating single-subject experiments"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ahmed T. Hammad",
    "section": "",
    "text": "My name is Ahmed and I am a Data Scientist, Quantitative Researcher and Lecturer. I have a PhD in Causal Machine Learning and I have been working on data for over 10 years in both the academic and private sectors.In my career, I have been working extensively with many programming languages such as R and Python which I use on a daily basis.\nI like to be constantly updated and challenge myself with the latest developments in Machine Learning. That’s why I am still involved in research and I publish the results of my analysis concerning a variety of topics, including Machine Learning and Econometrics applied to environmental analysis and public health.\nI really enjoy sharing my knowledge of statistics and machine learning. I find teaching the young generation and professionals the most fulfilling opportunity to share my years of experience in the field. Helping them learn how to “get things done” and how to handle real-world data problems, has brought me so much joy since I started teaching.\n\n\n\n\n\n\n\n\nMachine Learning\nEconometrics\nIncremental Machine Learning\nCausal Inference\n\n\n\nTime Series\nProbabilistic Machine Learning\nEnvironmental Analysis\nSensor & Satellite Data"
  },
  {
    "objectID": "index.html#my-fields-of-interest",
    "href": "index.html#my-fields-of-interest",
    "title": "Ahmed T. Hammad",
    "section": "",
    "text": "Machine Learning\nEconometrics\nIncremental Machine Learning\nCausal Inference\n\n\n\nTime Series\nProbabilistic Machine Learning\nEnvironmental Analysis\nSensor & Satellite Data"
  },
  {
    "objectID": "blog/Logistic Regression and Marginal Effects.html",
    "href": "blog/Logistic Regression and Marginal Effects.html",
    "title": "Logistic Regression and Marginal Effects",
    "section": "",
    "text": "Logistic regression is a widely used statistical technique for modeling the relationship between a binary outcome and one or more predictor variables. It is commonly employed in various fields, such as healthcare, finance, and social sciences, to predict the probability of an event occurring. While the coefficients in logistic regression provide valuable insights, they are difficult to interpret. In this blog post, we will explore the use of marginal effects in logistic regression, unraveling their significance and practical applications.\n\n\nIn logistic regression, we model the log-odds of the event of interest occurring as a linear combination of predictor variables. The logistic function (sigmoid function) is then applied to this linear combination to convert it into a probability value between 0 and 1.\nThe logistic regression equation is given by:\n\\[\\text{logit}(P(Y=1)) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p\\]\nWhere:\n\n\\(P(Y=1)\\) is the probability of the event of interest (e.g., success) occurring.\n\\(logit (P(Y=1))\\) is the log-odds (logit) of the event occurring.\n\\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_p\\) are the coefficients of the model.\n\\(X_1, X_2, ..., X_p\\) are the predictor variables.\n\n\n\nThe coefficients \\(\\beta_1, \\beta_2, ..., \\beta_p\\) in the logistic regression equation hold crucial information about the impact of each predictor variable on the log-odds of the event occurring. To interpret these coefficients, we need to consider their signs, magnitude, and statistical significance.\n\nSign of Coefficients:\n\nThe sign of a coefficient reveals the direction of the relationship between the predictor variable and the log-odds of the event occurring. A positive coefficient (\\(\\beta &gt; 0\\)) indicates that an increase in the predictor variable leads to an increase in the log-odds (probability) of the event happening. Conversely, a negative coefficient (\\(\\beta &lt; 0\\)) suggests that an increase in the predictor variable is associated with a decrease in the log-odds (probability) of the event occurring.\n\nMagnitude of Coefficients:\n\nThe magnitude of a coefficient provides information about the strength of the relationship between the predictor variable and the log-odds of the event. Larger absolute values of coefficients indicate stronger influences on the probability.\n\nStatistical Significance:\n\nEvaluating the statistical significance of coefficients is essential to determine if the relationship between a predictor variable and the event probability is meaningful or just due to random chance. Researchers often use hypothesis testing or confidence intervals to assess the statistical significance of coefficients.\n\n\nLet’s consider an example to illustrate the interpretation of coefficients in logistic regression. Suppose we are studying the factors that influence whether a customer will make a purchase on an e-commerce website. Our logistic regression model includes two predictor variables: “Time Spent on Website” and “Number of Items Added to Cart.” The estimated coefficients are as follows:\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\)\n\\(\\beta_{\\text{Number of Items}} = 0.2\\)\n\n\n\n\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\): For each additional minute a customer spends on the website, the log-odds (probability) of making a purchase increases by 0.05 units. This means that spending more time on the website is associated with a higher likelihood of making a purchase.\n\\(\\beta_{\\text{Number of Items}} = 0.2\\): For each additional item a customer adds to the cart, the log-odds (probability) of making a purchase increases by 0.2 units. This suggests that adding more items to the cart is associated with a higher probability of making a purchase.\n\n\n\n\nIdeally, we want to understand what the model is telling us on the probability scale and not in the odds scale, much less in the estimation scale, the log-odds.\nWe might be tempted to convert log-odds to odds and then to probability and interpret everything as usual. But this would be very wrong!\nWhy?\nThe issue here is that on the probability scale the function is non-linear!\n\n\n\n\nMarginal effects can be described as the change in outcome as a function of the change in the independent variable of interest holding all other variables in the model constant.\n\nNOTE: In linear regression, the estimated regression coefficients are marginal effects and are more easily interpreted.*\n\nOne measure of change in a system is that the rate of change of the system is non-zero.\nIf we had a simple linear regression model for a trend, then the estimated rate of change would be the slope of the line. Technically, this is the instantaneous rate of change of the function that defines the line.\nThe idea can be extended to any function, even one as potentially complex non-linear function.\nThe problem we have is that in general we don’t have an equation for the non-linear from which we can derive the derivatives.\nSo how do we estimate the derivative of a non-linear function ?\nOne solution is to use the method of finite differences but to understand it properly we need to introduce first the basic concept of differential calculus.\n\n\n\n\nDifferential calculus is a branch of calculus that focuses on the study of rates of change and slopes of curves. It deals with the concept of derivatives, which represent the rate of change of a function at a specific point. Derivatives allow us to understand how a function is changing at a particular location and provide valuable insights into the behavior of functions.\nThe derivative of a function \\(f(x)\\) with respect to \\(x\\) is denoted as \\(f'(x)\\) or \\(\\frac{df(x)}{dx}\\). It represents the slope of the tangent line to the graph of the function at a given point \\(x\\). The derivative can be interpreted as the instantaneous rate of change of \\(f(x)\\) with respect to \\(x\\).\nFor example, consider the function \\(f(x) = x^2\\). The derivative of \\(f(x)\\) is \\(f'(x) = 2x\\). This means that the slope of the tangent line to the graph of \\(f(x)\\) at any point \\(x\\) is twice the value of \\(x\\) at that point.\nDifferential calculus has wide-ranging applications in various fields, including physics, engineering, economics, and more. It is used to model and analyze phenomena that involve continuous changes, such as motion, growth, and optimization.\nFinite differences are a numerical technique used to approximate derivatives of functions. The method involves computing the difference in function values at two nearby points and then dividing by the difference in their corresponding independent variable values. By using a small interval between the two points, we can get an approximation of the derivative at a specific point.\nThe finite difference formula for approximating the first derivative of a function \\(f(x)\\) at a point \\(x\\) is given by:\n\\[ f’(x) \\approx \\frac{f(x + h) - f(x)}{h} \\]\nWhere:\n\n\\(h\\) is a small value representing the step size or interval between the points.\n\nSimilarly, we can use finite differences to approximate higher-order derivatives, such as the second derivative:\n\\[ f’’(x) \\approx \\frac{f(x + h) - 2f(x) + f(x - h)}{h^2} \\]\nFinite differences are particularly useful when the analytical expression for the derivative is difficult to obtain or when dealing with discrete data points instead of continuous functions.\nDifferential calculus and finite differences are both essential tools in mathematics and scientific disciplines. Differential calculus provides a rigorous framework for studying rates of change and slopes of curves through derivatives. On the other hand, finite differences offer a practical and numerical approach to approximate derivatives when the analytical solution is not readily available or when dealing with discrete data. Together, these concepts enable us to better understand the behavior of functions and analyze real-world phenomena involving continuous and discrete changes.\n\n\n\nIn logistic regression, the estimated coefficients (log odds) indicate how the log odds of the outcome change with a one-unit increase in the predictor variable while holding other variables constant. However, interpreting these coefficients can be challenging, especially for non-statisticians. This is where marginal effects come to the rescue.\nMarginal effects provide a more straightforward interpretation by measuring the impact of a one-unit change in the predictor variable on the probability of the event occurring (i.e., the probability of success). They express the change in the probability as a result of the predictor variable’s change, allowing us to understand the practical implications of the model."
  },
  {
    "objectID": "blog/Unraveling the Power of Causal Machine Learning.html",
    "href": "blog/Unraveling the Power of Causal Machine Learning.html",
    "title": "Unraveling the Power of Causal Machine Learning",
    "section": "",
    "text": "Hey fellow data enthusiasts! Today, let’s embark on a journey to explore the captivating world of causal machine learning. 🚀\n🔍 Understanding Causality:\nIn the realm of traditional machine learning, we primarily focus on identifying patterns and correlations in data to make accurate predictions. While this approach is incredibly useful, it often fails to decipher the cause-and-effect relationships that underpin these patterns. This limitation can lead to misguided conclusions and ineffective decision-making.\nCausality seeks to answer “why” questions, revealing the cause behind an observed effect. Imagine a scenario where we want to understand if a certain medication truly improves patient outcomes or if it’s just correlated with better health. Causal inference allows us to make more meaningful inferences, beyond mere correlations.\n🧩 The Challenge of Causal Inference:\nEstablishing causality is not a straightforward task. The fundamental problem lies in the fact that we cannot simultaneously observe both the treatment group (those who receive a particular intervention) and the control group (those who do not). Traditional observational data often suffers from confounding variables, making it challenging to disentangle cause and effect.\n💡 Enter Causal Machine Learning:\nCausal Machine Learning, a fascinating interdisciplinary field, bridges the gap between traditional machine learning and causal inference. Its primary goal is to leverage data to identify causal relationships and infer the effects of interventions accurately.\n🎯 Causal Inference Methods:\n1. Randomized Control Trials (RCTs): Considered the gold standard for establishing causality, RCTs involve randomly assigning subjects to either the treatment or control group. By eliminating confounding factors, RCTs provide strong causal evidence. However, they may not always be practical or ethical.\n2. Propensity Score Matching (PSM): When RCTs are not feasible, PSM is a popular method. It attempts to create a “quasi-experimental” design by matching treated and control units based on their propensity scores, which represent the likelihood of receiving the treatment. This helps mitigate confounding effects.\n3. Instrumental Variables (IV): IV analysis relies on instrumental variables that are correlated with the treatment but have no direct effect on the outcome. By utilizing these instruments, researchers can uncover causal relationships even in the presence of unobserved confounders.\n4. Synthetic Control (SC): The process of creating a synthetic counterfactual involves using historical data from similar individuals or groups that did not receive the treatment to construct a “synthetic” control group. This control group is designed to closely match the characteristics of the treated group before the intervention. By doing so, researchers aim to create a plausible estimate of what would have happened to the treated group had they not received the treatment.\n🚀 Real-World Applications:\nCausal Machine Learning has found applications in various domains, including public health, economics, social sciences, and marketing. It enables policymakers to make informed decisions, businesses to optimize interventions, and researchers to uncover hidden causal mechanisms.\n🌟 Embracing Causal Machine Learning:\nAs the world becomes more data-driven, understanding causality becomes paramount. Embracing causal machine learning techniques empowers us to make smarter decisions, avoid biases, and design better interventions that have a genuine impact on our lives.\nAn excellent example application of Causal Machine Learning is in the field of healthcare for personalized treatment recommendations or to estimate the impact of marketing campaigns. Beyonf the classic RCT or A/B testing, in both cases the use of Reinforcement Learning is a very interesting application that we will discuss in the next blog post."
  },
  {
    "objectID": "blog/Quantile Random Forest.html",
    "href": "blog/Quantile Random Forest.html",
    "title": "Quantile Random Forest",
    "section": "",
    "text": "Quantile Random Forest (QRF) is an extension of the traditional Random Forest algorithm that estimates not only the mean but also the entire conditional distribution of the target variable. The distribution is represented by its quantiles, which are specific points that divide the data into segments, such as quartiles (0.25, 0.50, 0.75). QRF is particularly useful when dealing with heteroscedastic data or when we need to assess the uncertainty in our predictions beyond just the central tendency.\nIn QRF, an ensemble of decision trees is constructed. Each decision tree is built by bootstrapping the training data, which means sampling the data points with replacement, and selecting a random subset of features at each split. This creates diversity among the trees in the ensemble.\nThe key difference between Random Forest and QRF lies in the prediction phase. In Random Forest, the predicted value for a new data point is the average (mean) of the individual tree predictions. However, in QRF, we aim to estimate multiple quantiles of the target variable’s distribution for a given input.\nWhen growing a tree in QRF, instead of splitting the data based solely on minimizing the variance or mean squared error, QRF performs splits to optimize quantile-specific criteria. Each tree node is split to minimize the quantile loss function, which reflects the error in predicting the specific quantile. This means that the tree seeks to minimize the difference between the true quantile value and the estimated quantile value for the data points that fall into each node.\nOnce the tree is fully grown, the terminal nodes (leaf nodes) contain specific quantile values rather than just the average value. Each terminal node in the QRF stores a quantile-specific estimate based on the data points that fall into that node. This means that each tree provides quantile-specific predictions.\nTo make predictions for a new data point, QRF evaluates the input through each decision tree, resulting in a set of quantile-specific predictions. The user can then choose the desired quantiles (e.g., 0.25, 0.50, 0.75) to obtain the corresponding quantile predictions. For example, if we want to estimate the 0.75 quantile, we collect the 0.75 quantile values from each tree, and these values form the quantile prediction for that data point.\nOne of the advantages of QRF is that it provides a measure of uncertainty in the predictions. Given the estimated quantiles, QRF can construct a prediction interval for each data point. The prediction interval provides a range of possible values, accounting for the uncertainty in the prediction due to the variability of the underlying data.\nAdvantages of Quantile Random Forest:\n\nRobustness to outliers: QRF can handle outliers effectively as it estimates the entire distribution, including extreme quantiles.\nProvides uncertainty information: By estimating quantiles, QRF offers a measure of uncertainty in the predictions, which is valuable in decision-making.\nVersatility: QRF can be applied to regression and quantile regression problems, and it can be extended to handle multivariate quantile estimation.\n\nApplications of Quantile Random Forest:\n\nFinancial forecasting: QRF can be used to predict quantiles of financial variables, such as stock prices or asset returns, to assess the risk of investment decisions.\nMedical applications: In healthcare, QRF can estimate quantiles of patient response times to treatments, enabling doctors to tailor treatments accordingly.\nEnvironmental modeling: QRF can estimate quantiles of environmental variables like pollutant concentrations, assisting in environmental risk assessments.\n\nIn summary, Quantile Random Forest is a powerful extension of the Random Forest algorithm that estimates the entire distribution of the target variable through quantile-specific predictions. It is useful when capturing the entire distribution or quantifying uncertainty is essential for making informed decisions in various applications such as finance, healthcare, and environmental modeling."
  },
  {
    "objectID": "blog/Embracing Change: Incremental vs. Batch Machine Learning.html",
    "href": "blog/Embracing Change: Incremental vs. Batch Machine Learning.html",
    "title": "Embracing Change: Incremental vs. Batch Machine Learning",
    "section": "",
    "text": "Title: Embracing Change: Incremental vs. Batch Machine Learning\nIntroduction\nMachine learning has revolutionized the way we interact with data, allowing us to uncover valuable insights and make informed decisions. Within the realm of machine learning, there are two fundamental approaches: incremental learning and batch learning. These methods serve distinct purposes and cater to diverse scenarios, offering unique advantages and challenges. In this blog post, we will embark on an exciting journey to understand the differences between incremental and batch machine learning, providing you with the knowledge to embrace these powerful techniques.\nIncremental Machine Learning: An Ever-Evolving Journey\nImagine a learning process that continuously adapts to new information, always striving to stay up-to-date with the latest trends. That’s incremental machine learning for you! Also known as online learning or streaming learning, this approach updates the model in real-time as fresh data arrives. Unlike batch learning, which requires processing the entire dataset at once, incremental learning embraces change, incorporating new observations into the existing model as they arrive.\nCharacteristics of Incremental Machine Learning:\n1. Real-Time Adaptation: The beauty of incremental learning lies in its ability to quickly respond to shifts in data distribution. This makes it ideal for applications dealing with rapidly evolving data, like online advertising or fraud detection.\n2. Low Memory Requirements: Incremental learning processes data in small batches or individual data points, reducing memory usage compared to batch learning, where the entire dataset needs to be stored.\n3. Constant Learning: With incremental learning, models can continuously learn and improve over time without the need for retraining on the entire dataset. It’s like a lifelong learning journey!\nAdvantages of Incremental Machine Learning:\n1. Efficiency: Incremental learning is computationally efficient since it updates the model incrementally, without the need to retrain on the entire dataset each time.\n2. Scalability: This approach is well-suited for large-scale applications where processing the entire dataset at once would be overwhelming.\n3. Dynamic Adaptation: Incremental learning is ideal for applications where data distribution changes frequently, such as natural language processing or recommendation systems. It embraces the unexpected!\nDisadvantages of Incremental Machine Learning:\n1. Forgetting Old Data: Just like we can’t remember everything we’ve learned in the past, incremental learning models may forget information from the past, leading to potential performance degradation on older data points.\n2. Model Drift: In dynamic environments, incremental learning models may struggle to adapt to sudden shifts in data patterns, leading to inaccuracies. Sometimes, change can be overwhelming!\nBatch Machine Learning: A Journey with Stability\nNow, let’s switch gears and explore batch machine learning. Picture a learning process that takes time to analyze and understand the whole story before making decisions. That’s batch machine learning for you! This approach involves training the model on the entire dataset and updating it periodically or when significant amounts of new data accumulate.\nCharacteristics of Batch Machine Learning:\n1. Periodic Updates: Batch learning involves training models from scratch on a fixed dataset, leveraging powerful hardware resources and sophisticated algorithms. It’s like taking a step back to see the bigger picture!\n2. Complete Data Usage: Unlike incremental learning, which processes data in smaller chunks, batch learning uses the entire dataset for training, ensuring no data points are left behind.\nAdvantages of Batch Machine Learning:\n1. Strong Convergence: By training on the entire dataset, batch learning often achieves more stable and reliable results, especially with sufficient computational resources. It’s like taking the time to contemplate before making decisions.\n2. Accuracy: Batch learning models tend to be more accurate due to their exhaustive training process. Sometimes, thoroughness pays off!\nDisadvantages of Batch Machine Learning:\n1. High Resource Demands: Batch learning requires substantial computational power and memory, making it less scalable for real-time applications or large datasets.\n2. Time Lag in Updates: Models may not adapt quickly to changes in data distribution since they are updated only after accumulating new data. Patience is key!\nUse Cases for Incremental and Batch Machine Learning\nIncremental Learning:\n- Real-time fraud detection in financial transactions.\n- Adaptive recommendation systems that learn from user interactions in real-time.\n- Sentiment analysis on streaming social media data.\nBatch Learning:\n- Image classification in computer vision tasks.\n- Natural language processing applications like language translation.\n- Training large-scale recommendation systems periodically.\nConclusion\nAs we bid farewell to this exploration, we have gained an understanding of the intriguing differences between incremental and batch machine learning. Incremental learning embraces change and adapts to evolving data, making it perfect for real-time applications. On the other hand, batch learning takes a more measured approach, achieving strong convergence and accuracy with thorough analysis. Both methods have their unique strengths, empowering data scientists and developers to make informed choices based on the demands of their applications. So, whether you’re venturing on a dynamic journey with incremental learning or taking a stable path with batch learning, remember to embrace the wonders of machine learning and let it lead you to exciting discoveries!"
  },
  {
    "objectID": "blog/Logistic Regression and Marginal Effects.html#interpretation-issues",
    "href": "blog/Logistic Regression and Marginal Effects.html#interpretation-issues",
    "title": "Logistic Regression and Marginal Effects",
    "section": "Interpretation Issues",
    "text": "Interpretation Issues\nIdeally, we want to understand what the model is telling us on the probability scale and not in the odds scale, much less in the estimation scale, the log-odds.\nWe might be tempted to convert log-odds to odds and then to probability and interpret everything as usual. But this would be very wrong!\nWhy?\nThe issue here is that on the probability scale the function is non-linear!"
  },
  {
    "objectID": "blog/Logistic Regression and Marginal Effects.html#logistic-regression",
    "href": "blog/Logistic Regression and Marginal Effects.html#logistic-regression",
    "title": "Logistic Regression and Marginal Effects",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nIn logistic regression, we model the log-odds of the event of interest occurring as a linear combination of predictor variables. The logistic function (sigmoid function) is then applied to this linear combination to convert it into a probability value between 0 and 1.\nThe logistic regression equation is given by:\n\\[\\text{logit}(P(Y=1)) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p\\]\nWhere:\n\n\\(P(Y=1)\\) is the probability of the event of interest (e.g., success) occurring.\n\\(logit (P(Y=1))\\) is the log-odds (logit) of the event occurring.\n\\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_p\\) are the coefficients of the model.\n\\(X_1, X_2, ..., X_p\\) are the predictor variables.\n\nInterpreting Coefficients in Logistic Regression\nThe coefficients \\(\\beta_1, \\beta_2, ..., \\beta_p\\) in the logistic regression equation hold crucial information about the impact of each predictor variable on the log-odds of the event occurring. To interpret these coefficients, we need to consider their signs, magnitude, and statistical significance.\n\nSign of Coefficients:\n\nThe sign of a coefficient reveals the direction of the relationship between the predictor variable and the log-odds of the event occurring. A positive coefficient (\\(\\beta &gt; 0\\)) indicates that an increase in the predictor variable leads to an increase in the log-odds (probability) of the event happening. Conversely, a negative coefficient (\\(\\beta &lt; 0\\)) suggests that an increase in the predictor variable is associated with a decrease in the log-odds (probability) of the event occurring.\n\nMagnitude of Coefficients:\n\nThe magnitude of a coefficient provides information about the strength of the relationship between the predictor variable and the log-odds of the event. Larger absolute values of coefficients indicate stronger influences on the probability.\n\nStatistical Significance:\n\nEvaluating the statistical significance of coefficients is essential to determine if the relationship between a predictor variable and the event probability is meaningful or just due to random chance. Researchers often use hypothesis testing or confidence intervals to assess the statistical significance of coefficients.\nPractical Interpretation Example\nLet’s consider an example to illustrate the interpretation of coefficients in logistic regression. Suppose we are studying the factors that influence whether a customer will make a purchase on an e-commerce website. Our logistic regression model includes two predictor variables: “Time Spent on Website” and “Number of Items Added to Cart.” The estimated coefficients are as follows:\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\)\n\\(\\beta_{\\text{Number of Items}} = 0.2\\)"
  },
  {
    "objectID": "blog/Logistic Regression and Marginal Effects.html#interpreting-coefficients-in-logistic-regression",
    "href": "blog/Logistic Regression and Marginal Effects.html#interpreting-coefficients-in-logistic-regression",
    "title": "Logistic Regression and Marginal Effects",
    "section": "",
    "text": "The coefficients \\(\\beta_1, \\beta_2, ..., \\beta_p\\) in the logistic regression equation hold crucial information about the impact of each predictor variable on the log-odds of the event occurring. To interpret these coefficients, we need to consider their signs, magnitude, and statistical significance.\n\nSign of Coefficients:\n\nThe sign of a coefficient reveals the direction of the relationship between the predictor variable and the log-odds of the event occurring. A positive coefficient (\\(\\beta &gt; 0\\)) indicates that an increase in the predictor variable leads to an increase in the log-odds (probability) of the event happening. Conversely, a negative coefficient (\\(\\beta &lt; 0\\)) suggests that an increase in the predictor variable is associated with a decrease in the log-odds (probability) of the event occurring.\n\nMagnitude of Coefficients:\n\nThe magnitude of a coefficient provides information about the strength of the relationship between the predictor variable and the log-odds of the event. Larger absolute values of coefficients indicate stronger influences on the probability.\n\nStatistical Significance:\n\nEvaluating the statistical significance of coefficients is essential to determine if the relationship between a predictor variable and the event probability is meaningful or just due to random chance. Researchers often use hypothesis testing or confidence intervals to assess the statistical significance of coefficients.\n\n\nLet’s consider an example to illustrate the interpretation of coefficients in logistic regression. Suppose we are studying the factors that influence whether a customer will make a purchase on an e-commerce website. Our logistic regression model includes two predictor variables: “Time Spent on Website” and “Number of Items Added to Cart.” The estimated coefficients are as follows:\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\)\n\\(\\beta_{\\text{Number of Items}} = 0.2\\)\n\n\n\n\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\): For each additional minute a customer spends on the website, the log-odds (probability) of making a purchase increases by 0.05 units. This means that spending more time on the website is associated with a higher likelihood of making a purchase.\n\\(\\beta_{\\text{Number of Items}} = 0.2\\): For each additional item a customer adds to the cart, the log-odds (probability) of making a purchase increases by 0.2 units. This suggests that adding more items to the cart is associated with a higher probability of making a purchase.\n\n\n\n\nIdeally, we want to understand what the model is telling us on the probability scale and not in the odds scale, much less in the estimation scale, the log-odds.\nWe might be tempted to convert log-odds to odds and then to probability and interpret everything as usual. But this would be very wrong!\nWhy?\nThe issue here is that on the probability scale the function is non-linear!"
  },
  {
    "objectID": "blog/Logistic Regression and Marginal Effects.html#interpretation",
    "href": "blog/Logistic Regression and Marginal Effects.html#interpretation",
    "title": "Logistic Regression and Marginal Effects",
    "section": "Interpretation:",
    "text": "Interpretation:\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\): For each additional minute a customer spends on the website, the log-odds (probability) of making a purchase increases by 0.05 units. This means that spending more time on the website is associated with a higher likelihood of making a purchase.\n\\(\\beta_{\\text{Number of Items}} = 0.2\\): For each additional item a customer adds to the cart, the log-odds (probability) of making a purchase increases by 0.2 units. This suggests that adding more items to the cart is associated with a higher probability of making a purchase.\n\n\nInterpretation Issues\nIdeally, we want to understand what the model is telling us on the probability scale and not in the odds scale, much less in the estimation scale, the log-odds.\nWe might be tempted to convert log-odds to odds and then to probability and interpret everything as usual. But this would be very wrong!\nWhy?\nThe issue here is that on the probability scale the function is non-linear!"
  },
  {
    "objectID": "blog/Logistic Regression and Marginal Effects.html#marginal-effects",
    "href": "blog/Logistic Regression and Marginal Effects.html#marginal-effects",
    "title": "Logistic Regression and Marginal Effects",
    "section": "",
    "text": "Marginal effects can be described as the change in outcome as a function of the change in the independent variable of interest holding all other variables in the model constant.\n\nNOTE: In linear regression, the estimated regression coefficients are marginal effects and are more easily interpreted.*\n\nOne measure of change in a system is that the rate of change of the system is non-zero.\nIf we had a simple linear regression model for a trend, then the estimated rate of change would be the slope of the line. Technically, this is the instantaneous rate of change of the function that defines the line.\nThe idea can be extended to any function, even one as potentially complex non-linear function.\nThe problem we have is that in general we don’t have an equation for the non-linear from which we can derive the derivatives.\nSo how do we estimate the derivative of a non-linear function ?\nOne solution is to use the method of finite differences but to understand it properly we need to introduce first the basic concept of differential calculus."
  },
  {
    "objectID": "posts/Unraveling the Power of Causal Machine Learning.html",
    "href": "posts/Unraveling the Power of Causal Machine Learning.html",
    "title": "Unraveling the Power of Causal Machine Learning",
    "section": "",
    "text": "In the realm of traditional machine learning, we primarily focus on identifying patterns and correlations in data to make accurate predictions. While this approach is incredibly useful, it often fails to decipher the cause-and-effect relationships that underpin these patterns. This limitation can lead to misguided conclusions and ineffective decision-making.\nCausality seeks to answer “why” questions, revealing the cause behind an observed effect. Imagine a scenario where we want to understand if a certain medication truly improves patient outcomes or if it’s just correlated with better health. Causal inference allows us to make more meaningful inferences, beyond mere correlations.\n\n\nEstablishing causality is not a straightforward task. The fundamental problem lies in the fact that we cannot simultaneously observe both the treatment group (those who receive a particular intervention) and the control group (those who do not). Traditional observational data often suffers from confounding variables, making it challenging to disentangle cause and effect.\n\n\n\nCausal Machine Learning, a fascinating interdisciplinary field, bridges the gap between traditional machine learning and causal inference. Its primary goal is to leverage data to identify causal relationships and infer the effects of interventions accurately.\n\n\n\n\nRandomized Control Trials (RCTs): Considered the gold standard for establishing causality, RCTs involve randomly assigning subjects to either the treatment or control group. By eliminating confounding factors, RCTs provide strong causal evidence. However, they may not always be practical or ethical.\nPropensity Score Matching (PSM): When RCTs are not feasible, PSM is a popular method. It attempts to create a “quasi-experimental” design by matching treated and control units based on their propensity scores, which represent the likelihood of receiving the treatment. This helps mitigate confounding effects.\nInstrumental Variables (IV): IV analysis relies on instrumental variables that are correlated with the treatment but have no direct effect on the outcome. By utilizing these instruments, researchers can uncover causal relationships even in the presence of unobserved confounders.\nSynthetic Control (SC): The process of creating a synthetic counterfactual involves using historical data from similar individuals or groups that did not receive the treatment to construct a “synthetic” control group. This control group is designed to closely match the characteristics of the treated group before the intervention. By doing so, researchers aim to create a plausible estimate of what would have happened to the treated group had they not received the treatment.\n\n\n\n\nCausal Machine Learning has found applications in various domains, including public health, economics, social sciences, and marketing. It enables policymakers to make informed decisions, businesses to optimize interventions, and researchers to uncover hidden causal mechanisms.\n\n\n\nAs the world becomes more data-driven, understanding causality becomes paramount. Embracing causal machine learning techniques empowers us to make smarter decisions, avoid biases, and design better interventions that have a genuine impact on our lives.\nAn excellent example application of Causal Machine Learning is in the field of healthcare for personalized treatment recommendations or to estimate the impact of marketing campaigns. Beyonf the classic RCT or A/B testing, in both cases the use of Reinforcement Learning is a very interesting application that we will discuss in the next blog post."
  },
  {
    "objectID": "posts/Quantile Random Forest.html",
    "href": "posts/Quantile Random Forest.html",
    "title": "Quantile Random Forest",
    "section": "",
    "text": "Quantile Random Forest (QRF) is an extension of the traditional Random Forest algorithm that estimates not only the mean but also the entire conditional distribution of the target variable. The distribution is represented by its quantiles, which are specific points that divide the data into segments, such as quartiles (0.25, 0.50, 0.75). QRF is particularly useful when dealing with heteroscedastic data or when we need to assess the uncertainty in our predictions beyond just the central tendency.\nIn QRF, an ensemble of decision trees is constructed. Each decision tree is built by bootstrapping the training data, which means sampling the data points with replacement, and selecting a random subset of features at each split. This creates diversity among the trees in the ensemble.\n\n\nThe key difference between Random Forest and QRF lies in the prediction phase. In Random Forest, the predicted value for a new data point is the average (mean) of the individual tree predictions. However, in QRF, we aim to estimate multiple quantiles of the target variable’s distribution for a given input.\nWhen growing a tree in QRF, instead of splitting the data based solely on minimizing the variance or mean squared error, QRF performs splits to optimize quantile-specific criteria. Each tree node is split to minimize the quantile loss function, which reflects the error in predicting the specific quantile. This means that the tree seeks to minimize the difference between the true quantile value and the estimated quantile value for the data points that fall into each node.\nOnce the tree is fully grown, the terminal nodes (leaf nodes) contain specific quantile values rather than just the average value. Each terminal node in the QRF stores a quantile-specific estimate based on the data points that fall into that node. This means that each tree provides quantile-specific predictions.\nTo make predictions for a new data point, QRF evaluates the input through each decision tree, resulting in a set of quantile-specific predictions. The user can then choose the desired quantiles (e.g., 0.25, 0.50, 0.75) to obtain the corresponding quantile predictions. For example, if we want to estimate the 0.75 quantile, we collect the 0.75 quantile values from each tree, and these values form the quantile prediction for that data point.\nOne of the advantages of QRF is that it provides a measure of uncertainty in the predictions. Given the estimated quantiles, QRF can construct a prediction interval for each data point. The prediction interval provides a range of possible values, accounting for the uncertainty in the prediction due to the variability of the underlying data.\nAdvantages of Quantile Random Forest\n\nRobustness to outliers: QRF can handle outliers effectively as it estimates the entire distribution, including extreme quantiles.\nProvides uncertainty information: By estimating quantiles, QRF offers a measure of uncertainty in the predictions, which is valuable in decision-making.\nVersatility: QRF can be applied to regression and quantile regression problems, and it can be extended to handle multivariate quantile estimation.\n\nApplications of Quantile Random Forest\n\nFinancial forecasting: QRF can be used to predict quantiles of financial variables, such as stock prices or asset returns, to assess the risk of investment decisions.\nMedical applications: In healthcare, QRF can estimate quantiles of patient response times to treatments, enabling doctors to tailor treatments accordingly.\nEnvironmental modeling: QRF can estimate quantiles of environmental variables like pollutant concentrations, assisting in environmental risk assessments.\n\n\n\n\nIn summary, Quantile Random Forest is a powerful extension of the Random Forest algorithm that estimates the entire distribution of the target variable through quantile-specific predictions. It is useful when capturing the entire distribution or quantifying uncertainty is essential for making informed decisions in various applications such as finance, healthcare, and environmental modeling."
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html",
    "title": "Embracing Change: Incremental vs. Batch Machine Learning",
    "section": "",
    "text": "Machine learning has revolutionized the way we interact with data, allowing us to uncover valuable insights and make informed decisions. Within the realm of machine learning, there are two fundamental approaches: incremental learning and batch learning. These methods serve distinct purposes and cater to diverse scenarios, offering unique advantages and challenges. In this blog post, we will embark on an exciting journey to understand the differences between incremental and batch machine learning, providing you with the knowledge to embrace these powerful techniques.\n\n\nImagine a learning process that continuously adapts to new information, always striving to stay up-to-date with the latest trends. That’s incremental machine learning for you! Also known as online learning or streaming learning, this approach updates the model in real-time as fresh data arrives. Unlike batch learning, which requires processing the entire dataset at once, incremental learning embraces change, incorporating new observations into the existing model as they arrive.\n\n\n\nReal-Time Adaptation: The beauty of incremental learning lies in its ability to quickly respond to shifts in data distribution. This makes it ideal for applications dealing with rapidly evolving data, like online advertising or fraud detection.\nLow Memory Requirements: Incremental learning processes data in small batches or individual data points, reducing memory usage compared to batch learning, where the entire dataset needs to be stored.\nConstant Learning: With incremental learning, models can continuously learn and improve over time without the need for retraining on the entire dataset. It’s like a lifelong learning journey!\n\nAdvantages of Incremental Machine Learning\n\nEfficiency: Incremental learning is computationally efficient since it updates the model incrementally, without the need to retrain on the entire dataset each time.\nScalability: This approach is well-suited for large-scale applications where processing the entire dataset at once would be overwhelming.\nDynamic Adaptation: Incremental learning is ideal for applications where data distribution changes frequently, such as natural language processing or recommendation systems. It embraces the unexpected!\n\nDisadvantages of Incremental Machine Learning\n\nForgetting Old Data: Just like we can’t remember everything we’ve learned in the past, incremental learning models may forget information from the past, leading to potential performance degradation on older data points.\nModel Drift: In dynamic environments, incremental learning models may struggle to adapt to sudden shifts in data patterns, leading to inaccuracies. Sometimes, change can be overwhelming!\n\n\n\n\n\nNow, let’s switch gears and explore batch machine learning. Picture a learning process that takes time to analyze and understand the whole story before making decisions. That’s batch machine learning for you! This approach involves training the model on the entire dataset and updating it periodically or when significant amounts of new data accumulate.\n\n\n\nPeriodic Updates: Batch learning involves training models from scratch on a fixed dataset, leveraging powerful hardware resources and sophisticated algorithms. It’s like taking a step back to see the bigger picture!\nComplete Data Usage: Unlike incremental learning, which processes data in smaller chunks, batch learning uses the entire dataset for training, ensuring no data points are left behind.\n\nAdvantages of Batch Machine Learning\n\nStrong Convergence: By training on the entire dataset, batch learning often achieves more stable and reliable results, especially with sufficient computational resources. It’s like taking the time to contemplate before making decisions.\nAccuracy: Batch learning models tend to be more accurate due to their exhaustive training process. Sometimes, thoroughness pays off!\n\nDisadvantages of Batch Machine Learning\n\nHigh Resource Demands: Batch learning requires substantial computational power and memory, making it less scalable for real-time applications or large datasets.\nTime Lag in Updates: Models may not adapt quickly to changes in data distribution since they are updated only after accumulating new data. Patience is key!\n\n\n\n\n\nAs we bid farewell to this exploration, we have gained an understanding of the intriguing differences between incremental and batch machine learning. Incremental learning embraces change and adapts to evolving data, making it perfect for real-time applications. On the other hand, batch learning takes a more measured approach, achieving strong convergence and accuracy with thorough analysis. Both methods have their unique strengths, empowering data scientists and developers to make informed choices based on the demands of their applications. So, whether you’re venturing on a dynamic journey with incremental learning or taking a stable path with batch learning, remember to embrace the wonders of machine learning and let it lead you to exciting discoveries!"
  },
  {
    "objectID": "posts/Logistic Regression and Marginal Effects.html",
    "href": "posts/Logistic Regression and Marginal Effects.html",
    "title": "Logistic Regression and Marginal Effects",
    "section": "",
    "text": "Logistic regression is a widely used statistical technique for modeling the relationship between a binary outcome and one or more predictor variables. It is commonly employed in various fields, such as healthcare, finance, and social sciences, to predict the probability of an event occurring. While the coefficients in logistic regression provide valuable insights, they are difficult to interpret. In this blog post, we will explore the use of marginal effects in logistic regression, unraveling their significance and practical applications.\n\n\nIn logistic regression, we model the log-odds of the event of interest occurring as a linear combination of predictor variables. The logistic function (sigmoid function) is then applied to this linear combination to convert it into a probability value between 0 and 1.\nThe logistic regression equation is given by:\n\\[\\text{logit}(P(Y=1)) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p\\]\nWhere:\n\n\\(P(Y=1)\\) is the probability of the event of interest (e.g., success) occurring.\n\\(logit (P(Y=1))\\) is the log-odds (logit) of the event occurring.\n\\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_p\\) are the coefficients of the model.\n\\(X_1, X_2, ..., X_p\\) are the predictor variables.\n\n\n\nThe coefficients \\(\\beta_1, \\beta_2, ..., \\beta_p\\) in the logistic regression equation hold crucial information about the impact of each predictor variable on the log-odds of the event occurring. To interpret these coefficients, we need to consider their signs, magnitude, and statistical significance.\n\nSign of Coefficients:\n\nThe sign of a coefficient reveals the direction of the relationship between the predictor variable and the log-odds of the event occurring. A positive coefficient (\\(\\beta &gt; 0\\)) indicates that an increase in the predictor variable leads to an increase in the log-odds (probability) of the event happening. Conversely, a negative coefficient (\\(\\beta &lt; 0\\)) suggests that an increase in the predictor variable is associated with a decrease in the log-odds (probability) of the event occurring.\n\nMagnitude of Coefficients:\n\nThe magnitude of a coefficient provides information about the strength of the relationship between the predictor variable and the log-odds of the event. Larger absolute values of coefficients indicate stronger influences on the probability.\n\nStatistical Significance:\n\nEvaluating the statistical significance of coefficients is essential to determine if the relationship between a predictor variable and the event probability is meaningful or just due to random chance. Researchers often use hypothesis testing or confidence intervals to assess the statistical significance of coefficients.\n\n\nLet’s consider an example to illustrate the interpretation of coefficients in logistic regression. Suppose we are studying the factors that influence whether a customer will make a purchase on an e-commerce website. Our logistic regression model includes two predictor variables: “Time Spent on Website” and “Number of Items Added to Cart.” The estimated coefficients are as follows:\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\)\n\\(\\beta_{\\text{Number of Items}} = 0.2\\)\n\n\n\n\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\): For each additional minute a customer spends on the website, the log-odds (probability) of making a purchase increases by 0.05 units. This means that spending more time on the website is associated with a higher likelihood of making a purchase.\n\\(\\beta_{\\text{Number of Items}} = 0.2\\): For each additional item a customer adds to the cart, the log-odds (probability) of making a purchase increases by 0.2 units. This suggests that adding more items to the cart is associated with a higher probability of making a purchase.\n\n\n\n\nIdeally, we want to understand what the model is telling us on the probability scale and not in the odds scale, much less in the estimation scale, the log-odds.\nWe might be tempted to convert log-odds to odds and then to probability and interpret everything as usual. But this would be very wrong!\nWhy?\nThe issue here is that on the probability scale the function is non-linear!\n\n\n\n\nMarginal effects can be described as the change in outcome as a function of the change in the independent variable of interest holding all other variables in the model constant.\n\nNOTE: In linear regression, the estimated regression coefficients are marginal effects and are more easily interpreted.*\n\nOne measure of change in a system is that the rate of change of the system is non-zero.\nIf we had a simple linear regression model for a trend, then the estimated rate of change would be the slope of the line. Technically, this is the instantaneous rate of change of the function that defines the line.\nThe idea can be extended to any function, even one as potentially complex non-linear function.\nThe problem we have is that in general we don’t have an equation for the non-linear from which we can derive the derivatives.\nSo how do we estimate the derivative of a non-linear function ?\nOne solution is to use the method of finite differences but to understand it properly we need to introduce first the basic concept of differential calculus.\n\n\n\n\nDifferential calculus is a branch of calculus that focuses on the study of rates of change and slopes of curves. It deals with the concept of derivatives, which represent the rate of change of a function at a specific point. Derivatives allow us to understand how a function is changing at a particular location and provide valuable insights into the behavior of functions.\nThe derivative of a function \\(f(x)\\) with respect to \\(x\\) is denoted as \\(f'(x)\\) or \\(\\frac{df(x)}{dx}\\). It represents the slope of the tangent line to the graph of the function at a given point \\(x\\). The derivative can be interpreted as the instantaneous rate of change of \\(f(x)\\) with respect to \\(x\\).\nFor example, consider the function \\(f(x) = x^2\\). The derivative of \\(f(x)\\) is \\(f'(x) = 2x\\). This means that the slope of the tangent line to the graph of \\(f(x)\\) at any point \\(x\\) is twice the value of \\(x\\) at that point.\nDifferential calculus has wide-ranging applications in various fields, including physics, engineering, economics, and more. It is used to model and analyze phenomena that involve continuous changes, such as motion, growth, and optimization.\nFinite differences are a numerical technique used to approximate derivatives of functions. The method involves computing the difference in function values at two nearby points and then dividing by the difference in their corresponding independent variable values. By using a small interval between the two points, we can get an approximation of the derivative at a specific point.\nThe finite difference formula for approximating the first derivative of a function \\(f(x)\\) at a point \\(x\\) is given by:\n\\[ f’(x) \\approx \\frac{f(x + h) - f(x)}{h} \\]\nWhere:\n\n\\(h\\) is a small value representing the step size or interval between the points.\n\nSimilarly, we can use finite differences to approximate higher-order derivatives, such as the second derivative:\n\\[ f’’(x) \\approx \\frac{f(x + h) - 2f(x) + f(x - h)}{h^2} \\]\nFinite differences are particularly useful when the analytical expression for the derivative is difficult to obtain or when dealing with discrete data points instead of continuous functions.\nDifferential calculus and finite differences are both essential tools in mathematics and scientific disciplines. Differential calculus provides a rigorous framework for studying rates of change and slopes of curves through derivatives. On the other hand, finite differences offer a practical and numerical approach to approximate derivatives when the analytical solution is not readily available or when dealing with discrete data. Together, these concepts enable us to better understand the behavior of functions and analyze real-world phenomena involving continuous and discrete changes.\n\n\n\nIn logistic regression, the estimated coefficients (log odds) indicate how the log odds of the outcome change with a one-unit increase in the predictor variable while holding other variables constant. However, interpreting these coefficients can be challenging, especially for non-statisticians. This is where marginal effects come to the rescue.\nMarginal effects provide a more straightforward interpretation by measuring the impact of a one-unit change in the predictor variable on the probability of the event occurring (i.e., the probability of success). They express the change in the probability as a result of the predictor variable’s change, allowing us to understand the practical implications of the model."
  },
  {
    "objectID": "posts/Logistic Regression and Marginal Effects.html#interpreting-coefficients-in-logistic-regression",
    "href": "posts/Logistic Regression and Marginal Effects.html#interpreting-coefficients-in-logistic-regression",
    "title": "Logistic Regression and Marginal Effects",
    "section": "",
    "text": "The coefficients \\(\\beta_1, \\beta_2, ..., \\beta_p\\) in the logistic regression equation hold crucial information about the impact of each predictor variable on the log-odds of the event occurring. To interpret these coefficients, we need to consider their signs, magnitude, and statistical significance.\n\nSign of Coefficients:\n\nThe sign of a coefficient reveals the direction of the relationship between the predictor variable and the log-odds of the event occurring. A positive coefficient (\\(\\beta &gt; 0\\)) indicates that an increase in the predictor variable leads to an increase in the log-odds (probability) of the event happening. Conversely, a negative coefficient (\\(\\beta &lt; 0\\)) suggests that an increase in the predictor variable is associated with a decrease in the log-odds (probability) of the event occurring.\n\nMagnitude of Coefficients:\n\nThe magnitude of a coefficient provides information about the strength of the relationship between the predictor variable and the log-odds of the event. Larger absolute values of coefficients indicate stronger influences on the probability.\n\nStatistical Significance:\n\nEvaluating the statistical significance of coefficients is essential to determine if the relationship between a predictor variable and the event probability is meaningful or just due to random chance. Researchers often use hypothesis testing or confidence intervals to assess the statistical significance of coefficients.\n\n\nLet’s consider an example to illustrate the interpretation of coefficients in logistic regression. Suppose we are studying the factors that influence whether a customer will make a purchase on an e-commerce website. Our logistic regression model includes two predictor variables: “Time Spent on Website” and “Number of Items Added to Cart.” The estimated coefficients are as follows:\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\)\n\\(\\beta_{\\text{Number of Items}} = 0.2\\)\n\n\n\n\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\): For each additional minute a customer spends on the website, the log-odds (probability) of making a purchase increases by 0.05 units. This means that spending more time on the website is associated with a higher likelihood of making a purchase.\n\\(\\beta_{\\text{Number of Items}} = 0.2\\): For each additional item a customer adds to the cart, the log-odds (probability) of making a purchase increases by 0.2 units. This suggests that adding more items to the cart is associated with a higher probability of making a purchase.\n\n\n\n\nIdeally, we want to understand what the model is telling us on the probability scale and not in the odds scale, much less in the estimation scale, the log-odds.\nWe might be tempted to convert log-odds to odds and then to probability and interpret everything as usual. But this would be very wrong!\nWhy?\nThe issue here is that on the probability scale the function is non-linear!"
  },
  {
    "objectID": "posts/Logistic Regression and Marginal Effects.html#marginal-effects",
    "href": "posts/Logistic Regression and Marginal Effects.html#marginal-effects",
    "title": "Logistic Regression and Marginal Effects",
    "section": "",
    "text": "Marginal effects can be described as the change in outcome as a function of the change in the independent variable of interest holding all other variables in the model constant.\n\nNOTE: In linear regression, the estimated regression coefficients are marginal effects and are more easily interpreted.*\n\nOne measure of change in a system is that the rate of change of the system is non-zero.\nIf we had a simple linear regression model for a trend, then the estimated rate of change would be the slope of the line. Technically, this is the instantaneous rate of change of the function that defines the line.\nThe idea can be extended to any function, even one as potentially complex non-linear function.\nThe problem we have is that in general we don’t have an equation for the non-linear from which we can derive the derivatives.\nSo how do we estimate the derivative of a non-linear function ?\nOne solution is to use the method of finite differences but to understand it properly we need to introduce first the basic concept of differential calculus."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "🎙️Blog",
    "section": "",
    "text": "Unraveling the Power of Causal Machine Learning\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nQuantile Random Forest\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nEmbracing Change: Incremental vs. Batch Machine Learning\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nLogistic Regression and Marginal Effects\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nContextual Multi-Armed Bandit: Maximizing Rewards with Intelligent Decision-Making\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWhen in doubt, just model it. Modelling uncertainty\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#incremental-machine-learning-an-ever-evolving-journey",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#incremental-machine-learning-an-ever-evolving-journey",
    "title": "Embracing Change: Incremental vs. Batch Machine Learning",
    "section": "",
    "text": "Imagine a learning process that continuously adapts to new information, always striving to stay up-to-date with the latest trends. That’s incremental machine learning for you! Also known as online learning or streaming learning, this approach updates the model in real-time as fresh data arrives. Unlike batch learning, which requires processing the entire dataset at once, incremental learning embraces change, incorporating new observations into the existing model as they arrive.\n\n\n\nReal-Time Adaptation: The beauty of incremental learning lies in its ability to quickly respond to shifts in data distribution. This makes it ideal for applications dealing with rapidly evolving data, like online advertising or fraud detection.\nLow Memory Requirements: Incremental learning processes data in small batches or individual data points, reducing memory usage compared to batch learning, where the entire dataset needs to be stored.\nConstant Learning: With incremental learning, models can continuously learn and improve over time without the need for retraining on the entire dataset. It’s like a lifelong learning journey!\n\nAdvantages of Incremental Machine Learning\n\nEfficiency: Incremental learning is computationally efficient since it updates the model incrementally, without the need to retrain on the entire dataset each time.\nScalability: This approach is well-suited for large-scale applications where processing the entire dataset at once would be overwhelming.\nDynamic Adaptation: Incremental learning is ideal for applications where data distribution changes frequently, such as natural language processing or recommendation systems. It embraces the unexpected!\n\nDisadvantages of Incremental Machine Learning\n\nForgetting Old Data: Just like we can’t remember everything we’ve learned in the past, incremental learning models may forget information from the past, leading to potential performance degradation on older data points.\nModel Drift: In dynamic environments, incremental learning models may struggle to adapt to sudden shifts in data patterns, leading to inaccuracies. Sometimes, change can be overwhelming!"
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#batch-machine-learning-a-journey-with-stability",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#batch-machine-learning-a-journey-with-stability",
    "title": "Embracing Change: Incremental vs. Batch Machine Learning",
    "section": "",
    "text": "Now, let’s switch gears and explore batch machine learning. Picture a learning process that takes time to analyze and understand the whole story before making decisions. That’s batch machine learning for you! This approach involves training the model on the entire dataset and updating it periodically or when significant amounts of new data accumulate.\n\n\n\nPeriodic Updates: Batch learning involves training models from scratch on a fixed dataset, leveraging powerful hardware resources and sophisticated algorithms. It’s like taking a step back to see the bigger picture!\nComplete Data Usage: Unlike incremental learning, which processes data in smaller chunks, batch learning uses the entire dataset for training, ensuring no data points are left behind.\n\nAdvantages of Batch Machine Learning\n\nStrong Convergence: By training on the entire dataset, batch learning often achieves more stable and reliable results, especially with sufficient computational resources. It’s like taking the time to contemplate before making decisions.\nAccuracy: Batch learning models tend to be more accurate due to their exhaustive training process. Sometimes, thoroughness pays off!\n\nDisadvantages of Batch Machine Learning\n\nHigh Resource Demands: Batch learning requires substantial computational power and memory, making it less scalable for real-time applications or large datasets.\nTime Lag in Updates: Models may not adapt quickly to changes in data distribution since they are updated only after accumulating new data. Patience is key!"
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#advantages-of-batch-machine-learning",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#advantages-of-batch-machine-learning",
    "title": "Embracing Change: Incremental vs. Batch Machine Learning",
    "section": "",
    "text": "Strong Convergence: By training on the entire dataset, batch learning often achieves more stable and reliable results, especially with sufficient computational resources. It’s like taking the time to contemplate before making decisions.\nAccuracy: Batch learning models tend to be more accurate due to their exhaustive training process. Sometimes, thoroughness pays off!"
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#disadvantages-of-batch-machine-learning",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#disadvantages-of-batch-machine-learning",
    "title": "Embracing Change: Incremental vs. Batch Machine Learning",
    "section": "",
    "text": "High Resource Demands: Batch learning requires substantial computational power and memory, making it less scalable for real-time applications or large datasets.\nTime Lag in Updates: Models may not adapt quickly to changes in data distribution since they are updated only after accumulating new data. Patience is key!\n\nUse Cases for Incremental and Batch Machine Learning"
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#incremental-learning",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#incremental-learning",
    "title": "Embracing Change: Incremental vs. Batch Machine Learning",
    "section": "",
    "text": "Real-time fraud detection in financial transactions.\nAdaptive recommendation systems that learn from user interactions in real-time.\nSentiment analysis on streaming social media data.\n\nBatch Learning:\n\nImage classification in computer vision tasks.\nNatural language processing applications like language translation.\nTraining large-scale recommendation systems periodically."
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#conclusion",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#conclusion",
    "title": "Embracing Change: Incremental vs. Batch Machine Learning",
    "section": "",
    "text": "As we bid farewell to this exploration, we have gained an understanding of the intriguing differences between incremental and batch machine learning. Incremental learning embraces change and adapts to evolving data, making it perfect for real-time applications. On the other hand, batch learning takes a more measured approach, achieving strong convergence and accuracy with thorough analysis. Both methods have their unique strengths, empowering data scientists and developers to make informed choices based on the demands of their applications. So, whether you’re venturing on a dynamic journey with incremental learning or taking a stable path with batch learning, remember to embrace the wonders of machine learning and let it lead you to exciting discoveries!"
  },
  {
    "objectID": "posts/Unraveling the Power of Causal Machine Learning.html#the-challenge-of-causal-inference",
    "href": "posts/Unraveling the Power of Causal Machine Learning.html#the-challenge-of-causal-inference",
    "title": "Unraveling the Power of Causal Machine Learning",
    "section": "",
    "text": "Establishing causality is not a straightforward task. The fundamental problem lies in the fact that we cannot simultaneously observe both the treatment group (those who receive a particular intervention) and the control group (those who do not). Traditional observational data often suffers from confounding variables, making it challenging to disentangle cause and effect."
  },
  {
    "objectID": "posts/Unraveling the Power of Causal Machine Learning.html#enter-causal-machine-learning",
    "href": "posts/Unraveling the Power of Causal Machine Learning.html#enter-causal-machine-learning",
    "title": "Unraveling the Power of Causal Machine Learning",
    "section": "",
    "text": "Causal Machine Learning, a fascinating interdisciplinary field, bridges the gap between traditional machine learning and causal inference. Its primary goal is to leverage data to identify causal relationships and infer the effects of interventions accurately."
  },
  {
    "objectID": "posts/Unraveling the Power of Causal Machine Learning.html#causal-inference-methods",
    "href": "posts/Unraveling the Power of Causal Machine Learning.html#causal-inference-methods",
    "title": "Unraveling the Power of Causal Machine Learning",
    "section": "",
    "text": "Randomized Control Trials (RCTs): Considered the gold standard for establishing causality, RCTs involve randomly assigning subjects to either the treatment or control group. By eliminating confounding factors, RCTs provide strong causal evidence. However, they may not always be practical or ethical.\nPropensity Score Matching (PSM): When RCTs are not feasible, PSM is a popular method. It attempts to create a “quasi-experimental” design by matching treated and control units based on their propensity scores, which represent the likelihood of receiving the treatment. This helps mitigate confounding effects.\nInstrumental Variables (IV): IV analysis relies on instrumental variables that are correlated with the treatment but have no direct effect on the outcome. By utilizing these instruments, researchers can uncover causal relationships even in the presence of unobserved confounders.\nSynthetic Control (SC): The process of creating a synthetic counterfactual involves using historical data from similar individuals or groups that did not receive the treatment to construct a “synthetic” control group. This control group is designed to closely match the characteristics of the treated group before the intervention. By doing so, researchers aim to create a plausible estimate of what would have happened to the treated group had they not received the treatment."
  },
  {
    "objectID": "posts/Unraveling the Power of Causal Machine Learning.html#real-world-applications",
    "href": "posts/Unraveling the Power of Causal Machine Learning.html#real-world-applications",
    "title": "Unraveling the Power of Causal Machine Learning",
    "section": "",
    "text": "Causal Machine Learning has found applications in various domains, including public health, economics, social sciences, and marketing. It enables policymakers to make informed decisions, businesses to optimize interventions, and researchers to uncover hidden causal mechanisms."
  },
  {
    "objectID": "posts/Unraveling the Power of Causal Machine Learning.html#embracing-causal-machine-learning",
    "href": "posts/Unraveling the Power of Causal Machine Learning.html#embracing-causal-machine-learning",
    "title": "Unraveling the Power of Causal Machine Learning",
    "section": "",
    "text": "As the world becomes more data-driven, understanding causality becomes paramount. Embracing causal machine learning techniques empowers us to make smarter decisions, avoid biases, and design better interventions that have a genuine impact on our lives.\nAn excellent example application of Causal Machine Learning is in the field of healthcare for personalized treatment recommendations or to estimate the impact of marketing campaigns. Beyonf the classic RCT or A/B testing, in both cases the use of Reinforcement Learning is a very interesting application that we will discuss in the next blog post."
  },
  {
    "objectID": "posts/Quantile Random Forest.html#random-forest-vs-qrf",
    "href": "posts/Quantile Random Forest.html#random-forest-vs-qrf",
    "title": "Quantile Random Forest",
    "section": "",
    "text": "The key difference between Random Forest and QRF lies in the prediction phase. In Random Forest, the predicted value for a new data point is the average (mean) of the individual tree predictions. However, in QRF, we aim to estimate multiple quantiles of the target variable’s distribution for a given input.\nWhen growing a tree in QRF, instead of splitting the data based solely on minimizing the variance or mean squared error, QRF performs splits to optimize quantile-specific criteria. Each tree node is split to minimize the quantile loss function, which reflects the error in predicting the specific quantile. This means that the tree seeks to minimize the difference between the true quantile value and the estimated quantile value for the data points that fall into each node.\nOnce the tree is fully grown, the terminal nodes (leaf nodes) contain specific quantile values rather than just the average value. Each terminal node in the QRF stores a quantile-specific estimate based on the data points that fall into that node. This means that each tree provides quantile-specific predictions.\nTo make predictions for a new data point, QRF evaluates the input through each decision tree, resulting in a set of quantile-specific predictions. The user can then choose the desired quantiles (e.g., 0.25, 0.50, 0.75) to obtain the corresponding quantile predictions. For example, if we want to estimate the 0.75 quantile, we collect the 0.75 quantile values from each tree, and these values form the quantile prediction for that data point.\nOne of the advantages of QRF is that it provides a measure of uncertainty in the predictions. Given the estimated quantiles, QRF can construct a prediction interval for each data point. The prediction interval provides a range of possible values, accounting for the uncertainty in the prediction due to the variability of the underlying data.\nAdvantages of Quantile Random Forest\n\nRobustness to outliers: QRF can handle outliers effectively as it estimates the entire distribution, including extreme quantiles.\nProvides uncertainty information: By estimating quantiles, QRF offers a measure of uncertainty in the predictions, which is valuable in decision-making.\nVersatility: QRF can be applied to regression and quantile regression problems, and it can be extended to handle multivariate quantile estimation.\n\nApplications of Quantile Random Forest\n\nFinancial forecasting: QRF can be used to predict quantiles of financial variables, such as stock prices or asset returns, to assess the risk of investment decisions.\nMedical applications: In healthcare, QRF can estimate quantiles of patient response times to treatments, enabling doctors to tailor treatments accordingly.\nEnvironmental modeling: QRF can estimate quantiles of environmental variables like pollutant concentrations, assisting in environmental risk assessments."
  },
  {
    "objectID": "posts/Quantile Random Forest.html#advantages",
    "href": "posts/Quantile Random Forest.html#advantages",
    "title": "Quantile Random Forest",
    "section": "",
    "text": "One of the advantages of QRF is that it provides a measure of uncertainty in the predictions. Given the estimated quantiles, QRF can construct a prediction interval for each data point. The prediction interval provides a range of possible values, accounting for the uncertainty in the prediction due to the variability of the underlying data.\nAdvantages of Quantile Random Forest\n\nRobustness to outliers: QRF can handle outliers effectively as it estimates the entire distribution, including extreme quantiles.\nProvides uncertainty information: By estimating quantiles, QRF offers a measure of uncertainty in the predictions, which is valuable in decision-making.\nVersatility: QRF can be applied to regression and quantile regression problems, and it can be extended to handle multivariate quantile estimation.\n\nApplications of Quantile Random Forest\n\nFinancial forecasting: QRF can be used to predict quantiles of financial variables, such as stock prices or asset returns, to assess the risk of investment decisions.\nMedical applications: In healthcare, QRF can estimate quantiles of patient response times to treatments, enabling doctors to tailor treatments accordingly.\nEnvironmental modeling: QRF can estimate quantiles of environmental variables like pollutant concentrations, assisting in environmental risk assessments."
  },
  {
    "objectID": "posts/Contextual Multi-Armed Bandit.html",
    "href": "posts/Contextual Multi-Armed Bandit.html",
    "title": "Contextual Multi-Armed Bandit: Maximizing Rewards with Intelligent Decision-Making",
    "section": "",
    "text": "Imagine a gambler facing a row of slot machines (arms), each with an unknown payout probability. The gambler’s goal is to maximize their cumulative reward while exploring the machines’ potential and exploiting the most promising ones. In recent years, the contextual multi-armed bandit (CMAB) has emerged as a powerful extension to this problem, incorporating context or additional information to make even more intelligent decisions. In this blog post, we will delve into the world of contextual multi-armed bandits, exploring their significance, strategies, and real-world applications.\n\n\nIn the classic multi-armed bandit problem, a gambler is faced with a set of arms (slot machines) that have different reward probabilities, but the gambler doesn’t know these probabilities initially. The gambler’s objective is to determine which arms to pull (explore) and which to favor (exploit) over time to maximize their cumulative reward. The challenge lies in striking a balance between exploring new arms to gather more information and exploiting the arms that seem to provide higher rewards.\n\n\n\nThe contextual multi-armed bandit takes the classic problem a step further by incorporating additional information or context. In CMAB, each arm is associated with a context, which can be thought of as features or attributes characterizing the current situation. This context provides valuable insights into the arms’ potential rewards and helps the decision-maker make more informed choices.\nFor instance, imagine a digital advertising scenario where ads (arms) are shown to users, and each ad has associated user characteristics as context, such as age, location, and interests. By considering the context, the CMAB algorithm can optimize the selection of ads for each user to maximize clicks or conversions.\n\n\n\nLinUCB Algorithm:\n\nOne popular approach for solving CMAB problems is the LinUCB algorithm. It uses linear models to estimate the expected rewards for each arm given the context. The algorithm maintains a confidence interval for each arm’s expected reward and selects the arm with the highest upper confidence bound, balancing exploration and exploitation.\n\nThompson Sampling:\n\nThompson Sampling is a widely used Bayesian approach for CMAB. It models the reward distribution for each arm based on observed rewards and context. The algorithm then samples from these distributions and selects the arm with the highest sample. This method effectively leverages uncertainty to explore and exploit in a principled manner.\nReal-World Applications of Contextual Multi-Armed Bandits\n\nPersonalized Recommendations:\n\nCMAB algorithms can be applied to personalized recommendation systems, where the context represents user preferences, past behavior, and demographics. By dynamically selecting the most relevant content or products, these systems can improve user engagement and satisfaction.\n\nHealthcare Treatment Selection:\n\nIn healthcare, CMAB can help optimize treatment selection based on patient characteristics, medical history, and response to previous treatments. By considering the context, doctors can make more data-driven decisions to improve patient outcomes.\n\nOnline Advertising:\n\nIn digital advertising, CMAB can enhance ad placement by taking into account user profiles, browsing behavior, and real-time context. This enables advertisers to show the most relevant ads to users, increasing click-through rates and conversions.\n\n\n\n\nContextual Multi-Armed Bandit is a powerful extension of the classic multi-armed bandit problem, incorporating additional information or context to make more intelligent decisions. By leveraging context, CMAB algorithms strike a balance between exploring new options and exploiting the most promising ones, maximizing cumulative rewards in various real-world scenarios. From personalized recommendations to healthcare treatment selection and online advertising, CMAB continues to revolutionize decision-making, offering smarter and more efficient solutions to challenging problems. As the field of reinforcement learning and contextual decision-making advances, we can expect even more exciting applications and innovations in the years to come."
  },
  {
    "objectID": "posts/Contextual Multi-Armed Bandit.html#strategies-for-contextual-multi-armed-bandits",
    "href": "posts/Contextual Multi-Armed Bandit.html#strategies-for-contextual-multi-armed-bandits",
    "title": "Contextual Multi-Armed Bandit: Maximizing Rewards with Intelligent Decision-Making",
    "section": "",
    "text": "LinUCB Algorithm:\n\nOne popular approach for solving CMAB problems is the LinUCB algorithm. It uses linear models to estimate the expected rewards for each arm given the context. The algorithm maintains a confidence interval for each arm’s expected reward and selects the arm with the highest upper confidence bound, balancing exploration and exploitation.\n\nThompson Sampling:\n\nThompson Sampling is a widely used Bayesian approach for CMAB. It models the reward distribution for each arm based on observed rewards and context. The algorithm then samples from these distributions and selects the arm with the highest sample. This method effectively leverages uncertainty to explore and exploit in a principled manner.\nReal-World Applications of Contextual Multi-Armed Bandits\n\nPersonalized Recommendations:\n\nCMAB algorithms can be applied to personalized recommendation systems, where the context represents user preferences, past behavior, and demographics. By dynamically selecting the most relevant content or products, these systems can improve user engagement and satisfaction.\n\nHealthcare Treatment Selection:\n\nIn healthcare, CMAB can help optimize treatment selection based on patient characteristics, medical history, and response to previous treatments. By considering the context, doctors can make more data-driven decisions to improve patient outcomes.\n\nOnline Advertising:\n\nIn digital advertising, CMAB can enhance ad placement by taking into account user profiles, browsing behavior, and real-time context. This enables advertisers to show the most relevant ads to users, increasing click-through rates and conversions."
  },
  {
    "objectID": "posts/When in doubt, just model it. Modelling uncertainty.html",
    "href": "posts/When in doubt, just model it. Modelling uncertainty.html",
    "title": "When in doubt, just model it. Modelling uncertainty",
    "section": "",
    "text": "Probabilistic modeling is a powerful approach in statistics and machine learning that enables us to quantify uncertainty in our predictions and decisions. It allows us to represent and reason about uncertain or incomplete information by incorporating probability distributions over the model parameters or the outcomes themselves. In this context, we can distinguish between two types of uncertainty: uncertainty and deep uncertainty.\n\n\nUncertainty refers to the lack of perfect knowledge about a particular event or outcome. In probabilistic modeling, uncertainty is captured by probability distributions. These distributions represent our beliefs about the likelihood of different outcomes or the variability in model parameters. By using probability distributions, we can express the confidence or lack thereof in our predictions.\nFor example, in linear regression, instead of providing a single point estimate for the model coefficients, we can use a probability distribution to describe the uncertainty around these coefficients. This allows us to understand the range of plausible values and the level of confidence we have in our estimates.\nProbabilistic modeling allows us to make more robust decisions by considering the uncertainty and accounting for potential errors in our predictions. It is widely used in various applications, including finance, healthcare, weather forecasting, and natural language processing.\n\n\n\nDeep uncertainty, on the other hand, goes beyond simple uncertainty and arises when we have limited knowledge about the underlying data-generating process or when there is ambiguity in the model assumptions. In such cases, traditional probabilistic models may not be sufficient to capture the full extent of uncertainty.\nDeep uncertainty is particularly prevalent in complex and chaotic systems, where there are many interacting variables and non-linear relationships that are difficult to model precisely. In these scenarios, traditional probabilistic models may yield unreliable estimates or may not provide meaningful uncertainty quantification.\nTo address deep uncertainty, researchers have developed alternative methods, such as robust optimization, scenario-based analysis, and Bayesian non-parametric models. These techniques are designed to handle situations where the underlying assumptions are uncertain or where we lack sufficient data to make accurate probabilistic estimates.\nIn recent years, the development of deep learning and Bayesian deep learning has also allowed us to tackle deep uncertainty in more complex models. Bayesian neural networks, for example, use Bayesian inference to represent uncertainty in neural network weights, enabling better uncertainty quantification in deep learning models.\n\n\n\nProbabilistic modeling is a versatile tool for quantifying uncertainty in various applications. It allows us to represent our beliefs probabilistically and make more informed decisions based on the available information. However, in situations where uncertainty is deeper and the underlying data-generating process is complex or uncertain, traditional probabilistic models may not suffice. In these cases, advanced techniques like Bayesian non-parametric models and Bayesian deep learning can help us handle deep uncertainty and make more robust predictions and decisions in challenging and uncertain environments. As our understanding of uncertainty and deep uncertainty advances, probabilistic modeling will continue to play a pivotal role in tackling real-world problems with incomplete information."
  },
  {
    "objectID": "posts/When in doubt, just model it. Modelling uncertainty.html#uncertainty",
    "href": "posts/When in doubt, just model it. Modelling uncertainty.html#uncertainty",
    "title": "When in doubt, just model it. Modelling uncertainty",
    "section": "",
    "text": "Uncertainty refers to the lack of perfect knowledge about a particular event or outcome. In probabilistic modeling, uncertainty is captured by probability distributions. These distributions represent our beliefs about the likelihood of different outcomes or the variability in model parameters. By using probability distributions, we can express the confidence or lack thereof in our predictions.\nFor example, in linear regression, instead of providing a single point estimate for the model coefficients, we can use a probability distribution to describe the uncertainty around these coefficients. This allows us to understand the range of plausible values and the level of confidence we have in our estimates.\nProbabilistic modeling allows us to make more robust decisions by considering the uncertainty and accounting for potential errors in our predictions. It is widely used in various applications, including finance, healthcare, weather forecasting, and natural language processing."
  },
  {
    "objectID": "posts/When in doubt, just model it. Modelling uncertainty.html#deep-uncertainty",
    "href": "posts/When in doubt, just model it. Modelling uncertainty.html#deep-uncertainty",
    "title": "When in doubt, just model it. Modelling uncertainty",
    "section": "",
    "text": "Deep uncertainty, on the other hand, goes beyond simple uncertainty and arises when we have limited knowledge about the underlying data-generating process or when there is ambiguity in the model assumptions. In such cases, traditional probabilistic models may not be sufficient to capture the full extent of uncertainty.\nDeep uncertainty is particularly prevalent in complex and chaotic systems, where there are many interacting variables and non-linear relationships that are difficult to model precisely. In these scenarios, traditional probabilistic models may yield unreliable estimates or may not provide meaningful uncertainty quantification.\nTo address deep uncertainty, researchers have developed alternative methods, such as robust optimization, scenario-based analysis, and Bayesian non-parametric models. These techniques are designed to handle situations where the underlying assumptions are uncertain or where we lack sufficient data to make accurate probabilistic estimates.\nIn recent years, the development of deep learning and Bayesian deep learning has also allowed us to tackle deep uncertainty in more complex models. Bayesian neural networks, for example, use Bayesian inference to represent uncertainty in neural network weights, enabling better uncertainty quantification in deep learning models."
  }
]