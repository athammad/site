[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "🛰 Research",
    "section": "",
    "text": "The majority of my current research revolves around the nexus between causal inference, machine learning and environmental analysis. In the last few years my focus has been on topics such as climate change and extreme events but I had the chance to explore other avenues such as optimal allocation of health care facilities and transport networks analysis.\nMy interest is in exploring the interplay between causal inference, machine learning, and environmental analysis. The urgency of addressing environmental challenges necessitates innovative approaches to understanding the complex relationships between human activities, environmental factors, and their consequences. By combining techniques from causal inference and machine learning, I aim to develop novel methodologies that can effectively analyse environmental data, identify causal relationships, and inform evidence-based decision-making for sustainable environmental management.\n\n\n\n\nThe advent of big data and the increasing availability of diverse environmental datasets have provided researchers with an unprecedented opportunity to investigate complex environmental systems. However, extracting meaningful insights from these datasets poses significant challenges due to confounding factors, selection biases, and non-randomness inherent in observational data. Traditional statistical methods often fall short in capturing the complex and non-linear relationships within environmental systems. Therefore, there is a pressing need to integrate causal inference and machine learning techniques to overcome these limitations and achieve a deeper understanding of environmental dynamics.\n\n\n\n\n\nMachine learning algorithms have shown remarkable success in pattern recognition, predictive modelling, and data-driven decision-making. The ability to automatically extract complex patterns and relationships from large-scale environmental datasets makes machine learning an invaluable tool for environmental analysis. By leveraging machine learning techniques, develop predictive models to forecast environmental outcomes and assess the potential impacts of policy interventions.. Furthermore, machine learning methods can aid in feature selection, data imputation, and anomaly detection, contributing to improved data quality and reliability for environmental analysis.\n\n\n\n\n\nCausal inference provides a powerful framework for identifying cause-and-effect relationships within complex systems. By applying causal inference methods, we can go beyond correlations and uncover the underlying causal mechanisms driving observed environmental phenomena. Approaches such as propensity score matching, instrumental variables, and difference-in-differences analysis allow us to address confounding biases and estimate causal effects more accurately. Incorporating these causal inference techniques into environmental analysis can enhance our ability to discern the impacts of specific environmental factors and human activities on various ecological systems.\n\n\n\n\n\nIntegrating causal inference with machine learning techniques presents exciting opportunities for advancing environmental analysis. By combining the strengths of both fields, we can develop robust methodologies that provide accurate causal estimates while harnessing the predictive power of machine learning algorithms. However, several challenges need to be addressed to achieve this integration successfully. These challenges include the treatment of unmeasured confounding, scalability of methods for large environmental datasets, interpretability of complex machine learning models, and the consideration of uncertainty in causal inference.\n\n\n\n\n\nIn my research, I aim to address these challenges and contribute to the nexus between causal inference, machine learning, and environmental analysis. Specifically, I intend to:\n\nDevelop novel methodologies that integrate causal inference and machine learning techniques to estimate causal effects and predict environmental outcomes accurately.\nInvestigate the use of deep learning architectures and explainable AI methods to enhance the interpretability of complex machine learning models for environmental analysis.\nExplore scalable algorithms and parallel computing techniques to handle the computational demands of large environmental datasets.\nInvestigate methods for handling unmeasured confounding and uncertainties inherent in causal inference within the context of environmental analysis.\nApply the developed methodologies to real-world environmental challenges, such as climate change and extreme event analysis. By pursuing these research objectives, I aspire to advance the understanding of causal relationships in environmental systems, contribute to evidence-based decision-making for environmental management, and ultimately support the development of sustainable practices for a better future."
  },
  {
    "objectID": "research.html#background",
    "href": "research.html#background",
    "title": "🛰 Research",
    "section": "",
    "text": "The advent of big data and the increasing availability of diverse environmental datasets have provided researchers with an unprecedented opportunity to investigate complex environmental systems. However, extracting meaningful insights from these datasets poses significant challenges due to confounding factors, selection biases, and non-randomness inherent in observational data. Traditional statistical methods often fall short in capturing the complex and non-linear relationships within environmental systems. Therefore, there is a pressing need to integrate causal inference and machine learning techniques to overcome these limitations and achieve a deeper understanding of environmental dynamics."
  },
  {
    "objectID": "research.html#machine-learning",
    "href": "research.html#machine-learning",
    "title": "🛰 Research",
    "section": "",
    "text": "Machine learning algorithms have shown remarkable success in pattern recognition, predictive modelling, and data-driven decision-making. The ability to automatically extract complex patterns and relationships from large-scale environmental datasets makes machine learning an invaluable tool for environmental analysis. By leveraging machine learning techniques, develop predictive models to forecast environmental outcomes and assess the potential impacts of policy interventions.. Furthermore, machine learning methods can aid in feature selection, data imputation, and anomaly detection, contributing to improved data quality and reliability for environmental analysis."
  },
  {
    "objectID": "research.html#causal-inference",
    "href": "research.html#causal-inference",
    "title": "🛰 Research",
    "section": "",
    "text": "Causal inference provides a powerful framework for identifying cause-and-effect relationships within complex systems. By applying causal inference methods, we can go beyond correlations and uncover the underlying causal mechanisms driving observed environmental phenomena. Approaches such as propensity score matching, instrumental variables, and difference-in-differences analysis allow us to address confounding biases and estimate causal effects more accurately. Incorporating these causal inference techniques into environmental analysis can enhance our ability to discern the impacts of specific environmental factors and human activities on various ecological systems."
  },
  {
    "objectID": "research.html#integration-and-challenges",
    "href": "research.html#integration-and-challenges",
    "title": "🛰 Research",
    "section": "",
    "text": "Integrating causal inference with machine learning techniques presents exciting opportunities for advancing environmental analysis. By combining the strengths of both fields, we can develop robust methodologies that provide accurate causal estimates while harnessing the predictive power of machine learning algorithms. However, several challenges need to be addressed to achieve this integration successfully. These challenges include the treatment of unmeasured confounding, scalability of methods for large environmental datasets, interpretability of complex machine learning models, and the consideration of uncertainty in causal inference."
  },
  {
    "objectID": "research.html#research-objectives",
    "href": "research.html#research-objectives",
    "title": "🛰 Research",
    "section": "",
    "text": "In my research, I aim to address these challenges and contribute to the nexus between causal inference, machine learning, and environmental analysis. Specifically, I intend to:\n\nDevelop novel methodologies that integrate causal inference and machine learning techniques to estimate causal effects and predict environmental outcomes accurately.\nInvestigate the use of deep learning architectures and explainable AI methods to enhance the interpretability of complex machine learning models for environmental analysis.\nExplore scalable algorithms and parallel computing techniques to handle the computational demands of large environmental datasets.\nInvestigate methods for handling unmeasured confounding and uncertainties inherent in causal inference within the context of environmental analysis.\nApply the developed methodologies to real-world environmental challenges, such as climate change and extreme event analysis. By pursuing these research objectives, I aspire to advance the understanding of causal relationships in environmental systems, contribute to evidence-based decision-making for environmental management, and ultimately support the development of sustainable practices for a better future."
  },
  {
    "objectID": "cv/cv.html",
    "href": "cv/cv.html",
    "title": "Ahmed T. Hammad",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "🧑‍🏫 Teaching",
    "section": "",
    "text": "As a teacher, I aim to perpetuate knowledge and inspire learning.\n\n\n\n\nAs a data scientist and academic researcher, I introduce students to an array of fundamental concepts of our understanding of reality through data and always ask them to articulate their reactions and understanding of the topics by providing me, and their classmates with alternative examples and practical applications of the concepts and techniques we learned together.\nMy teaching approach engages students’ natural creativity with a challenging mix of theory and practice. I teach theoretical concepts with a commitment to connecting these concepts with concrete real-life examples and by sharing the implications in business. For example, as the lead Data Science Instructor at Le Wagon Singapore, I taught several machine learning algorithms by explaining the implications of their use in different business and real-life contexts. I found that this approach helped students understand why, when and how to use a specific algorithm rather than another and how to articulate and justify their decision process.\n\n\n\n\n\nI seek a balance in my courses between lecturing to students and asking them to make discoveries. I encourage students to engage with the topic at hand, with me and with each other, in the belief that good teaching depends upon intellectual exchange. My end goal is to enable my students to envision and develop a “product” around the concepts and techniques we learned in class.\nMy approach to student assessment reflects my two goals. First, the students are expected to master a body of knowledge by demonstrating the practical use of different algorithms and methodologies at the end of the lecture. Second, students are given the opportunity to reflect upon the material at greater leisure and detail on project assignments.\n\n\n\n\n\nWhile my standards are high, I do not hesitate to help my students to meet expectations by providing office hours, review sessions and private time to discuss their concerns, fears and dreams. My approach has been to always make myself readily available to students and make it clear that if they need something then I am available at any time and point in their life to support them in their journey.\nI also have a strong passion for helping students who are dedicated but not necessarily performing well in class. In my experience, I have found some students performing far better in very applied tasks, shining in applied academic research or reaching success in business settings.\n\n\n\n\n\nMy teaching, research, and work experience have covered a wide variety of topics, including machine learning, causal machine learning, incremental machine learning, econometrics, causal inference, probabilistic modelling, time series modelling and risk analysis. Given the need, I am qualified to and would readily commit the effort required to effectively teach undergraduate and graduate classes in these subjects."
  },
  {
    "objectID": "teaching.html#teaching-approach",
    "href": "teaching.html#teaching-approach",
    "title": "🧑‍🏫 Teaching",
    "section": "",
    "text": "As a data scientist and academic researcher, I introduce students to an array of fundamental concepts of our understanding of reality through data and always ask them to articulate their reactions and understanding of the topics by providing me, and their classmates with alternative examples and practical applications of the concepts and techniques we learned together.\nMy teaching approach engages students’ natural creativity with a challenging mix of theory and practice. I teach theoretical concepts with a commitment to connecting these concepts with concrete real-life examples and by sharing the implications in business. For example, as the lead Data Science Instructor at Le Wagon Singapore, I taught several machine learning algorithms by explaining the implications of their use in different business and real-life contexts. I found that this approach helped students understand why, when and how to use a specific algorithm rather than another and how to articulate and justify their decision process."
  },
  {
    "objectID": "teaching.html#lecturing-approach",
    "href": "teaching.html#lecturing-approach",
    "title": "🧑‍🏫 Teaching",
    "section": "",
    "text": "I seek a balance in my courses between lecturing to students and asking them to make discoveries. I encourage students to engage with the topic at hand, with me and with each other, in the belief that good teaching depends upon intellectual exchange. My end goal is to enable my students to envision and develop a “product” around the concepts and techniques we learned in class.\nMy approach to student assessment reflects my two goals. First, the students are expected to master a body of knowledge by demonstrating the practical use of different algorithms and methodologies at the end of the lecture. Second, students are given the opportunity to reflect upon the material at greater leisure and detail on project assignments."
  },
  {
    "objectID": "teaching.html#advising-approach",
    "href": "teaching.html#advising-approach",
    "title": "🧑‍🏫 Teaching",
    "section": "",
    "text": "While my standards are high, I do not hesitate to help my students to meet expectations by providing office hours, review sessions and private time to discuss their concerns, fears and dreams. My approach has been to always make myself readily available to students and make it clear that if they need something then I am available at any time and point in their life to support them in their journey.\nI also have a strong passion for helping students who are dedicated but not necessarily performing well in class. In my experience, I have found some students performing far better in very applied tasks, shining in applied academic research or reaching success in business settings."
  },
  {
    "objectID": "teaching.html#teaching-interests",
    "href": "teaching.html#teaching-interests",
    "title": "🧑‍🏫 Teaching",
    "section": "",
    "text": "My teaching, research, and work experience have covered a wide variety of topics, including machine learning, causal machine learning, incremental machine learning, econometrics, causal inference, probabilistic modelling, time series modelling and risk analysis. Given the need, I am qualified to and would readily commit the effort required to effectively teach undergraduate and graduate classes in these subjects."
  },
  {
    "objectID": "students.html",
    "href": "students.html",
    "title": "🧑‍🎓 Students",
    "section": "",
    "text": "What my students say about me\n\n\nI was very fortunate to get Ahmed as my instructor during my data science boot camp. Overall, Ahmed was very passionate in teaching, his lessons were entertaining and enjoyable, and he was also able to explain complicated concepts in a clean and simple manner. What I liked most was that he often imparted little tips and advice on how to do well in data science, and also shared his personal experiences and challenges he faced in his own projects and how he managed to resolve them. I couldn’t have asked for a better instructor to kickstart my data science journey. Marcus Tan\n\n\n\nAhmed was my teacher during the 9 weeks data science bootcamp. He is an exceptional and passionate teacher. He teaches with great enthusiasm and patience. Additionally, he goes beyond to provide excellent support and guidance. Speaking for all my classmates, we were truly grateful to have him as a teacher. George Lee\n\n\n\nI had the pleasure of being taught by Ahmed for two months. It’s worth noting that, thanks to Ahmed’s passion for his subject, he makes it easy for his students to grasp complex mathematical and statistical concepts through relatable analogies and clear explanations. Not only is he one of the best teachers I’ve had in recent years, but he also possesses a kind heart and knows how to balance professional and personal relationships. I wish every teacher could be as passionate about their subject as Ahmed and that many could take inspiration from him. He teaches with immense enthusiasm and patience and consistently goes above and beyond to offer exceptional support and guidance. An additional bonus, is that you get to learn some Italian as well when being thought by him ;) Samuel Saleh\n\n\n\nThis guy restored my hope in education! I had the pleasure to be Ahmed’s student throughout the summer of 2023, and I can highly recommend his teaching. Ahmed knows how to adapt perfectly to his audience; he is a teacher who listens to his students and has a true talent for teaching. Even the most complex topics seem simple with him. Moreover, Ahmed, through his enthusiasm and natural authority, knows how to effectively convey his passion and high standards to his students. Beyond his qualities as a teacher, Ahmed is also a very warm person who can strike a pleasant balance between seriousness and lightness in his classes. In fact, after 2 months under his guidance, I found my path in data science and found in my teacher a friend. My loss function never has been that low ;) Taddeo Carpinelli\n\n\n\nAhmed is an outstanding teacher with a remarkable talent for simplifying complex concepts through clear explanations and abundant examples. His passion for the subject creates an engaging learning environment, and he approaches every question with unwavering dedication. Ahmed’s extensive knowledge in data science ensures he can provide thorough and confident responses, instilling a sense of confidence in his students. It has been a privilege to learn from him, and I have no doubt that he will continue to inspire and educate others with his exceptional teaching abilities. Morris Tang\n\n\n\nI highly recommend Ahmed for the position of university lecturer in data science. I had the pleasure of having him as my teacher in the Le Wagon data science bootcamp. He is a talented data scientist with a passion for teaching. He is also a highly motivated and hardworking individual. I am confident that he would be a positive addition to any company that is willing to hired.\nHere are some of Ahmed’s strengths as a data science educator:\nDeep knowledge of the subject matter\nAbility to communicate complex concepts in a clear and engaging way\nPatient and helpful in answering student questions\nPassionate about data science and eager to share his knowledge with others. Zheng YongShun\n\n\n\nAhmed, thank you for being an amazing teacher. Your passion for the subject and the way you create a welcoming classroom environment have made learning so much more enjoyable. You have a talent for making complicated things easy to understand, and I appreciate how you always go the extra mile to help us succeed. I feel lucky to be your student. Yong Sin Tan\n\n\n\nDr. Ahmed is someone who can break down complicated mathematical concepts into easily understandable components. Not only that, he demonstrates genuine care for his students’ progress.\nEven until now, long after our classes with him, he is still always available when we need help or clarification in data science/career progression topics. I’m very fortunate to have the opportunity to be taught by this outstanding and passionate teacher. Yong Chew\n\n\n\nI had the privilege of attending Ahmed’s lectures for Le Wagon’s full time data science program that spanned nine weeks.\n\nAhmed is an exceptional educator with a profound expertise in various domains of data science. His proficiency in statistical modeling, data analytics, SQL and Python programming, project management, machine learning, and deep learning is truly impressive. Throughout the bootcamp, Ahmed demonstrated a deep understanding of these subjects, and his ability to effectively convey complex concepts made the learning experience incredibly rewarding.\n\nOne aspect that truly stood out during the bootcamp was the final project on machine learning. We took on the task to predict possible side effects when two drugs are taken together, otherwise known as drug-drug interaction. Ahmed’s knowledge in statistics played a crucial role in guiding us through the project. His guidance and facilitation during the sessions ensured that we had a comprehensive understanding of each phase of the project. Under his supervision, we were able to complete the project from scratch within a remarkably short timeframe of just 2-3 weeks.\n\nHowever, what truly sets Ahmed apart is his dedication to ensuring that every student grasps the topics taught during the bootcamp. He goes above and beyond to make sure that even those without a computer science or statistics background can understand the material. Ahmed’s ability to blend well with us students and convey the teachings in a relatable manner is exceptional. He takes pride in his work and is a responsible lecturer who is always available to provide guidance and support.\n\nAhmed’s teaching style is highly engaging and interactive. He fosters an inclusive learning environment, encouraging students to ask questions, collaborate, and explore practical applications of the concepts learned. His dedication to our success was evident in the time and effort he invested in each student, providing valuable feedback and guidance to help us reach our full potential.\n\nI highly recommend Ahmed as a data science lecturer and researcher. His exceptional knowledge, dedication, and teaching abilities make him an invaluable asset to any learning environment. It was a privilege to learn from him, and I am confident that he will continue to inspire and shape the careers of aspiring data scientists. Xin Er Chong\n\n\n\nOverall very strong communication skills. Classes were taught in English to students with varying knowledge base and language levels. Lectures are packaged as so complex theories are presented in the most effective/digestible forms for learning. The teaching methods are highly effective. Making compression “simple”.\nAhmed was able to help me transfer the knowledge/skills from the classroom to the professional field (finance). He quickly understood my work and explained the ML methods available for implementation for a systematic approach for strategies. Jo Lam Ye\n\n\n\nAhmed is a passionate teacher whose patience and care for his students is palpable. I was privileged to be his student at Le Wagon in Bali where he masterfully guided me through difficult data science concepts. Ahmed is a rare leader who is honest, kind, and selfless, yet revered for his knowledge. I have a great deal of respect for him and am grateful for the time spent under his tutelage. Thank you, Ahmed! Stevie Gordon\n\n\n\nAhmed is a passionate teacher and educator. I particularly enjoyed his blend of academic and practical experience. While he’s very knowledgable of the state-of-the-art technology and algorithms, he also has a flair to simplify complex concepts to make them practical. If I were given the opportunity, I’d love to continue my studies with Ahmed in the future to further deepen the knowledge and skills learned from the 9-week bootcamp. Harris Soetikno\n\n\n\nOne of the most experienced instructors. It was very clear how well-prepared he was. I liked how he added his own signature to the training and one day decided to have an open session of questions and answers about real-life doubts we had about data scientists, actual projects, actual challenges and so on. It was very refreshing to hear from an experienced professional about the real challenges we might face. Anonymous Student\n\n\n\nOne of the best instructors, passionate. It was easy to follow his lectures, he was way straight to the point, totally appreciated his analogies, very helpful. Anonymous Student\n\n\n\nLike all other instructors he is a passionate of what he does. Kind of instructor that we would like to have longer. Anonymous Student\n\n\n\nbest technical teacher in the training, very good attitude and knowledge, he has the skills to transmit the knowledge in an easy way. Anonymous Student"
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "✍ Papers",
    "section": "",
    "text": "Publications\n\n\nPrioritizing COVID-19 vaccine allocation in resource poor settings: Towards an Artificial Intelligence-enabled and Geospatial-assisted decision support framework\nShayegh S.,[…], Hammad, A.T., et al., PLOS ONE\nProbabilistic forecasting of remotely sensed cropland vegetation health and its relevance for food security\nAT Hammad, G Falchetta .Science of the Total Environment 838, 156157\nBack to the fields? Increased agricultural land greenness after a COVID-19 lockdown\nAT Hammad, G Falchetta, IBM Wirawan. Environmental Research Communications 3 (5), 051007\nComparing paratransit in seven major African cities: an accessibility and network analysis\nG Falchetta, M Noussan, AT Hammad. Journal of Transport Geography 94, 103131\nPlanning universal accessibility to public health care in sub-Saharan Africa\nG Falchetta, AT Hammad, S Shayegh. Proceedings of the National Academy of Sciences 117 (50), 31760-31769\n\n\n\n\nCurrent work\n\n\nTracking global urban green space trends\nG Falchetta, AT Hammad. EGU23\n\n\n\n\nForthcoming\n\n\nA note on treatment effects: we are missing\nsomething on the tails\nA probabilistic box to explore climate data"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ahmed T. Hammad",
    "section": "",
    "text": "Hello, my name is Ahmed, and I am a Data Scientist, Quantitative Researcher, and Lecturer. I have a PhD in Causal Machine Learning and over 10 years of experience working with data in both academic and private sectors. I have extensive knowledge of programming languages such as R and Python, which I use daily to conduct my research and analysis.\nAs a passionate learner, I always strive to stay up-to-date with the latest developments in Machine Learning. That’s why I’m still involved in research and regularly publish my analysis on a variety of topics, including Machine Learning and Econometrics applied to environmental analysis and other fields.\nIn the last few years, I have been focusing mainly on Environmental Data Modeling, Climate Change and Extreme Event Analysis with a strong emphasis on probabilistic models to assist both private and public sector organizations.\nOne of the most fulfilling aspects of my career is sharing my knowledge of statistics and machine learning with others. Teaching the younger generation and professionals how to “get things done” and handle real-world data problems is something that brings me so much joy. It’s an opportunity for me to share my years of experience in the field and help others succeed.\n\n\n\n\n\n\n\n\nMachine Learning\nEconometrics\nExtreme Events Analysis\nCausal Inference\n\n\n\nTime Series\nProbabilistic Models\nEnvironmental Analysis\nSensor & Satellite Data\n\n\n\n\n\n\n\n                         \n\n\n\n\n\nFor curious visitors, the name of the website is just a tentative to mix the initial letter of my full name and statistics. By chance, I ended up with something that has an actual meaning in Arabic 🤷. Athsas (أتحسس) means “to feel something” or a sensation given by an object or material when touched."
  },
  {
    "objectID": "index.html#my-fields-of-interest",
    "href": "index.html#my-fields-of-interest",
    "title": "Ahmed T. Hammad",
    "section": "",
    "text": "Machine Learning\nEconometrics\nIncremental Machine Learning\nCausal Inference\n\n\n\nTime Series\nProbabilistic Machine Learning\nEnvironmental Analysis\nSensor & Satellite Data"
  },
  {
    "objectID": "blog/Logistic Regression and Marginal Effects.html",
    "href": "blog/Logistic Regression and Marginal Effects.html",
    "title": "Logistic Regression and Marginal Effects",
    "section": "",
    "text": "Logistic regression is a widely used statistical technique for modeling the relationship between a binary outcome and one or more predictor variables. It is commonly employed in various fields, such as healthcare, finance, and social sciences, to predict the probability of an event occurring. While the coefficients in logistic regression provide valuable insights, they are difficult to interpret. In this blog post, we will explore the use of marginal effects in logistic regression, unraveling their significance and practical applications.\n\n\nIn logistic regression, we model the log-odds of the event of interest occurring as a linear combination of predictor variables. The logistic function (sigmoid function) is then applied to this linear combination to convert it into a probability value between 0 and 1.\nThe logistic regression equation is given by:\n\\[\\text{logit}(P(Y=1)) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p\\]\nWhere:\n\n\\(P(Y=1)\\) is the probability of the event of interest (e.g., success) occurring.\n\\(logit (P(Y=1))\\) is the log-odds (logit) of the event occurring.\n\\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_p\\) are the coefficients of the model.\n\\(X_1, X_2, ..., X_p\\) are the predictor variables.\n\n\n\nThe coefficients \\(\\beta_1, \\beta_2, ..., \\beta_p\\) in the logistic regression equation hold crucial information about the impact of each predictor variable on the log-odds of the event occurring. To interpret these coefficients, we need to consider their signs, magnitude, and statistical significance.\n\nSign of Coefficients:\n\nThe sign of a coefficient reveals the direction of the relationship between the predictor variable and the log-odds of the event occurring. A positive coefficient (\\(\\beta &gt; 0\\)) indicates that an increase in the predictor variable leads to an increase in the log-odds (probability) of the event happening. Conversely, a negative coefficient (\\(\\beta &lt; 0\\)) suggests that an increase in the predictor variable is associated with a decrease in the log-odds (probability) of the event occurring.\n\nMagnitude of Coefficients:\n\nThe magnitude of a coefficient provides information about the strength of the relationship between the predictor variable and the log-odds of the event. Larger absolute values of coefficients indicate stronger influences on the probability.\n\nStatistical Significance:\n\nEvaluating the statistical significance of coefficients is essential to determine if the relationship between a predictor variable and the event probability is meaningful or just due to random chance. Researchers often use hypothesis testing or confidence intervals to assess the statistical significance of coefficients.\n\n\nLet’s consider an example to illustrate the interpretation of coefficients in logistic regression. Suppose we are studying the factors that influence whether a customer will make a purchase on an e-commerce website. Our logistic regression model includes two predictor variables: “Time Spent on Website” and “Number of Items Added to Cart.” The estimated coefficients are as follows:\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\)\n\\(\\beta_{\\text{Number of Items}} = 0.2\\)\n\n\n\n\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\): For each additional minute a customer spends on the website, the log-odds (probability) of making a purchase increases by 0.05 units. This means that spending more time on the website is associated with a higher likelihood of making a purchase.\n\\(\\beta_{\\text{Number of Items}} = 0.2\\): For each additional item a customer adds to the cart, the log-odds (probability) of making a purchase increases by 0.2 units. This suggests that adding more items to the cart is associated with a higher probability of making a purchase.\n\n\n\n\nIdeally, we want to understand what the model is telling us on the probability scale and not in the odds scale, much less in the estimation scale, the log-odds.\nWe might be tempted to convert log-odds to odds and then to probability and interpret everything as usual. But this would be very wrong!\nWhy?\nThe issue here is that on the probability scale the function is non-linear!\n\n\n\n\nMarginal effects can be described as the change in outcome as a function of the change in the independent variable of interest holding all other variables in the model constant.\n\nNOTE: In linear regression, the estimated regression coefficients are marginal effects and are more easily interpreted.*\n\nOne measure of change in a system is that the rate of change of the system is non-zero.\nIf we had a simple linear regression model for a trend, then the estimated rate of change would be the slope of the line. Technically, this is the instantaneous rate of change of the function that defines the line.\nThe idea can be extended to any function, even one as potentially complex non-linear function.\nThe problem we have is that in general we don’t have an equation for the non-linear from which we can derive the derivatives.\nSo how do we estimate the derivative of a non-linear function ?\nOne solution is to use the method of finite differences but to understand it properly we need to introduce first the basic concept of differential calculus.\n\n\n\n\nDifferential calculus is a branch of calculus that focuses on the study of rates of change and slopes of curves. It deals with the concept of derivatives, which represent the rate of change of a function at a specific point. Derivatives allow us to understand how a function is changing at a particular location and provide valuable insights into the behavior of functions.\nThe derivative of a function \\(f(x)\\) with respect to \\(x\\) is denoted as \\(f'(x)\\) or \\(\\frac{df(x)}{dx}\\). It represents the slope of the tangent line to the graph of the function at a given point \\(x\\). The derivative can be interpreted as the instantaneous rate of change of \\(f(x)\\) with respect to \\(x\\).\nFor example, consider the function \\(f(x) = x^2\\). The derivative of \\(f(x)\\) is \\(f'(x) = 2x\\). This means that the slope of the tangent line to the graph of \\(f(x)\\) at any point \\(x\\) is twice the value of \\(x\\) at that point.\nDifferential calculus has wide-ranging applications in various fields, including physics, engineering, economics, and more. It is used to model and analyze phenomena that involve continuous changes, such as motion, growth, and optimization.\nFinite differences are a numerical technique used to approximate derivatives of functions. The method involves computing the difference in function values at two nearby points and then dividing by the difference in their corresponding independent variable values. By using a small interval between the two points, we can get an approximation of the derivative at a specific point.\nThe finite difference formula for approximating the first derivative of a function \\(f(x)\\) at a point \\(x\\) is given by:\n\\[ f’(x) \\approx \\frac{f(x + h) - f(x)}{h} \\]\nWhere:\n\n\\(h\\) is a small value representing the step size or interval between the points.\n\nSimilarly, we can use finite differences to approximate higher-order derivatives, such as the second derivative:\n\\[ f’’(x) \\approx \\frac{f(x + h) - 2f(x) + f(x - h)}{h^2} \\]\nFinite differences are particularly useful when the analytical expression for the derivative is difficult to obtain or when dealing with discrete data points instead of continuous functions.\nDifferential calculus and finite differences are both essential tools in mathematics and scientific disciplines. Differential calculus provides a rigorous framework for studying rates of change and slopes of curves through derivatives. On the other hand, finite differences offer a practical and numerical approach to approximate derivatives when the analytical solution is not readily available or when dealing with discrete data. Together, these concepts enable us to better understand the behavior of functions and analyze real-world phenomena involving continuous and discrete changes.\n\n\n\nIn logistic regression, the estimated coefficients (log odds) indicate how the log odds of the outcome change with a one-unit increase in the predictor variable while holding other variables constant. However, interpreting these coefficients can be challenging, especially for non-statisticians. This is where marginal effects come to the rescue.\nMarginal effects provide a more straightforward interpretation by measuring the impact of a one-unit change in the predictor variable on the probability of the event occurring (i.e., the probability of success). They express the change in the probability as a result of the predictor variable’s change, allowing us to understand the practical implications of the model."
  },
  {
    "objectID": "blog/Unraveling the Power of Causal Machine Learning.html",
    "href": "blog/Unraveling the Power of Causal Machine Learning.html",
    "title": "Unraveling the Power of Causal Machine Learning",
    "section": "",
    "text": "Hey fellow data enthusiasts! Today, let’s embark on a journey to explore the captivating world of causal machine learning. 🚀\n🔍 Understanding Causality:\nIn the realm of traditional machine learning, we primarily focus on identifying patterns and correlations in data to make accurate predictions. While this approach is incredibly useful, it often fails to decipher the cause-and-effect relationships that underpin these patterns. This limitation can lead to misguided conclusions and ineffective decision-making.\nCausality seeks to answer “why” questions, revealing the cause behind an observed effect. Imagine a scenario where we want to understand if a certain medication truly improves patient outcomes or if it’s just correlated with better health. Causal inference allows us to make more meaningful inferences, beyond mere correlations.\n🧩 The Challenge of Causal Inference:\nEstablishing causality is not a straightforward task. The fundamental problem lies in the fact that we cannot simultaneously observe both the treatment group (those who receive a particular intervention) and the control group (those who do not). Traditional observational data often suffers from confounding variables, making it challenging to disentangle cause and effect.\n💡 Enter Causal Machine Learning:\nCausal Machine Learning, a fascinating interdisciplinary field, bridges the gap between traditional machine learning and causal inference. Its primary goal is to leverage data to identify causal relationships and infer the effects of interventions accurately.\n🎯 Causal Inference Methods:\n1. Randomized Control Trials (RCTs): Considered the gold standard for establishing causality, RCTs involve randomly assigning subjects to either the treatment or control group. By eliminating confounding factors, RCTs provide strong causal evidence. However, they may not always be practical or ethical.\n2. Propensity Score Matching (PSM): When RCTs are not feasible, PSM is a popular method. It attempts to create a “quasi-experimental” design by matching treated and control units based on their propensity scores, which represent the likelihood of receiving the treatment. This helps mitigate confounding effects.\n3. Instrumental Variables (IV): IV analysis relies on instrumental variables that are correlated with the treatment but have no direct effect on the outcome. By utilizing these instruments, researchers can uncover causal relationships even in the presence of unobserved confounders.\n4. Synthetic Control (SC): The process of creating a synthetic counterfactual involves using historical data from similar individuals or groups that did not receive the treatment to construct a “synthetic” control group. This control group is designed to closely match the characteristics of the treated group before the intervention. By doing so, researchers aim to create a plausible estimate of what would have happened to the treated group had they not received the treatment.\n🚀 Real-World Applications:\nCausal Machine Learning has found applications in various domains, including public health, economics, social sciences, and marketing. It enables policymakers to make informed decisions, businesses to optimize interventions, and researchers to uncover hidden causal mechanisms.\n🌟 Embracing Causal Machine Learning:\nAs the world becomes more data-driven, understanding causality becomes paramount. Embracing causal machine learning techniques empowers us to make smarter decisions, avoid biases, and design better interventions that have a genuine impact on our lives.\nAn excellent example application of Causal Machine Learning is in the field of healthcare for personalized treatment recommendations or to estimate the impact of marketing campaigns. Beyonf the classic RCT or A/B testing, in both cases the use of Reinforcement Learning is a very interesting application that we will discuss in the next blog post."
  },
  {
    "objectID": "blog/Quantile Random Forest.html",
    "href": "blog/Quantile Random Forest.html",
    "title": "Quantile Random Forest",
    "section": "",
    "text": "Quantile Random Forest (QRF) is an extension of the traditional Random Forest algorithm that estimates not only the mean but also the entire conditional distribution of the target variable. The distribution is represented by its quantiles, which are specific points that divide the data into segments, such as quartiles (0.25, 0.50, 0.75). QRF is particularly useful when dealing with heteroscedastic data or when we need to assess the uncertainty in our predictions beyond just the central tendency.\nIn QRF, an ensemble of decision trees is constructed. Each decision tree is built by bootstrapping the training data, which means sampling the data points with replacement, and selecting a random subset of features at each split. This creates diversity among the trees in the ensemble.\nThe key difference between Random Forest and QRF lies in the prediction phase. In Random Forest, the predicted value for a new data point is the average (mean) of the individual tree predictions. However, in QRF, we aim to estimate multiple quantiles of the target variable’s distribution for a given input.\nWhen growing a tree in QRF, instead of splitting the data based solely on minimizing the variance or mean squared error, QRF performs splits to optimize quantile-specific criteria. Each tree node is split to minimize the quantile loss function, which reflects the error in predicting the specific quantile. This means that the tree seeks to minimize the difference between the true quantile value and the estimated quantile value for the data points that fall into each node.\nOnce the tree is fully grown, the terminal nodes (leaf nodes) contain specific quantile values rather than just the average value. Each terminal node in the QRF stores a quantile-specific estimate based on the data points that fall into that node. This means that each tree provides quantile-specific predictions.\nTo make predictions for a new data point, QRF evaluates the input through each decision tree, resulting in a set of quantile-specific predictions. The user can then choose the desired quantiles (e.g., 0.25, 0.50, 0.75) to obtain the corresponding quantile predictions. For example, if we want to estimate the 0.75 quantile, we collect the 0.75 quantile values from each tree, and these values form the quantile prediction for that data point.\nOne of the advantages of QRF is that it provides a measure of uncertainty in the predictions. Given the estimated quantiles, QRF can construct a prediction interval for each data point. The prediction interval provides a range of possible values, accounting for the uncertainty in the prediction due to the variability of the underlying data.\nAdvantages of Quantile Random Forest:\n\nRobustness to outliers: QRF can handle outliers effectively as it estimates the entire distribution, including extreme quantiles.\nProvides uncertainty information: By estimating quantiles, QRF offers a measure of uncertainty in the predictions, which is valuable in decision-making.\nVersatility: QRF can be applied to regression and quantile regression problems, and it can be extended to handle multivariate quantile estimation.\n\nApplications of Quantile Random Forest:\n\nFinancial forecasting: QRF can be used to predict quantiles of financial variables, such as stock prices or asset returns, to assess the risk of investment decisions.\nMedical applications: In healthcare, QRF can estimate quantiles of patient response times to treatments, enabling doctors to tailor treatments accordingly.\nEnvironmental modeling: QRF can estimate quantiles of environmental variables like pollutant concentrations, assisting in environmental risk assessments.\n\nIn summary, Quantile Random Forest is a powerful extension of the Random Forest algorithm that estimates the entire distribution of the target variable through quantile-specific predictions. It is useful when capturing the entire distribution or quantifying uncertainty is essential for making informed decisions in various applications such as finance, healthcare, and environmental modeling."
  },
  {
    "objectID": "blog/Embracing Change: Incremental vs. Batch Machine Learning.html",
    "href": "blog/Embracing Change: Incremental vs. Batch Machine Learning.html",
    "title": "Embracing Change: Incremental vs. Batch Machine Learning",
    "section": "",
    "text": "Title: Embracing Change: Incremental vs. Batch Machine Learning\nIntroduction\nMachine learning has revolutionized the way we interact with data, allowing us to uncover valuable insights and make informed decisions. Within the realm of machine learning, there are two fundamental approaches: incremental learning and batch learning. These methods serve distinct purposes and cater to diverse scenarios, offering unique advantages and challenges. In this blog post, we will embark on an exciting journey to understand the differences between incremental and batch machine learning, providing you with the knowledge to embrace these powerful techniques.\nIncremental Machine Learning: An Ever-Evolving Journey\nImagine a learning process that continuously adapts to new information, always striving to stay up-to-date with the latest trends. That’s incremental machine learning for you! Also known as online learning or streaming learning, this approach updates the model in real-time as fresh data arrives. Unlike batch learning, which requires processing the entire dataset at once, incremental learning embraces change, incorporating new observations into the existing model as they arrive.\nCharacteristics of Incremental Machine Learning:\n1. Real-Time Adaptation: The beauty of incremental learning lies in its ability to quickly respond to shifts in data distribution. This makes it ideal for applications dealing with rapidly evolving data, like online advertising or fraud detection.\n2. Low Memory Requirements: Incremental learning processes data in small batches or individual data points, reducing memory usage compared to batch learning, where the entire dataset needs to be stored.\n3. Constant Learning: With incremental learning, models can continuously learn and improve over time without the need for retraining on the entire dataset. It’s like a lifelong learning journey!\nAdvantages of Incremental Machine Learning:\n1. Efficiency: Incremental learning is computationally efficient since it updates the model incrementally, without the need to retrain on the entire dataset each time.\n2. Scalability: This approach is well-suited for large-scale applications where processing the entire dataset at once would be overwhelming.\n3. Dynamic Adaptation: Incremental learning is ideal for applications where data distribution changes frequently, such as natural language processing or recommendation systems. It embraces the unexpected!\nDisadvantages of Incremental Machine Learning:\n1. Forgetting Old Data: Just like we can’t remember everything we’ve learned in the past, incremental learning models may forget information from the past, leading to potential performance degradation on older data points.\n2. Model Drift: In dynamic environments, incremental learning models may struggle to adapt to sudden shifts in data patterns, leading to inaccuracies. Sometimes, change can be overwhelming!\nBatch Machine Learning: A Journey with Stability\nNow, let’s switch gears and explore batch machine learning. Picture a learning process that takes time to analyze and understand the whole story before making decisions. That’s batch machine learning for you! This approach involves training the model on the entire dataset and updating it periodically or when significant amounts of new data accumulate.\nCharacteristics of Batch Machine Learning:\n1. Periodic Updates: Batch learning involves training models from scratch on a fixed dataset, leveraging powerful hardware resources and sophisticated algorithms. It’s like taking a step back to see the bigger picture!\n2. Complete Data Usage: Unlike incremental learning, which processes data in smaller chunks, batch learning uses the entire dataset for training, ensuring no data points are left behind.\nAdvantages of Batch Machine Learning:\n1. Strong Convergence: By training on the entire dataset, batch learning often achieves more stable and reliable results, especially with sufficient computational resources. It’s like taking the time to contemplate before making decisions.\n2. Accuracy: Batch learning models tend to be more accurate due to their exhaustive training process. Sometimes, thoroughness pays off!\nDisadvantages of Batch Machine Learning:\n1. High Resource Demands: Batch learning requires substantial computational power and memory, making it less scalable for real-time applications or large datasets.\n2. Time Lag in Updates: Models may not adapt quickly to changes in data distribution since they are updated only after accumulating new data. Patience is key!\nUse Cases for Incremental and Batch Machine Learning\nIncremental Learning:\n- Real-time fraud detection in financial transactions.\n- Adaptive recommendation systems that learn from user interactions in real-time.\n- Sentiment analysis on streaming social media data.\nBatch Learning:\n- Image classification in computer vision tasks.\n- Natural language processing applications like language translation.\n- Training large-scale recommendation systems periodically.\nConclusion\nAs we bid farewell to this exploration, we have gained an understanding of the intriguing differences between incremental and batch machine learning. Incremental learning embraces change and adapts to evolving data, making it perfect for real-time applications. On the other hand, batch learning takes a more measured approach, achieving strong convergence and accuracy with thorough analysis. Both methods have their unique strengths, empowering data scientists and developers to make informed choices based on the demands of their applications. So, whether you’re venturing on a dynamic journey with incremental learning or taking a stable path with batch learning, remember to embrace the wonders of machine learning and let it lead you to exciting discoveries!"
  },
  {
    "objectID": "blog/Logistic Regression and Marginal Effects.html#interpretation-issues",
    "href": "blog/Logistic Regression and Marginal Effects.html#interpretation-issues",
    "title": "Logistic Regression and Marginal Effects",
    "section": "Interpretation Issues",
    "text": "Interpretation Issues\nIdeally, we want to understand what the model is telling us on the probability scale and not in the odds scale, much less in the estimation scale, the log-odds.\nWe might be tempted to convert log-odds to odds and then to probability and interpret everything as usual. But this would be very wrong!\nWhy?\nThe issue here is that on the probability scale the function is non-linear!"
  },
  {
    "objectID": "blog/Logistic Regression and Marginal Effects.html#logistic-regression",
    "href": "blog/Logistic Regression and Marginal Effects.html#logistic-regression",
    "title": "Logistic Regression and Marginal Effects",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nIn logistic regression, we model the log-odds of the event of interest occurring as a linear combination of predictor variables. The logistic function (sigmoid function) is then applied to this linear combination to convert it into a probability value between 0 and 1.\nThe logistic regression equation is given by:\n\\[\\text{logit}(P(Y=1)) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p\\]\nWhere:\n\n\\(P(Y=1)\\) is the probability of the event of interest (e.g., success) occurring.\n\\(logit (P(Y=1))\\) is the log-odds (logit) of the event occurring.\n\\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_p\\) are the coefficients of the model.\n\\(X_1, X_2, ..., X_p\\) are the predictor variables.\n\nInterpreting Coefficients in Logistic Regression\nThe coefficients \\(\\beta_1, \\beta_2, ..., \\beta_p\\) in the logistic regression equation hold crucial information about the impact of each predictor variable on the log-odds of the event occurring. To interpret these coefficients, we need to consider their signs, magnitude, and statistical significance.\n\nSign of Coefficients:\n\nThe sign of a coefficient reveals the direction of the relationship between the predictor variable and the log-odds of the event occurring. A positive coefficient (\\(\\beta &gt; 0\\)) indicates that an increase in the predictor variable leads to an increase in the log-odds (probability) of the event happening. Conversely, a negative coefficient (\\(\\beta &lt; 0\\)) suggests that an increase in the predictor variable is associated with a decrease in the log-odds (probability) of the event occurring.\n\nMagnitude of Coefficients:\n\nThe magnitude of a coefficient provides information about the strength of the relationship between the predictor variable and the log-odds of the event. Larger absolute values of coefficients indicate stronger influences on the probability.\n\nStatistical Significance:\n\nEvaluating the statistical significance of coefficients is essential to determine if the relationship between a predictor variable and the event probability is meaningful or just due to random chance. Researchers often use hypothesis testing or confidence intervals to assess the statistical significance of coefficients.\nPractical Interpretation Example\nLet’s consider an example to illustrate the interpretation of coefficients in logistic regression. Suppose we are studying the factors that influence whether a customer will make a purchase on an e-commerce website. Our logistic regression model includes two predictor variables: “Time Spent on Website” and “Number of Items Added to Cart.” The estimated coefficients are as follows:\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\)\n\\(\\beta_{\\text{Number of Items}} = 0.2\\)"
  },
  {
    "objectID": "blog/Logistic Regression and Marginal Effects.html#interpreting-coefficients-in-logistic-regression",
    "href": "blog/Logistic Regression and Marginal Effects.html#interpreting-coefficients-in-logistic-regression",
    "title": "Logistic Regression and Marginal Effects",
    "section": "",
    "text": "The coefficients \\(\\beta_1, \\beta_2, ..., \\beta_p\\) in the logistic regression equation hold crucial information about the impact of each predictor variable on the log-odds of the event occurring. To interpret these coefficients, we need to consider their signs, magnitude, and statistical significance.\n\nSign of Coefficients:\n\nThe sign of a coefficient reveals the direction of the relationship between the predictor variable and the log-odds of the event occurring. A positive coefficient (\\(\\beta &gt; 0\\)) indicates that an increase in the predictor variable leads to an increase in the log-odds (probability) of the event happening. Conversely, a negative coefficient (\\(\\beta &lt; 0\\)) suggests that an increase in the predictor variable is associated with a decrease in the log-odds (probability) of the event occurring.\n\nMagnitude of Coefficients:\n\nThe magnitude of a coefficient provides information about the strength of the relationship between the predictor variable and the log-odds of the event. Larger absolute values of coefficients indicate stronger influences on the probability.\n\nStatistical Significance:\n\nEvaluating the statistical significance of coefficients is essential to determine if the relationship between a predictor variable and the event probability is meaningful or just due to random chance. Researchers often use hypothesis testing or confidence intervals to assess the statistical significance of coefficients.\n\n\nLet’s consider an example to illustrate the interpretation of coefficients in logistic regression. Suppose we are studying the factors that influence whether a customer will make a purchase on an e-commerce website. Our logistic regression model includes two predictor variables: “Time Spent on Website” and “Number of Items Added to Cart.” The estimated coefficients are as follows:\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\)\n\\(\\beta_{\\text{Number of Items}} = 0.2\\)\n\n\n\n\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\): For each additional minute a customer spends on the website, the log-odds (probability) of making a purchase increases by 0.05 units. This means that spending more time on the website is associated with a higher likelihood of making a purchase.\n\\(\\beta_{\\text{Number of Items}} = 0.2\\): For each additional item a customer adds to the cart, the log-odds (probability) of making a purchase increases by 0.2 units. This suggests that adding more items to the cart is associated with a higher probability of making a purchase.\n\n\n\n\nIdeally, we want to understand what the model is telling us on the probability scale and not in the odds scale, much less in the estimation scale, the log-odds.\nWe might be tempted to convert log-odds to odds and then to probability and interpret everything as usual. But this would be very wrong!\nWhy?\nThe issue here is that on the probability scale the function is non-linear!"
  },
  {
    "objectID": "blog/Logistic Regression and Marginal Effects.html#interpretation",
    "href": "blog/Logistic Regression and Marginal Effects.html#interpretation",
    "title": "Logistic Regression and Marginal Effects",
    "section": "Interpretation:",
    "text": "Interpretation:\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\): For each additional minute a customer spends on the website, the log-odds (probability) of making a purchase increases by 0.05 units. This means that spending more time on the website is associated with a higher likelihood of making a purchase.\n\\(\\beta_{\\text{Number of Items}} = 0.2\\): For each additional item a customer adds to the cart, the log-odds (probability) of making a purchase increases by 0.2 units. This suggests that adding more items to the cart is associated with a higher probability of making a purchase.\n\n\nInterpretation Issues\nIdeally, we want to understand what the model is telling us on the probability scale and not in the odds scale, much less in the estimation scale, the log-odds.\nWe might be tempted to convert log-odds to odds and then to probability and interpret everything as usual. But this would be very wrong!\nWhy?\nThe issue here is that on the probability scale the function is non-linear!"
  },
  {
    "objectID": "blog/Logistic Regression and Marginal Effects.html#marginal-effects",
    "href": "blog/Logistic Regression and Marginal Effects.html#marginal-effects",
    "title": "Logistic Regression and Marginal Effects",
    "section": "",
    "text": "Marginal effects can be described as the change in outcome as a function of the change in the independent variable of interest holding all other variables in the model constant.\n\nNOTE: In linear regression, the estimated regression coefficients are marginal effects and are more easily interpreted.*\n\nOne measure of change in a system is that the rate of change of the system is non-zero.\nIf we had a simple linear regression model for a trend, then the estimated rate of change would be the slope of the line. Technically, this is the instantaneous rate of change of the function that defines the line.\nThe idea can be extended to any function, even one as potentially complex non-linear function.\nThe problem we have is that in general we don’t have an equation for the non-linear from which we can derive the derivatives.\nSo how do we estimate the derivative of a non-linear function ?\nOne solution is to use the method of finite differences but to understand it properly we need to introduce first the basic concept of differential calculus."
  },
  {
    "objectID": "posts/Unraveling the Power of Causal Machine Learning.html",
    "href": "posts/Unraveling the Power of Causal Machine Learning.html",
    "title": "Unraveling the Power of Causal Machine Learning",
    "section": "",
    "text": "In the realm of traditional machine learning, we primarily focus on identifying patterns and correlations in data to make accurate predictions. While this approach is incredibly useful, it often fails to decipher the cause-and-effect relationships that underpin these patterns. This limitation can lead to misguided conclusions and ineffective decision-making.\nCausality seeks to answer “why” questions, revealing the cause behind an observed effect. Imagine a scenario where we want to understand if a certain medication truly improves patient outcomes or if it’s just correlated with better health. Causal inference allows us to make more meaningful inferences, beyond mere correlations.\n\n\nEstablishing causality is not a straightforward task. The fundamental problem lies in the fact that we cannot simultaneously observe both the treatment group (those who receive a particular intervention) and the control group (those who do not). Traditional observational data often suffers from confounding variables, making it challenging to disentangle cause and effect.\n\n\n\nCausal Machine Learning, a fascinating interdisciplinary field, bridges the gap between traditional machine learning and causal inference. Its primary goal is to leverage data to identify causal relationships and infer the effects of interventions accurately.\n\n\n\n\nRandomized Control Trials (RCTs): Considered the gold standard for establishing causality, RCTs involve randomly assigning subjects to either the treatment or control group. By eliminating confounding factors, RCTs provide strong causal evidence. However, they may not always be practical or ethical.\nPropensity Score Matching (PSM): When RCTs are not feasible, PSM is a popular method. It attempts to create a “quasi-experimental” design by matching treated and control units based on their propensity scores, which represent the likelihood of receiving the treatment. This helps mitigate confounding effects.\nInstrumental Variables (IV): IV analysis relies on instrumental variables that are correlated with the treatment but have no direct effect on the outcome. By utilizing these instruments, researchers can uncover causal relationships even in the presence of unobserved confounders.\nSynthetic Control (SC): The process of creating a synthetic counterfactual involves using historical data from similar individuals or groups that did not receive the treatment to construct a “synthetic” control group. This control group is designed to closely match the characteristics of the treated group before the intervention. By doing so, researchers aim to create a plausible estimate of what would have happened to the treated group had they not received the treatment.\n\n\n\n\nCausal Machine Learning has found applications in various domains, including public health, economics, social sciences, and marketing. It enables policymakers to make informed decisions, businesses to optimize interventions, and researchers to uncover hidden causal mechanisms.\n\n\n\nAs the world becomes more data-driven, understanding causality becomes paramount. Embracing causal machine learning techniques empowers us to make smarter decisions, avoid biases, and design better interventions that have a genuine impact on our lives.\nAn excellent example application of Causal Machine Learning is in the field of healthcare for personalized treatment recommendations or to estimate the impact of marketing campaigns. Beyond the classic RCT or A/B testing, in both cases the use of Reinforcement Learning is a very interesting application that we will discuss in the next blog post."
  },
  {
    "objectID": "posts/Quantile Random Forest.html",
    "href": "posts/Quantile Random Forest.html",
    "title": "Quantile Random Forest",
    "section": "",
    "text": "Quantile Random Forest (QRF) is an extension of the traditional Random Forest algorithm that estimates not only the mean but also the entire conditional distribution of the target variable. The distribution is represented by its quantiles, which are specific points that divide the data into segments, such as quartiles (0.25, 0.50, 0.75). QRF is particularly useful when dealing with heteroscedastic data or when we need to assess the uncertainty in our predictions beyond just the central tendency.\nIn QRF, an ensemble of decision trees is constructed. Each decision tree is built by bootstrapping the training data, which means sampling the data points with replacement, and selecting a random subset of features at each split. This creates diversity among the trees in the ensemble."
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html",
    "title": "Embracing Change: Incremental vs. Batch Machine Learning",
    "section": "",
    "text": "Machine learning has revolutionized the way we interact with data, allowing us to uncover valuable insights and make informed decisions. Within the realm of machine learning, there are two fundamental approaches: incremental learning and batch learning. These methods serve distinct purposes and cater to diverse scenarios, offering unique advantages and challenges. In this blog post, we will embark on an exciting journey to understand the differences between incremental and batch machine learning, providing you with the knowledge to embrace these powerful techniques.\n\n\nImagine a learning process that continuously adapts to new information, always striving to stay up-to-date with the latest trends. That’s incremental machine learning for you! Also known as online learning or streaming learning, this approach updates the model in real-time as fresh data arrives. Unlike batch learning, which requires processing the entire dataset at once, incremental learning embraces change, incorporating new observations into the existing model as they arrive.\n\n\n\nReal-Time Adaptation: The beauty of incremental learning lies in its ability to quickly respond to shifts in data distribution. This makes it ideal for applications dealing with rapidly evolving data, like online advertising or fraud detection.\nLow Memory Requirements: Incremental learning processes data in small batches or individual data points, reducing memory usage compared to batch learning, where the entire dataset needs to be stored.\nConstant Learning: With incremental learning, models can continuously learn and improve over time without the need for retraining on the entire dataset. It’s like a lifelong learning journey!\n\nAdvantages of Incremental Machine Learning\n\nEfficiency: Incremental learning is computationally efficient since it updates the model incrementally, without the need to retrain on the entire dataset each time.\nScalability: This approach is well-suited for large-scale applications where processing the entire dataset at once would be overwhelming.\nDynamic Adaptation: Incremental learning is ideal for applications where data distribution changes frequently, such as natural language processing or recommendation systems. It embraces the unexpected!\n\nDisadvantages of Incremental Machine Learning\n\nForgetting Old Data: Just like we can’t remember everything we’ve learned in the past, incremental learning models may forget information from the past, leading to potential performance degradation on older data points.\nModel Drift: In dynamic environments, incremental learning models may struggle to adapt to sudden shifts in data patterns, leading to inaccuracies. Sometimes, change can be overwhelming!\n\n\n\n\n\nNow, let’s switch gears and explore batch machine learning. Picture a learning process that takes time to analyze and understand the whole story before making decisions. That’s batch machine learning for you! This approach involves training the model on the entire dataset and updating it periodically or when significant amounts of new data accumulate.\n\n\n\nPeriodic Updates: Batch learning involves training models from scratch on a fixed dataset, leveraging powerful hardware resources and sophisticated algorithms. It’s like taking a step back to see the bigger picture!\nComplete Data Usage: Unlike incremental learning, which processes data in smaller chunks, batch learning uses the entire dataset for training, ensuring no data points are left behind.\n\nAdvantages of Batch Machine Learning\n\nStrong Convergence: By training on the entire dataset, batch learning often achieves more stable and reliable results, especially with sufficient computational resources. It’s like taking the time to contemplate before making decisions.\nAccuracy: Batch learning models tend to be more accurate due to their exhaustive training process. Sometimes, thoroughness pays off!\n\nDisadvantages of Batch Machine Learning\n\nHigh Resource Demands: Batch learning requires substantial computational power and memory, making it less scalable for real-time applications or large datasets.\nTime Lag in Updates: Models may not adapt quickly to changes in data distribution since they are updated only after accumulating new data. Patience is key!\n\n\n\n\n\nAs we bid farewell to this exploration, we have gained an understanding of the intriguing differences between incremental and batch machine learning. Incremental learning embraces change and adapts to evolving data, making it perfect for real-time applications. On the other hand, batch learning takes a more measured approach, achieving strong convergence and accuracy with thorough analysis. Both methods have their unique strengths, empowering data scientists and developers to make informed choices based on the demands of their applications. So, whether you’re venturing on a dynamic journey with incremental learning or taking a stable path with batch learning, remember to embrace the wonders of machine learning and let it lead you to exciting discoveries!\n\n\n\nIf you’re interested in delving deeper into the subject, I’ve created a comprehensive one-hour video introduction on the topic."
  },
  {
    "objectID": "posts/Logistic Regression and Marginal Effects.html",
    "href": "posts/Logistic Regression and Marginal Effects.html",
    "title": "Logistic Regression and Marginal Effects",
    "section": "",
    "text": "Logistic regression is a widely used statistical technique for modeling the relationship between a binary outcome and one or more predictor variables. It is commonly employed in various fields, such as healthcare, finance, and social sciences, to predict the probability of an event occurring. While the coefficients in logistic regression provide valuable insights, they are difficult to interpret. In this blog post, we will explore the use of marginal effects in logistic regression, unraveling their significance and practical applications."
  },
  {
    "objectID": "posts/Logistic Regression and Marginal Effects.html#interpreting-coefficients-in-logistic-regression",
    "href": "posts/Logistic Regression and Marginal Effects.html#interpreting-coefficients-in-logistic-regression",
    "title": "Logistic Regression and Marginal Effects",
    "section": "Interpreting Coefficients in Logistic Regression",
    "text": "Interpreting Coefficients in Logistic Regression\nThe coefficients \\(\\beta_1, \\beta_2, ..., \\beta_p\\) in the logistic regression equation hold crucial information about the impact of each predictor variable on the log-odds of the event occurring. To interpret these coefficients, we need to consider their signs, magnitude, and statistical significance.\n\nSign of Coefficients:\n\nThe sign of a coefficient reveals the direction of the relationship between the predictor variable and the log-odds of the event occurring. A positive coefficient (\\(\\beta &gt; 0\\)) indicates that an increase in the predictor variable leads to an increase in the log-odds (probability) of the event happening. Conversely, a negative coefficient (\\(\\beta &lt; 0\\)) suggests that an increase in the predictor variable is associated with a decrease in the log-odds (probability) of the event occurring.\n\nMagnitude of Coefficients:\n\nThe magnitude of a coefficient provides information about the strength of the relationship between the predictor variable and the log-odds of the event. Larger absolute values of coefficients indicate stronger influences on the probability.\n\nStatistical Significance:\n\nEvaluating the statistical significance of coefficients is essential to determine if the relationship between a predictor variable and the event probability is meaningful or just due to random chance. Researchers often use hypothesis testing or confidence intervals to assess the statistical significance of coefficients."
  },
  {
    "objectID": "posts/Logistic Regression and Marginal Effects.html#marginal-effects",
    "href": "posts/Logistic Regression and Marginal Effects.html#marginal-effects",
    "title": "Logistic Regression and Marginal Effects",
    "section": "",
    "text": "Marginal effects can be described as the change in outcome as a function of the change in the independent variable of interest holding all other variables in the model constant.\n\nNOTE: In linear regression, the estimated regression coefficients are marginal effects and are more easily interpreted.*\n\nOne measure of change in a system is that the rate of change of the system is non-zero.\nIf we had a simple linear regression model for a trend, then the estimated rate of change would be the slope of the line. Technically, this is the instantaneous rate of change of the function that defines the line.\nThe idea can be extended to any function, even one as potentially complex non-linear function.\nThe problem we have is that in general we don’t have an equation for the non-linear from which we can derive the derivatives.\nSo how do we estimate the derivative of a non-linear function ?\nOne solution is to use the method of finite differences but to understand it properly we need to introduce first the basic concept of differential calculus."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "🎙️Blog",
    "section": "",
    "text": "Unraveling the Power of Causal Machine Learning\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nQuantile Random Forest\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nLogistic Regression and Marginal Effects\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nContextual Multi-Armed Bandit: Maximizing Rewards with Intelligent Decision-Making\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWhen in doubt, just model it. Modelling uncertainty\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nDeveloping in a Docker container\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nData Science Books\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nData Science project Boilerplate\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBayesian updating\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nThe 3 + 1 pillars of data science\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nProbability Box with Kernel Density Estimation\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMachine Learning, Copula and Synthetic Data\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nEmbracing Change: Incremental vs. Batch Machine Learning\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#incremental-machine-learning-an-ever-evolving-journey",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#incremental-machine-learning-an-ever-evolving-journey",
    "title": "Embracing Change: Incremental vs. Batch Machine Learning",
    "section": "",
    "text": "Imagine a learning process that continuously adapts to new information, always striving to stay up-to-date with the latest trends. That’s incremental machine learning for you! Also known as online learning or streaming learning, this approach updates the model in real-time as fresh data arrives. Unlike batch learning, which requires processing the entire dataset at once, incremental learning embraces change, incorporating new observations into the existing model as they arrive.\n\n\n\nReal-Time Adaptation: The beauty of incremental learning lies in its ability to quickly respond to shifts in data distribution. This makes it ideal for applications dealing with rapidly evolving data, like online advertising or fraud detection.\nLow Memory Requirements: Incremental learning processes data in small batches or individual data points, reducing memory usage compared to batch learning, where the entire dataset needs to be stored.\nConstant Learning: With incremental learning, models can continuously learn and improve over time without the need for retraining on the entire dataset. It’s like a lifelong learning journey!\n\nAdvantages of Incremental Machine Learning\n\nEfficiency: Incremental learning is computationally efficient since it updates the model incrementally, without the need to retrain on the entire dataset each time.\nScalability: This approach is well-suited for large-scale applications where processing the entire dataset at once would be overwhelming.\nDynamic Adaptation: Incremental learning is ideal for applications where data distribution changes frequently, such as natural language processing or recommendation systems. It embraces the unexpected!\n\nDisadvantages of Incremental Machine Learning\n\nForgetting Old Data: Just like we can’t remember everything we’ve learned in the past, incremental learning models may forget information from the past, leading to potential performance degradation on older data points.\nModel Drift: In dynamic environments, incremental learning models may struggle to adapt to sudden shifts in data patterns, leading to inaccuracies. Sometimes, change can be overwhelming!"
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#batch-machine-learning-a-journey-with-stability",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#batch-machine-learning-a-journey-with-stability",
    "title": "Embracing Change: Incremental vs. Batch Machine Learning",
    "section": "",
    "text": "Now, let’s switch gears and explore batch machine learning. Picture a learning process that takes time to analyze and understand the whole story before making decisions. That’s batch machine learning for you! This approach involves training the model on the entire dataset and updating it periodically or when significant amounts of new data accumulate.\n\n\n\nPeriodic Updates: Batch learning involves training models from scratch on a fixed dataset, leveraging powerful hardware resources and sophisticated algorithms. It’s like taking a step back to see the bigger picture!\nComplete Data Usage: Unlike incremental learning, which processes data in smaller chunks, batch learning uses the entire dataset for training, ensuring no data points are left behind.\n\nAdvantages of Batch Machine Learning\n\nStrong Convergence: By training on the entire dataset, batch learning often achieves more stable and reliable results, especially with sufficient computational resources. It’s like taking the time to contemplate before making decisions.\nAccuracy: Batch learning models tend to be more accurate due to their exhaustive training process. Sometimes, thoroughness pays off!\n\nDisadvantages of Batch Machine Learning\n\nHigh Resource Demands: Batch learning requires substantial computational power and memory, making it less scalable for real-time applications or large datasets.\nTime Lag in Updates: Models may not adapt quickly to changes in data distribution since they are updated only after accumulating new data. Patience is key!"
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#advantages-of-batch-machine-learning",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#advantages-of-batch-machine-learning",
    "title": "Embracing Change: Incremental vs. Batch Machine Learning",
    "section": "",
    "text": "Strong Convergence: By training on the entire dataset, batch learning often achieves more stable and reliable results, especially with sufficient computational resources. It’s like taking the time to contemplate before making decisions.\nAccuracy: Batch learning models tend to be more accurate due to their exhaustive training process. Sometimes, thoroughness pays off!"
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#disadvantages-of-batch-machine-learning",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#disadvantages-of-batch-machine-learning",
    "title": "Embracing Change: Incremental vs. Batch Machine Learning",
    "section": "",
    "text": "High Resource Demands: Batch learning requires substantial computational power and memory, making it less scalable for real-time applications or large datasets.\nTime Lag in Updates: Models may not adapt quickly to changes in data distribution since they are updated only after accumulating new data. Patience is key!\n\nUse Cases for Incremental and Batch Machine Learning"
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#incremental-learning",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#incremental-learning",
    "title": "Embracing Change: Incremental vs. Batch Machine Learning",
    "section": "",
    "text": "Real-time fraud detection in financial transactions.\nAdaptive recommendation systems that learn from user interactions in real-time.\nSentiment analysis on streaming social media data.\n\nBatch Learning:\n\nImage classification in computer vision tasks.\nNatural language processing applications like language translation.\nTraining large-scale recommendation systems periodically."
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#conclusion",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#conclusion",
    "title": "Embracing Change: Incremental vs. Batch Machine Learning",
    "section": "",
    "text": "As we bid farewell to this exploration, we have gained an understanding of the intriguing differences between incremental and batch machine learning. Incremental learning embraces change and adapts to evolving data, making it perfect for real-time applications. On the other hand, batch learning takes a more measured approach, achieving strong convergence and accuracy with thorough analysis. Both methods have their unique strengths, empowering data scientists and developers to make informed choices based on the demands of their applications. So, whether you’re venturing on a dynamic journey with incremental learning or taking a stable path with batch learning, remember to embrace the wonders of machine learning and let it lead you to exciting discoveries!"
  },
  {
    "objectID": "posts/Unraveling the Power of Causal Machine Learning.html#the-challenge-of-causal-inference",
    "href": "posts/Unraveling the Power of Causal Machine Learning.html#the-challenge-of-causal-inference",
    "title": "Unraveling the Power of Causal Machine Learning",
    "section": "",
    "text": "Establishing causality is not a straightforward task. The fundamental problem lies in the fact that we cannot simultaneously observe both the treatment group (those who receive a particular intervention) and the control group (those who do not). Traditional observational data often suffers from confounding variables, making it challenging to disentangle cause and effect."
  },
  {
    "objectID": "posts/Unraveling the Power of Causal Machine Learning.html#enter-causal-machine-learning",
    "href": "posts/Unraveling the Power of Causal Machine Learning.html#enter-causal-machine-learning",
    "title": "Unraveling the Power of Causal Machine Learning",
    "section": "",
    "text": "Causal Machine Learning, a fascinating interdisciplinary field, bridges the gap between traditional machine learning and causal inference. Its primary goal is to leverage data to identify causal relationships and infer the effects of interventions accurately."
  },
  {
    "objectID": "posts/Unraveling the Power of Causal Machine Learning.html#causal-inference-methods",
    "href": "posts/Unraveling the Power of Causal Machine Learning.html#causal-inference-methods",
    "title": "Unraveling the Power of Causal Machine Learning",
    "section": "",
    "text": "Randomized Control Trials (RCTs): Considered the gold standard for establishing causality, RCTs involve randomly assigning subjects to either the treatment or control group. By eliminating confounding factors, RCTs provide strong causal evidence. However, they may not always be practical or ethical.\nPropensity Score Matching (PSM): When RCTs are not feasible, PSM is a popular method. It attempts to create a “quasi-experimental” design by matching treated and control units based on their propensity scores, which represent the likelihood of receiving the treatment. This helps mitigate confounding effects.\nInstrumental Variables (IV): IV analysis relies on instrumental variables that are correlated with the treatment but have no direct effect on the outcome. By utilizing these instruments, researchers can uncover causal relationships even in the presence of unobserved confounders.\nSynthetic Control (SC): The process of creating a synthetic counterfactual involves using historical data from similar individuals or groups that did not receive the treatment to construct a “synthetic” control group. This control group is designed to closely match the characteristics of the treated group before the intervention. By doing so, researchers aim to create a plausible estimate of what would have happened to the treated group had they not received the treatment."
  },
  {
    "objectID": "posts/Unraveling the Power of Causal Machine Learning.html#real-world-applications",
    "href": "posts/Unraveling the Power of Causal Machine Learning.html#real-world-applications",
    "title": "Unraveling the Power of Causal Machine Learning",
    "section": "",
    "text": "Causal Machine Learning has found applications in various domains, including public health, economics, social sciences, and marketing. It enables policymakers to make informed decisions, businesses to optimize interventions, and researchers to uncover hidden causal mechanisms."
  },
  {
    "objectID": "posts/Unraveling the Power of Causal Machine Learning.html#embracing-causal-machine-learning",
    "href": "posts/Unraveling the Power of Causal Machine Learning.html#embracing-causal-machine-learning",
    "title": "Unraveling the Power of Causal Machine Learning",
    "section": "",
    "text": "As the world becomes more data-driven, understanding causality becomes paramount. Embracing causal machine learning techniques empowers us to make smarter decisions, avoid biases, and design better interventions that have a genuine impact on our lives.\nAn excellent example application of Causal Machine Learning is in the field of healthcare for personalized treatment recommendations or to estimate the impact of marketing campaigns. Beyond the classic RCT or A/B testing, in both cases the use of Reinforcement Learning is a very interesting application that we will discuss in the next blog post."
  },
  {
    "objectID": "posts/Quantile Random Forest.html#random-forest-vs-qrf",
    "href": "posts/Quantile Random Forest.html#random-forest-vs-qrf",
    "title": "Quantile Random Forest",
    "section": "",
    "text": "The key difference between Random Forest and QRF lies in the prediction phase. In Random Forest, the predicted value for a new data point is the average (mean) of the individual tree predictions. However, in QRF, we aim to estimate multiple quantiles of the target variable’s distribution for a given input.\nWhen growing a tree in QRF, instead of splitting the data based solely on minimizing the variance or mean squared error, QRF performs splits to optimize quantile-specific criteria. Each tree node is split to minimize the quantile loss function, which reflects the error in predicting the specific quantile. This means that the tree seeks to minimize the difference between the true quantile value and the estimated quantile value for the data points that fall into each node.\nOnce the tree is fully grown, the terminal nodes (leaf nodes) contain specific quantile values rather than just the average value. Each terminal node in the QRF stores a quantile-specific estimate based on the data points that fall into that node. This means that each tree provides quantile-specific predictions.\nTo make predictions for a new data point, QRF evaluates the input through each decision tree, resulting in a set of quantile-specific predictions. The user can then choose the desired quantiles (e.g., 0.25, 0.50, 0.75) to obtain the corresponding quantile predictions. For example, if we want to estimate the 0.75 quantile, we collect the 0.75 quantile values from each tree, and these values form the quantile prediction for that data point.\nOne of the advantages of QRF is that it provides a measure of uncertainty in the predictions. Given the estimated quantiles, QRF can construct a prediction interval for each data point. The prediction interval provides a range of possible values, accounting for the uncertainty in the prediction due to the variability of the underlying data."
  },
  {
    "objectID": "posts/Quantile Random Forest.html#advantages",
    "href": "posts/Quantile Random Forest.html#advantages",
    "title": "Quantile Random Forest",
    "section": "",
    "text": "One of the advantages of QRF is that it provides a measure of uncertainty in the predictions. Given the estimated quantiles, QRF can construct a prediction interval for each data point. The prediction interval provides a range of possible values, accounting for the uncertainty in the prediction due to the variability of the underlying data.\nAdvantages of Quantile Random Forest\n\nRobustness to outliers: QRF can handle outliers effectively as it estimates the entire distribution, including extreme quantiles.\nProvides uncertainty information: By estimating quantiles, QRF offers a measure of uncertainty in the predictions, which is valuable in decision-making.\nVersatility: QRF can be applied to regression and quantile regression problems, and it can be extended to handle multivariate quantile estimation.\n\nApplications of Quantile Random Forest\n\nFinancial forecasting: QRF can be used to predict quantiles of financial variables, such as stock prices or asset returns, to assess the risk of investment decisions.\nMedical applications: In healthcare, QRF can estimate quantiles of patient response times to treatments, enabling doctors to tailor treatments accordingly.\nEnvironmental modeling: QRF can estimate quantiles of environmental variables like pollutant concentrations, assisting in environmental risk assessments."
  },
  {
    "objectID": "posts/Contextual Multi-Armed Bandit.html",
    "href": "posts/Contextual Multi-Armed Bandit.html",
    "title": "Contextual Multi-Armed Bandit: Maximizing Rewards with Intelligent Decision-Making",
    "section": "",
    "text": "Imagine a gambler facing a row of slot machines (arms), each with an unknown payout probability. The gambler’s goal is to maximize their cumulative reward while exploring the machines’ potential and exploiting the most promising ones. In recent years, the contextual multi-armed bandit (CMAB) has emerged as a powerful extension to this problem, incorporating context or additional information to make even more intelligent decisions. In this blog post, we will delve into the world of contextual multi-armed bandits, exploring their significance, strategies, and real-world applications.\n\n\nIn the classic multi-armed bandit problem, a gambler is faced with a set of arms (slot machines) that have different reward probabilities, but the gambler doesn’t know these probabilities initially. The gambler’s objective is to determine which arms to pull (explore) and which to favor (exploit) over time to maximize their cumulative reward. The challenge lies in striking a balance between exploring new arms to gather more information and exploiting the arms that seem to provide higher rewards.\n\n\n\nThe contextual multi-armed bandit takes the classic problem a step further by incorporating additional information or context. In CMAB, each arm is associated with a context, which can be thought of as features or attributes characterizing the current situation. This context provides valuable insights into the arms’ potential rewards and helps the decision-maker make more informed choices.\nFor instance, imagine a digital advertising scenario where ads (arms) are shown to users, and each ad has associated user characteristics as context, such as age, location, and interests. By considering the context, the CMAB algorithm can optimize the selection of ads for each user to maximize clicks or conversions.\n\n\n\nLinUCB Algorithm:\n\nOne popular approach for solving CMAB problems is the LinUCB algorithm. It uses linear models to estimate the expected rewards for each arm given the context. The algorithm maintains a confidence interval for each arm’s expected reward and selects the arm with the highest upper confidence bound, balancing exploration and exploitation.\n\nThompson Sampling:\n\nThompson Sampling is a widely used Bayesian approach for CMAB. It models the reward distribution for each arm based on observed rewards and context. The algorithm then samples from these distributions and selects the arm with the highest sample. This method effectively leverages uncertainty to explore and exploit in a principled manner.\nReal-World Applications of Contextual Multi-Armed Bandits\n\nPersonalized Recommendations:\n\nCMAB algorithms can be applied to personalized recommendation systems, where the context represents user preferences, past behavior, and demographics. By dynamically selecting the most relevant content or products, these systems can improve user engagement and satisfaction.\n\nHealthcare Treatment Selection:\n\nIn healthcare, CMAB can help optimize treatment selection based on patient characteristics, medical history, and response to previous treatments. By considering the context, doctors can make more data-driven decisions to improve patient outcomes.\n\nOnline Advertising:\n\nIn digital advertising, CMAB can enhance ad placement by taking into account user profiles, browsing behavior, and real-time context. This enables advertisers to show the most relevant ads to users, increasing click-through rates and conversions."
  },
  {
    "objectID": "posts/Contextual Multi-Armed Bandit.html#strategies-for-contextual-multi-armed-bandits",
    "href": "posts/Contextual Multi-Armed Bandit.html#strategies-for-contextual-multi-armed-bandits",
    "title": "Contextual Multi-Armed Bandit: Maximizing Rewards with Intelligent Decision-Making",
    "section": "",
    "text": "LinUCB Algorithm:\n\nOne popular approach for solving CMAB problems is the LinUCB algorithm. It uses linear models to estimate the expected rewards for each arm given the context. The algorithm maintains a confidence interval for each arm’s expected reward and selects the arm with the highest upper confidence bound, balancing exploration and exploitation.\n\nThompson Sampling:\n\nThompson Sampling is a widely used Bayesian approach for CMAB. It models the reward distribution for each arm based on observed rewards and context. The algorithm then samples from these distributions and selects the arm with the highest sample. This method effectively leverages uncertainty to explore and exploit in a principled manner.\nReal-World Applications of Contextual Multi-Armed Bandits\n\nPersonalized Recommendations:\n\nCMAB algorithms can be applied to personalized recommendation systems, where the context represents user preferences, past behavior, and demographics. By dynamically selecting the most relevant content or products, these systems can improve user engagement and satisfaction.\n\nHealthcare Treatment Selection:\n\nIn healthcare, CMAB can help optimize treatment selection based on patient characteristics, medical history, and response to previous treatments. By considering the context, doctors can make more data-driven decisions to improve patient outcomes.\n\nOnline Advertising:\n\nIn digital advertising, CMAB can enhance ad placement by taking into account user profiles, browsing behavior, and real-time context. This enables advertisers to show the most relevant ads to users, increasing click-through rates and conversions."
  },
  {
    "objectID": "posts/When in doubt, just model it. Modelling uncertainty.html",
    "href": "posts/When in doubt, just model it. Modelling uncertainty.html",
    "title": "When in doubt, just model it. Modelling uncertainty",
    "section": "",
    "text": "Probabilistic modeling is a powerful approach in statistics and machine learning that enables us to quantify uncertainty in our predictions and decisions. It allows us to represent and reason about uncertain or incomplete information by incorporating probability distributions over the model parameters or the outcomes themselves. In this context, we can distinguish between two types of uncertainty: uncertainty and deep uncertainty."
  },
  {
    "objectID": "posts/When in doubt, just model it. Modelling uncertainty.html#uncertainty",
    "href": "posts/When in doubt, just model it. Modelling uncertainty.html#uncertainty",
    "title": "When in doubt, just model it. Modelling uncertainty",
    "section": "Uncertainty",
    "text": "Uncertainty\nUncertainty refers to the lack of perfect knowledge about a particular event or outcome. In probabilistic modeling, uncertainty is captured by probability distributions. These distributions represent our beliefs about the likelihood of different outcomes or the variability in model parameters. By using probability distributions, we can express the confidence or lack thereof in our predictions.\nFor example, in linear regression, instead of providing a single point estimate for the model coefficients, we can use a probability distribution to describe the uncertainty around these coefficients. This allows us to understand the range of plausible values and the level of confidence we have in our estimates.\nProbabilistic modeling allows us to make more robust decisions by considering the uncertainty and accounting for potential errors in our predictions. It is widely used in various applications, including finance, healthcare, weather forecasting, and natural language processing."
  },
  {
    "objectID": "posts/When in doubt, just model it. Modelling uncertainty.html#deep-uncertainty",
    "href": "posts/When in doubt, just model it. Modelling uncertainty.html#deep-uncertainty",
    "title": "When in doubt, just model it. Modelling uncertainty",
    "section": "Deep Uncertainty",
    "text": "Deep Uncertainty\nDeep uncertainty, on the other hand, goes beyond simple uncertainty and arises when we have limited knowledge about the underlying data-generating process or when there is ambiguity in the model assumptions. In such cases, traditional probabilistic models may not be sufficient to capture the full extent of uncertainty.\nDeep uncertainty is particularly prevalent in complex and chaotic systems, where there are many interacting variables and non-linear relationships that are difficult to model precisely. In these scenarios, traditional probabilistic models may yield unreliable estimates or may not provide meaningful uncertainty quantification.\nTo address deep uncertainty, researchers have developed alternative methods, such as robust optimization, scenario-based analysis, and Bayesian non-parametric models. These techniques are designed to handle situations where the underlying assumptions are uncertain or where we lack sufficient data to make accurate probabilistic estimates.\nIn recent years, the development of deep learning and Bayesian deep learning has also allowed us to tackle deep uncertainty in more complex models. Bayesian neural networks, for example, use Bayesian inference to represent uncertainty in neural network weights, enabling better uncertainty quantification in deep learning models."
  },
  {
    "objectID": "posts/Logistic Regression and Marginal Effects.html#logistic-regression",
    "href": "posts/Logistic Regression and Marginal Effects.html#logistic-regression",
    "title": "Logistic Regression and Marginal Effects",
    "section": "",
    "text": "In logistic regression, we model the log-odds of the event of interest occurring as a linear combination of predictor variables. The logistic function (sigmoid function) is then applied to this linear combination to convert it into a probability value between 0 and 1.\nThe logistic regression equation is given by:\n\\[\\text{logit}(P(Y=1)) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p\\]\nWhere:\n\n\\(P(Y=1)\\) is the probability of the event of interest (e.g., success) occurring.\n\\(logit (P(Y=1))\\) is the log-odds (logit) of the event occurring.\n\\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_p\\) are the coefficients of the model.\n\\(X_1, X_2, ..., X_p\\) are the predictor variables."
  },
  {
    "objectID": "posts/Logistic Regression and Marginal Effects.html#practical-example",
    "href": "posts/Logistic Regression and Marginal Effects.html#practical-example",
    "title": "Logistic Regression and Marginal Effects",
    "section": "Practical Example",
    "text": "Practical Example\nLet’s consider an example to illustrate the interpretation of coefficients in logistic regression. Suppose we are studying the factors that influence whether a customer will make a purchase on an e-commerce website. Our logistic regression model includes two predictor variables: “Time Spent on Website” and “Number of Items Added to Cart.” The estimated coefficients are as follows:\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\)\n\\(\\beta_{\\text{Number of Items}} = 0.2\\)\n\n\nInterpretation\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\): For each additional minute a customer spends on the website, the log-odds (probability) of making a purchase increases by 0.05 units. This means that spending more time on the website is associated with a higher likelihood of making a purchase.\n\\(\\beta_{\\text{Number of Items}} = 0.2\\): For each additional item a customer adds to the cart, the log-odds (probability) of making a purchase increases by 0.2 units. This suggests that adding more items to the cart is associated with a higher probability of making a purchase."
  },
  {
    "objectID": "posts/Contextual Multi-Armed Bandit.html#the-classic-multi-armed-bandit-problem",
    "href": "posts/Contextual Multi-Armed Bandit.html#the-classic-multi-armed-bandit-problem",
    "title": "Contextual Multi-Armed Bandit: Maximizing Rewards with Intelligent Decision-Making",
    "section": "",
    "text": "In the classic multi-armed bandit problem, a gambler is faced with a set of arms (slot machines) that have different reward probabilities, but the gambler doesn’t know these probabilities initially. The gambler’s objective is to determine which arms to pull (explore) and which to favor (exploit) over time to maximize their cumulative reward. The challenge lies in striking a balance between exploring new arms to gather more information and exploiting the arms that seem to provide higher rewards."
  },
  {
    "objectID": "posts/Contextual Multi-Armed Bandit.html#introducing-contextual-multi-armed-bandit",
    "href": "posts/Contextual Multi-Armed Bandit.html#introducing-contextual-multi-armed-bandit",
    "title": "Contextual Multi-Armed Bandit: Maximizing Rewards with Intelligent Decision-Making",
    "section": "",
    "text": "The contextual multi-armed bandit takes the classic problem a step further by incorporating additional information or context. In CMAB, each arm is associated with a context, which can be thought of as features or attributes characterizing the current situation. This context provides valuable insights into the arms’ potential rewards and helps the decision-maker make more informed choices.\nFor instance, imagine a digital advertising scenario where ads (arms) are shown to users, and each ad has associated user characteristics as context, such as age, location, and interests. By considering the context, the CMAB algorithm can optimize the selection of ads for each user to maximize clicks or conversions.\n\n\n\nLinUCB Algorithm:\n\nOne popular approach for solving CMAB problems is the LinUCB algorithm. It uses linear models to estimate the expected rewards for each arm given the context. The algorithm maintains a confidence interval for each arm’s expected reward and selects the arm with the highest upper confidence bound, balancing exploration and exploitation.\n\nThompson Sampling:\n\nThompson Sampling is a widely used Bayesian approach for CMAB. It models the reward distribution for each arm based on observed rewards and context. The algorithm then samples from these distributions and selects the arm with the highest sample. This method effectively leverages uncertainty to explore and exploit in a principled manner.\nReal-World Applications of Contextual Multi-Armed Bandits\n\nPersonalized Recommendations:\n\nCMAB algorithms can be applied to personalized recommendation systems, where the context represents user preferences, past behavior, and demographics. By dynamically selecting the most relevant content or products, these systems can improve user engagement and satisfaction.\n\nHealthcare Treatment Selection:\n\nIn healthcare, CMAB can help optimize treatment selection based on patient characteristics, medical history, and response to previous treatments. By considering the context, doctors can make more data-driven decisions to improve patient outcomes.\n\nOnline Advertising:\n\nIn digital advertising, CMAB can enhance ad placement by taking into account user profiles, browsing behavior, and real-time context. This enables advertisers to show the most relevant ads to users, increasing click-through rates and conversions."
  },
  {
    "objectID": "posts/Quantile Random Forest.html#conclusion",
    "href": "posts/Quantile Random Forest.html#conclusion",
    "title": "Quantile Random Forest",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, Quantile Random Forest is a powerful extension of the Random Forest algorithm that estimates the entire distribution of the target variable through quantile-specific predictions. It is useful when capturing the entire distribution or quantifying uncertainty is essential for making informed decisions in various applications such as finance, healthcare, and environmental modeling."
  },
  {
    "objectID": "posts/Quantile Random Forest.html#advantages-of-quantile-random-forest",
    "href": "posts/Quantile Random Forest.html#advantages-of-quantile-random-forest",
    "title": "Quantile Random Forest",
    "section": "Advantages of Quantile Random Forest",
    "text": "Advantages of Quantile Random Forest\n\nRobustness to outliers: QRF can handle outliers effectively as it estimates the entire distribution, including extreme quantiles.\nProvides uncertainty information: By estimating quantiles, QRF offers a measure of uncertainty in the predictions, which is valuable in decision-making.\nVersatility: QRF can be applied to regression and quantile regression problems, and it can be extended to handle multivariate quantile estimation."
  },
  {
    "objectID": "posts/Quantile Random Forest.html#applications-of-quantile-random-forest",
    "href": "posts/Quantile Random Forest.html#applications-of-quantile-random-forest",
    "title": "Quantile Random Forest",
    "section": "Applications of Quantile Random Forest",
    "text": "Applications of Quantile Random Forest\n\nFinancial forecasting: QRF can be used to predict quantiles of financial variables, such as stock prices or asset returns, to assess the risk of investment decisions.\nMedical applications: In healthcare, QRF can estimate quantiles of patient response times to treatments, enabling doctors to tailor treatments accordingly.\nEnvironmental modeling: QRF can estimate quantiles of environmental variables like pollutant concentrations, assisting in environmental risk assessments."
  },
  {
    "objectID": "posts/Logistic Regression and Marginal Effects.html#interpretation",
    "href": "posts/Logistic Regression and Marginal Effects.html#interpretation",
    "title": "Logistic Regression and Marginal Effects",
    "section": "Interpretation",
    "text": "Interpretation\n\n\\(\\beta_{\\text{Time Spent}} = 0.05\\): For each additional minute a customer spends on the website, the log-odds (probability) of making a purchase increases by 0.05 units. This means that spending more time on the website is associated with a higher likelihood of making a purchase.\n\\(\\beta_{\\text{Number of Items}} = 0.2\\): For each additional item a customer adds to the cart, the log-odds (probability) of making a purchase increases by 0.2 units. This suggests that adding more items to the cart is associated with a higher probability of making a purchase."
  },
  {
    "objectID": "posts/Logistic Regression and Marginal Effects.html#interpretation-issues",
    "href": "posts/Logistic Regression and Marginal Effects.html#interpretation-issues",
    "title": "Logistic Regression and Marginal Effects",
    "section": "Interpretation Issues",
    "text": "Interpretation Issues\nIdeally, we want to understand what the model is telling us on the probability scale and not in the odds scale, much less in the estimation scale, the log-odds.\nWe might be tempted to convert log-odds to odds and then to probability and interpret everything as usual. But this would be very wrong!\nWhy?\nThe issue here is that on the probability scale the function is non-linear!"
  },
  {
    "objectID": "posts/Logistic Regression and Marginal Effects.html#finite-differences",
    "href": "posts/Logistic Regression and Marginal Effects.html#finite-differences",
    "title": "Logistic Regression and Marginal Effects",
    "section": "Finite differences",
    "text": "Finite differences\nDifferential calculus is a branch of calculus that focuses on the study of rates of change and slopes of curves. It deals with the concept of derivatives, which represent the rate of change of a function at a specific point. Derivatives allow us to understand how a function is changing at a particular location and provide valuable insights into the behavior of functions.\nThe derivative of a function \\(f(x)\\) with respect to \\(x\\) is denoted as \\(f'(x)\\) or \\(\\frac{df(x)}{dx}\\). It represents the slope of the tangent line to the graph of the function at a given point \\(x\\). The derivative can be interpreted as the instantaneous rate of change of \\(f(x)\\) with respect to \\(x\\).\nFor example, consider the function \\(f(x) = x^2\\). The derivative of \\(f(x)\\) is \\(f'(x) = 2x\\). This means that the slope of the tangent line to the graph of \\(f(x)\\) at any point \\(x\\) is twice the value of \\(x\\) at that point.\nDifferential calculus has wide-ranging applications in various fields, including physics, engineering, economics, and more. It is used to model and analyze phenomena that involve continuous changes, such as motion, growth, and optimization.\nFinite differences are a numerical technique used to approximate derivatives of functions. The method involves computing the difference in function values at two nearby points and then dividing by the difference in their corresponding independent variable values. By using a small interval between the two points, we can get an approximation of the derivative at a specific point.\nThe finite difference formula for approximating the first derivative of a function \\(f(x)\\) at a point \\(x\\) is given by:\n\\[ f’(x) \\approx \\frac{f(x + h) - f(x)}{h} \\]\nWhere:\n\n\\(h\\) is a small value representing the step size or interval between the points.\n\nSimilarly, we can use finite differences to approximate higher-order derivatives, such as the second derivative:\n\\[ f’’(x) \\approx \\frac{f(x + h) - 2f(x) + f(x - h)}{h^2} \\]\nFinite differences are particularly useful when the analytical expression for the derivative is difficult to obtain or when dealing with discrete data points instead of continuous functions.\nDifferential calculus and finite differences are both essential tools in mathematics and scientific disciplines. Differential calculus provides a rigorous framework for studying rates of change and slopes of curves through derivatives. On the other hand, finite differences offer a practical and numerical approach to approximate derivatives when the analytical solution is not readily available or when dealing with discrete data. Together, these concepts enable us to better understand the behavior of functions and analyze real-world phenomena involving continuous and discrete changes."
  },
  {
    "objectID": "posts/Developing in a Docker container.html",
    "href": "posts/Developing in a Docker container.html",
    "title": "Developing in a Docker container",
    "section": "",
    "text": "Developing within a Docker container has become a common practice in the software development workflow. It offers various benefits, such as ensuring consistent environments, reproducibility, and isolation.\nUsing docker means not having to have complex software server applications installed locally. Instead, they are run in ‘sealed’ containers, held discrete from the local computer. In an example Use Case of developing code in Python, for example, a docker container holding the latest version of Python, plus the Python script can be built and run. However, in this use case, to edit the Python script and rerun it, the image container needs to be rebuilt in docker each time there are changes made to the code. Docker can use ‘bind mounts’ to circumvent this for development purposes. The folder holding the source code is ‘mounted’ into the container image, meaning that the source code can then be edited and run immediately in the image without needing to be rebuilt.\nCombining Docker with a bind mount in VScode makes life so much easier. Here’s a step-by-step guide to get started with developing in a Docker container:\n\nInstall Docker: Make sure you have Docker installed on your system. You can download and install it from the official Docker website based on your operating system. 2. Create a Dockerfile: A Dockerfile is a text file that contains instructions for building a Docker image. Create a new file named Dockerfile in your project’s root directory. Here’s a basic example of a Python project:\n\n# Use an official Python runtime as the base image\nFROM python:3.10.6-buster\n\nCOPY requirements.txt /requirements.txt\n\nRUN pip install --upgrade pip RUN pip install -r requirements.txt\n\n# Set the working directory\n\nWORKDIR /app\n\n# Define the command to run when the container starts\n\nCMD \\[\"bash\"\\]\n\nBuild the Docker Image: Open a terminal, navigate to your project’s directory containing the Dockerfile, and run the following command to build the Docker image:\n\ndocker build -t myapp .\nHere, myapp is the name you’re giving to your Docker image, and the dot . indicates the build context (current directory).\n\nRun the Docker Container: After building the image, you can run a container from it using the following command:\n\ndocker run -d -it --hostname yourname -v /path/to/folder/holding/the/source/code:/app myapp\nHere, -d runs the container in detached mode, -it allocates a pseudo-TTY and keeps STDIN open even if not attached, -- hostname simply indicates the container’s hostname , finally -v will bind mount a volume following given its path.\n\nConnect with VScode: Now that you have your development environment inside a Docker container, you can edit your code directly inside the container using VScode. Changes you make to your code will be reflected within the container. This way, you can edit code on your host machine and see the changes in the container immediately.\nTo do so, you will need to install the docker extension as shown below.\n\n\nOnce you have installed the extension, go on the docker icon in the left side bar and this is what you will see.\n\nAnd here is the app folder inside the container.\n\nFinally you just need to right click on the container and you will see th option to Attach Visual Studio Code. This will open a new VScode window. Now you are inside the container you can start to develop. All changes you will made to the files in the attached folder will affect the local folder as well."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "📽️️ Slides",
    "section": "",
    "text": "Data Management in Python. Here I showcasing how Pandas empowers us to efficiently handle, analyze, and manipulate datasets\n\n\nStatistics & Probabilities. All you need to know to get your started with Statistics & Probabilities\n\n\nIntro to ML. History and application of Machine Learning with Python\n\n\nLinear Regression. Running, intepreting and using the most famous algorithm ever!\n\n\nEnsemble Methods. Improve accuracy of results in models by combining multiple models instead of using a single model. All you need to know about Trees algorithms and more.\n\nModel Tuning. Adjusting a machine learning model involves tweaking specific settings, like control knobs, to help it learn better."
  },
  {
    "objectID": "posts/Data Science Books.html",
    "href": "posts/Data Science Books.html",
    "title": "Data Science Books",
    "section": "",
    "text": "Whether you’re a seasoned data scientist looking to expand your knowledge or a newcomer eager to dive into this exciting field, I made this curated list of must-read books that will support you throughout your career or journey with data, statistics, and machine learning.\n\n\n\n\n\n\n\n“Introductory Econometrics: A Modern Approach” by Jeffrey M. Wooldridge is a widely acclaimed textbook that serves as a comprehensive introduction to the field of econometrics. This book is a valuable resource for students, researchers, and practitioners interested in using statistical methods to analyze economic data. Wooldridge’s approach is modern and intuitive, making complex econometric concepts accessible to readers at various levels of expertise.\nThe book covers key topics such as regression analysis, hypothesis testing and more advanced teniques such as instrumental variables, panel data analysis, and time series analysis. What sets it apart is its focus on real-world applications, using examples and datasets from various fields of economics to illustrate the concepts discussed. This practical approach helps readers bridge the gap between theory and practice, preparing them to conduct meaningful empirical research in economics.\nFurthermore, Wooldridge’s writing style is clear and engaging, making it easier for readers to grasp challenging econometric concepts. With its emphasis on modern techniques and hands-on applications, “Introductory Econometrics: A Modern Approach” has become a trusted resource in the field, helping students and researchers develop the skills needed to analyze and interpret economic data effectively.\n\n\n\n\n\n\n\n\n“An Introduction to Statistical Learning” by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor is a comprehensive textbook that provides an introduction to statistical learning methods. It covers key concepts and techniques in statistical learning, offering a balanced blend of theory and practical applications. Notably, the book is accessible to users of both R and Python programming languages, making it versatile for a wide range of readers.\nThe book begins by introducing fundamental concepts in statistical learning. It then covers topics such as linear regression, classification methods, resampling techniques, model selection, and regularization. Nonlinear models, tree-based methods, support vector machines, and unsupervised learning methods are also discussed.\nA distinctive feature of the book is its emphasis on practical applications and the statistical nature of Machine Learning, drawing important parallelisms between the classic vocabulary used in statistics and the the Machine Learning jargon. It includes case studies and examples throughout, illustrating how statistical learning methods can be applied to real-world problems. The inclusion of R and Python code examples further enhances its utility for readers using either programming language.\nIn summary, “An Introduction to Statistical Learning” is a valuable resource for students, researchers, and practitioners seeking a solid foundation in statistical learning. Its flexibility in catering to both R and Python users makes it an inclusive and versatile choice for individuals interested in applying statistical learning techniques to data analysis and modeling.\n\n\n\n\n\n\n\n\n“Introduction to Machine Learning with Python: A Guide for Data Scientists” by Andreas C. Müller and Sarah Guido is a highly regarded and practical book that provides an excellent introduction to machine learning concepts and their implementation using Python. This book is particularly beneficial for data scientists, engineers, and anyone interested in learning the fundamentals of machine learning in a hands-on manner.\nThe book covers a wide range of machine learning topics, including supervised learning, unsupervised learning, and deep learning. It also introduces essential libraries and tools for machine learning in Python, such as scikit-learn and TensorFlow. One of the strengths of this book is its focus on practical examples and code snippets, which allow readers to apply what they learn immediately.\nOverall, “Introduction to Machine Learning with Python” is a valuable resource for those looking to gain a solid foundation in machine learning while leveraging the power of the Python programming language. It’s suitable for both beginners and intermediate learners and provides a practical and hands-on approach to mastering the essentials of machine learning for data science.\n\n\n\n\n\n\n\n\n“Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems” by Aurélien Géron is a popular and comprehensive book that provides a practical guide to building machine learning models using three widely-used libraries: scikit-learn, Keras, and TensorFlow. The book is designed for both beginners and experienced practitioners in the field of machine learning and artificial intelligence.\nThe book is known for its clarity, practicality, and hands-on approach. It has been praised for its ability to take readers from the fundamentals of machine learning to building and deploying sophisticated machine learning models. Whether you’re a beginner looking to learn the basics or an experienced data scientist seeking to expand your knowledge, “Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow” is a valuable resource for anyone interested in the field of machine learning.\n\n\n\n\n\n\n\n\n“Deep Learning with Python” by François Chollet is a highly regarded book that serves as a practical guide to understanding and implementing deep learning techniques using the Python programming language and the Keras deep learning framework. François Chollet is the creator of Keras, which has become a popular tool for building deep neural networks due to its simplicity and flexibility.\nThis must read book is known for its accessibility and practical approach. It’s suitable for readers with varying levels of experience in deep learning, from beginners looking to get started to experienced practitioners seeking to deepen their knowledge. The book’s focus on Keras and Python makes it an excellent choice for those interested in hands-on deep learning projects, and Chollet’s expertise as the creator of Keras ensures that readers receive valuable insights into the field of deep learning.\n\n\n\n\n\n\n\n\nThe Incerto book series, written by Nassim Nicholas Taleb, is a collection of five interconnected books that delve into the themes of uncertainty, probability, risk, and decision-making under conditions of unpredictability. Nassim Nicholas Taleb is a renowned author, philosopher, and former Wall Street trader who has become well-known for his ideas on risk and randomness.\nThe series is known for its challenging and unconventional ideas, and Taleb’s writing style combines philosophy, economics, and practical insights to encourage readers to think critically about risk, uncertainty, and decision-making in a complex world. The series has gained a dedicated following and has had a significant impact on discussions around risk management, finance, and decision science. It is a fundamental and enjoyably read (no code) to understand how reality works and how our own intuitions are sometimes completely misleading."
  },
  {
    "objectID": "posts/Data Science project Boilerplate.html",
    "href": "posts/Data Science project Boilerplate.html",
    "title": "Data Science project Boilerplate",
    "section": "",
    "text": "A data science boilerplate, in the context of software development and data science projects, refers to a standardized and reusable set of code, templates, libraries, and best practices that are pre-defined and organized to kickstart a data science project. It serves as a foundation or starting point for data scientists and analysts, helping them save time and effort when beginning a new project or analysis. Here’s an ideal data science boilerplate that I recommend for basically any project.\n\nThe folder structure\n.\n├── data\n│   └── data.csv\n├── Dockerfile\n├── Makefile\n├── .env\n├──.envrc\n├── .gitignore\n├── notebooks\n│   └── datascientist_deliverable.ipynb\n├── README.md\n├── requirements.txt\n├── scripts\n│   └── script.py\n├── setup.py\n└── thepkg\n    ├── __init__.py\n    ├── interface\n    │   └── __init__.py\n    ├── ml_logic\n    │   ├── data.py\n    │   ├── __init__.py\n    │   ├── model.py\n    │   └── preprocessor.py\n    ├── params.py\n    └── utils.py\n\nThis folder structure represents a typical directory layout for a data science project. Each folder and file serves a specific purpose in organizing and managing the project’s code, data, documentation, and other resources. I will now give an explanation of each item in the structure:\n\ndata: This folder contains the project’s data files. In this case, there is a single CSV file named data.csv, but you can add more data files as needed.\nDockerfile: This file is used to define the instructions for creating a Docker container for your project. Docker allows you to encapsulate your project environment and dependencies for consistency and portability. This is a key element for most of my projects since I love to delevope inside a Docker Container (see my other blog post on how to use Docker with ‘bind mounts’.)\nMakefile: A Makefile contains a set of rules and commands for building, testing, and running various aspects of your project. It can automate common development tasks.\n.env: This file is often used to store environment variables specific to your project. These variables can include API keys, database connection strings, or other sensitive information.\n.envrc: This file is typically used in conjunction with a tool like direnv to manage environment variables for your project, ensuring that the correct environment is set up when you enter the project directory.\n.gitignore: This file specifies files and folders that should be ignored by Git when tracking changes. It helps avoid including sensitive or unnecessary files in version control.\nnotebooks: This folder is meant for Jupyter notebooks used for data exploration, analysis, and documentation. In this case, there’s a single notebook file named datascientist_deliverable.ipynb.\nREADME.md: This Markdown file is used to provide an overview and documentation of the project. It typically includes project goals, setup instructions, usage examples, and other relevant information.\nrequirements.txt: This file lists the Python packages and their versions required for the project. You can use it to recreate the project’s environment on another system.\nscripts: This folder is intended for Python scripts that are part of your project. In this example, there’s a single script file named script.py.\nsetup.py: This is a Python script used for packaging and distributing your project as a Python package and is directly related with the folder `thepkg`. It’s often used when you want to share your code with others or publish it on platforms like PyPI.\nthepkg: This folder represents the main Python package of your project. In development phase we would install the package using the classic pip install . e (editablemode) to updated function without having to reinstalling it over and over again. The folder is organized in a way that follows Python package conventions:\n\ninit.py: These files indicate that the directories are Python packages and can be imported as modules.\ninterface: This subpackage appears to be a module for defining an interface or API for your project.\nml_logic: This subpackage seems to contain modules related to machine learning logic, including data processing (data.py and preprocessor.py) and modeling (model.py).\nparams.py: This file could contain project-specific configuration parameters or settings.\nutils.py: This file likely contains utility functions or helper code used throughout the project.\n\n\nOverall, this folder structure provides a well-organized framework for a data science project, making it easier to collaborate, manage dependencies, and maintain consistency in your work.\nRemember that I boilerplate is just a template. Feel free to use this structure as a first step into your new Data Science project.\nFinally, here is a little bash script to create the entire folder structure.\n\n\n\n\nCode\n#!/bin/bash\n\n# Create the main project directory\nmkdir -p my_data_science_project\n\n# Create subdirectories and files\ncd my_data_science_project\n\nmkdir -p data\ntouch data/data.csv\n\ntouch Dockerfile\ntouch Makefile\ntouch .env\ntouch .envrc\ntouch .gitignore\n\nmkdir -p notebooks\ntouch notebooks/datascientist_deliverable.ipynb\n\ntouch README.md\ntouch requirements.txt\n\nmkdir -p scripts\ntouch scripts/script.py\n\ntouch setup.py\n\nmkdir -p thepkg\ntouch thepkg/__init__.py\n\nmkdir -p thepkg/interface\ntouch thepkg/interface/__init__.py\n\nmkdir -p thepkg/ml_logic\ntouch thepkg/ml_logic/data.py\ntouch thepkg/ml_logic/__init__.py\ntouch thepkg/ml_logic/model.py\ntouch thepkg/ml_logic/preprocessor.py\n\ntouch thepkg/params.py\ntouch thepkg/utils.py"
  },
  {
    "objectID": "posts/Data Science Books.html#introductory-econometrics-a-modern-approach-by-jeffrey-m.-wooldridge",
    "href": "posts/Data Science Books.html#introductory-econometrics-a-modern-approach-by-jeffrey-m.-wooldridge",
    "title": "Data Science Books",
    "section": "",
    "text": "“Introductory Econometrics: A Modern Approach” by Jeffrey M. Wooldridge is a widely acclaimed textbook that serves as a comprehensive introduction to the field of econometrics. This book is a valuable resource for students, researchers, and practitioners interested in using statistical methods to analyze economic data. Wooldridge’s approach is modern and intuitive, making complex econometric concepts accessible to readers at various levels of expertise.\nThe book covers key topics such as regression analysis, hypothesis testing and more advanced teniques such as instrumental variables, panel data analysis, and time series analysis. What sets it apart is its focus on real-world applications, using examples and datasets from various fields of economics to illustrate the concepts discussed. This practical approach helps readers bridge the gap between theory and practice, preparing them to conduct meaningful empirical research in economics.\nFurthermore, Wooldridge’s writing style is clear and engaging, making it easier for readers to grasp challenging econometric concepts. With its emphasis on modern techniques and hands-on applications, “Introductory Econometrics: A Modern Approach” has become a trusted resource in the field, helping students and researchers develop the skills needed to analyze and interpret economic data effectively."
  },
  {
    "objectID": "posts/Data Science Books.html#introduction-to-machine-learning-with-python-a-guide-for-data-scientists-1st-edition-by-andreas-müller-sarah-guido",
    "href": "posts/Data Science Books.html#introduction-to-machine-learning-with-python-a-guide-for-data-scientists-1st-edition-by-andreas-müller-sarah-guido",
    "title": "Data Science Books",
    "section": "",
    "text": "“Introduction to Machine Learning with Python: A Guide for Data Scientists” by Andreas C. Müller and Sarah Guido is a highly regarded and practical book that provides an excellent introduction to machine learning concepts and their implementation using Python. This book is particularly beneficial for data scientists, engineers, and anyone interested in learning the fundamentals of machine learning in a hands-on manner.\nThe book covers a wide range of machine learning topics, including supervised learning, unsupervised learning, and deep learning. It also introduces essential libraries and tools for machine learning in Python, such as scikit-learn and TensorFlow. One of the strengths of this book is its focus on practical examples and code snippets, which allow readers to apply what they learn immediately.\nOverall, “Introduction to Machine Learning with Python” is a valuable resource for those looking to gain a solid foundation in machine learning while leveraging the power of the Python programming language. It’s suitable for both beginners and intermediate learners and provides a practical and hands-on approach to mastering the essentials of machine learning for data science."
  },
  {
    "objectID": "posts/Data Science Books.html#hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow-concepts-tools-and-techniques-to-build-intelligent-systems-by-aurélien-géron",
    "href": "posts/Data Science Books.html#hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow-concepts-tools-and-techniques-to-build-intelligent-systems-by-aurélien-géron",
    "title": "Data Science Books",
    "section": "",
    "text": "“Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems” by Aurélien Géron is a popular and comprehensive book that provides a practical guide to building machine learning models using three widely-used libraries: scikit-learn, Keras, and TensorFlow. The book is designed for both beginners and experienced practitioners in the field of machine learning and artificial intelligence.\nThe book is known for its clarity, practicality, and hands-on approach. It has been praised for its ability to take readers from the fundamentals of machine learning to building and deploying sophisticated machine learning models. Whether you’re a beginner looking to learn the basics or an experienced data scientist seeking to expand your knowledge, “Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow” is a valuable resource for anyone interested in the field of machine learning."
  },
  {
    "objectID": "posts/Data Science Books.html#deep-learning-with-python-by-francois-chollet",
    "href": "posts/Data Science Books.html#deep-learning-with-python-by-francois-chollet",
    "title": "Data Science Books",
    "section": "",
    "text": "“Deep Learning with Python” by François Chollet is a highly regarded book that serves as a practical guide to understanding and implementing deep learning techniques using the Python programming language and the Keras deep learning framework. François Chollet is the creator of Keras, which has become a popular tool for building deep neural networks due to its simplicity and flexibility.\nThis must read book is known for its accessibility and practical approach. It’s suitable for readers with varying levels of experience in deep learning, from beginners looking to get started to experienced practitioners seeking to deepen their knowledge. The book’s focus on Keras and Python makes it an excellent choice for those interested in hands-on deep learning projects, and Chollet’s expertise as the creator of Keras ensures that readers receive valuable insights into the field of deep learning."
  },
  {
    "objectID": "posts/Data Science Books.html#incerto-book-series-by-nassim-nicholas-taleb",
    "href": "posts/Data Science Books.html#incerto-book-series-by-nassim-nicholas-taleb",
    "title": "Data Science Books",
    "section": "",
    "text": "The Incerto book series, written by Nassim Nicholas Taleb, is a collection of five interconnected books that delve into the themes of uncertainty, probability, risk, and decision-making under conditions of unpredictability. Nassim Nicholas Taleb is a renowned author, philosopher, and former Wall Street trader who has become well-known for his ideas on risk and randomness.\nThe series is known for its challenging and unconventional ideas, and Taleb’s writing style combines philosophy, economics, and practical insights to encourage readers to think critically about risk, uncertainty, and decision-making in a complex world. The series has gained a dedicated following and has had a significant impact on discussions around risk management, finance, and decision science. It is a fundamental and enjoyably read (no code) to understand how reality works and how our own intuitions are sometimes completely misleading."
  },
  {
    "objectID": "posts/Bayesian updating.html",
    "href": "posts/Bayesian updating.html",
    "title": "Bayesian updating",
    "section": "",
    "text": "To elucidate the workings of Bayesian updating, I often find it helpful to draw parallels from everyday experiences. Previously, I’ve used examples like my dog and my first date with my girlfriend. This time, let’s delve into the concept using a job interview scenario.\n\n\nImagine you’ve landed a job interview – you’re simultaneously excited and apprehensive. As you stand at the threshold, you’re completely uncertain about what awaits you inside. You don’t know who the interviewers are, what to expect, or how the interview will unfold. The odds of everything going either terribly wrong or wonderfully right seem equally plausible.\nIn the realm of probability, we can liken your state of mind to a uniform distribution, where all outcomes are equally probable. This state of not knowing what will happen or how the interview will conclude is what Bayesian terminology dubs the “prior belief” – it’s the starting point for your assessment.\n\n\n\n\n\n\n\n\nInterestingly, we still call it a “prior belief” even if you harbor some inkling about how the interview might unfold. For instance, let’s say you had an unpleasant preliminary conversation with the company, and as you enter the room, you’re not filled with optimism, estimating a 40% chance of success. In Bayesian terms, this, too, can be translated into probabilities.\n\n\n\n\n\nKeep in mind, though, that this assessment is occurring just as you’re about to step through the door.\n\n\n\nNow, envision yourself crossing that threshold, and the interviewers pose their initial question. Your response is swift, and you notice a confident smile on the interviewer’s face. This moment represents a pivotal piece of new information drawn from your real-time experience, and naturally, you’ll use it to revise your initial belief about the interview’s outcome. We call this new information and “evidence”.\nWhat’s happening inside your mind, that surge of heightened confidence, can once again be translated into numerical probabilities to update your initial belief. This is precisely where the Bayes comes into play.\nLet’s show how the update takes place. Let’s assign to that smile, the evidence, a probability of 80% to pass the interview.\n\n\n\n\n\n\n\n\nNow, let’s plug in all the information we have in the Bayesian therm we all know. We started with a not so confident prior belief (40% chance of success), then we got a new smiling evidence (80% chance of success). From here we can combine these two information and calculate our updated beliefs or as we call them “posterior beliefs”.\n\n\n\n\n\nAs you can see, now you have around 60% of passing the interview. These process can keep on going with every new information you collect during the interview as in this gif.\n\n\n\n\n\nIn essence, this is the core of Bayesian updating. It allows you to adapt your beliefs based on new information as you progress through an uncertain situation, much like navigating the twists and turns of a job interview.\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\n#make this example reproducible.\nnp.random.seed(1)\ndataUni = np.random.uniform(size=1000)\nsns.kdeplot(dataUni)\n# Adjust x-axis limits to show the tails\nplt.xlim(-0.1, 1.1)\nplt.title('Prior Belief with no idea')\n# Show the plot\n#plt.show()\n##########################################################\n# Define the parameters for the prior belief and evidence\nprior_prob = 0.4\n# Create a range of values for the probability (x-axis)\nx = np.linspace(0, 1, 1000)\n# Calculate the prior probability distribution (prior belief)\nprior_distribution = stats.beta.pdf(x, 2, 3)  # Beta distribution parameters (2, 3)\nsns.lineplot(x=x, y=prior_distribution,color=\"blue\")\nplt.title('Prior Belief with some idea')\n# Show the plot\n#plt.show()\n\nevidence_prob = 0.8\n# Create a range of values for the probability (x-axis)\nx = np.linspace(0, 1, 1000)\n\nevidence_distribution = stats.beta.pdf(x, 4, 2)  # Beta distribution parameters (4, 2)\nsns.lineplot(x=x, y=evidence_distribution, color='green')\nplt.title('Evidence')\n# Show the plot\n#plt.show()\n\n#########################################################\n\n# Create a range of values for the probability (x-axis)\nx = np.linspace(0, 1, 1000)\n\n# Calculate the prior probability distribution (prior belief)\nprior_distribution = stats.beta.pdf(x, 2, 3)  # Beta distribution parameters (2, 3)\n\n# Calculate the evidence (as a Beta distribution with parameters based on evidence probability)\nevidence_distribution = stats.beta.pdf(x, 4, 2)  # Beta distribution parameters (4, 2)\n\n# Calculate the posterior probability distribution (updated belief)\nposterior_distribution = prior_distribution * evidence_distribution\n\n# Create a Seaborn plot\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=x, y=prior_distribution, label='Prior Belief (40%)', color='blue')\nsns.lineplot(x=x, y=evidence_distribution, label='Evidence (80%)', color='green')\nsns.lineplot(x=x, y=posterior_distribution, label='Posterior Belief', color='red')\n\n# Customize the plot\nplt.title('Bayes update Representation')\nplt.xlabel('Probability')\nplt.ylabel('Density')\nplt.legend()\n\n#plt.show()"
  },
  {
    "objectID": "posts/Bayesian updating.html#the-job-interview",
    "href": "posts/Bayesian updating.html#the-job-interview",
    "title": "Bayesian updating",
    "section": "",
    "text": "Imagine you’ve landed a job interview – you’re simultaneously excited and apprehensive. As you stand at the threshold, you’re completely uncertain about what awaits you inside. You don’t know who the interviewers are, what to expect, or how the interview will unfold. The odds of everything going either terribly wrong or wonderfully right seem equally plausible.\nIn the realm of probability, we can liken your state of mind to a uniform distribution, where all outcomes are equally probable. This state of not knowing what will happen or how the interview will conclude is what Bayesian terminology dubs the “prior belief” – it’s the starting point for your assessment."
  },
  {
    "objectID": "posts/Bayesian updating.html#beliefs",
    "href": "posts/Bayesian updating.html#beliefs",
    "title": "Bayesian updating",
    "section": "",
    "text": "Interestingly, we still call it a “prior belief” even if you harbor some inkling about how the interview might unfold. For instance, let’s say you had an unpleasant preliminary conversation with the company, and as you enter the room, you’re not filled with optimism, estimating a 40% chance of success. In Bayesian terms, this, too, can be translated into probabilities.\n\n\n\n\n\nKeep in mind, though, that this assessment is occurring just as you’re about to step through the door."
  },
  {
    "objectID": "posts/Bayesian updating.html#smile",
    "href": "posts/Bayesian updating.html#smile",
    "title": "Bayesian updating",
    "section": "",
    "text": "Now, envision yourself crossing that threshold, and the interviewers pose their initial question. Your response is swift, and you notice a confident smile on the interviewer’s face. This moment represents a pivotal piece of new information drawn from your real-time experience, and naturally, you’ll use it to revise your initial belief about the interview’s outcome. We call this new information and “evidence”.\nWhat’s happening inside your mind, that surge of heightened confidence, can once again be translated into numerical probabilities to update your initial belief. This is precisely where the Bayes comes into play.\nLet’s show how the update takes place. Let’s assign to that smile, the evidence, a probability of 80% to pass the interview."
  },
  {
    "objectID": "posts/Bayesian updating.html#update",
    "href": "posts/Bayesian updating.html#update",
    "title": "Bayesian updating",
    "section": "",
    "text": "Now, let’s plug in all the information we have in the Bayesian therm we all know. We started with a not so confident prior belief (40% chance of success), then we got a new smiling evidence (80% chance of success). From here we can combine these two information and calculate our updated beliefs or as we call them “posterior beliefs”.\n\n\n\n\n\nAs you can see, now you have around 60% of passing the interview. These process can keep on going with every new information you collect during the interview as in this gif.\n\n\n\n\n\nIn essence, this is the core of Bayesian updating. It allows you to adapt your beliefs based on new information as you progress through an uncertain situation, much like navigating the twists and turns of a job interview."
  },
  {
    "objectID": "posts/The 3 + 1 pillars of data science (part 1).html",
    "href": "posts/The 3 + 1 pillars of data science (part 1).html",
    "title": "The 3 + 1 pillars of data science",
    "section": "",
    "text": "A few weeks ago, one of my students posed a compelling question:\n\n“Ahmed, what are the three core things I need to learn to become a data scientist?”\n\nMy initial response was that if I could distill it down to just three things, perhaps I wouldn’t be here. However, upon reflection, I realized that there are indeed fundamental aspects to focus on.\n\n\nUnderstanding derivatives both mathematically and conceptually serves as the bedrock of knowledge, unveiling the intricate relationships between variables. Derivatives, in essence, capture our way of perceiving the world. Whether we’re dealing with linear regression, logistic regression, or neural networks, derivatives are at the core. They signify the rate of change—a fundamental concept in data science.\nIn the realm of linear regression, we witness derivatives in action when determining the coefficients of the linear equation that best fits the data. In simple linear regression, the coefficient \\(\\beta\\) signifies the change in the dependent variable \\(y\\) for a one-unit change in the independent variable \\(x\\).\n\n\n\nα is the intercept and represents the expected value of the dependent variable when the independent variable is zero. β is the slope and indicates how much the expected value of the dependent variable changes for a one-unit change in the independent variable.\n\n\nFor multiple linear regression with multiple independent variables, we delve into the realm of partial derivatives, each \\(\\beta\\) portraying the change in the dependent variable while holding all others constant.\nAs soon as we take one step further with logistic regression, things become a bit more challenging. In the log-odds space, interpretations can still be linear, but if we aim to work with probabilities, we must acknowledge the non-linear nature of this space, characterized by the infamous sigmoid function. In this context, we rely on marginal effects to calculate the partial effects of each regressor on the probabilistic space. To be more precise, we can employ the method of finite differences (for further details, please refer to my previous blog post on the topic).\nIn neuronal networks is again all about derivatives. In the end Neuronal Networks are nothing more than multiple linear regressions stacked together with some fancy non-linear modification.\n\n\n\nA neural network with a single input layer, one output layer, and no hidden layers can essentially function as a linear regression model\n\n\n\n\n\nProbability & Uncertainty play a fundamental role in decision-making. All my students get shocked when they understand the relation between the two.\n\n\n\n\n\nProbability is a measure of the likelihood or chance of an event occurring. It is a numerical value between 0 (indicating impossibility) and 1 (indicating certainty).\nUncertainty refers to a lack of certainty or the state of not knowing the outcome of a particular event or situation. It arises when there are multiple possible outcomes, and we may not have complete information to predict which outcome will occur.\nNow, probability is used to quantify uncertainty. When we assign probabilities to different outcomes or events, we are essentially expressing our uncertainty about which of those outcomes will occur. Higher probabilities indicate a higher degree of certainty or confidence, while lower probabilities represent greater uncertainty.\nIf we take these definitions further, we can say that a probability encapsulate what we do not know about reality. In essence, probabilities and probability theory allows us to navigate the inherent uncertainties and variability in the world by providing a rigorous framework for expressing, quantifying, and reasoning about what we do not know. It is a powerful tool for making informed decisions, conducting scientific investigations, and understanding complex systems, all while acknowledging the limits of our knowledge about reality.\n\n\n\nLet’s say you’ve received a promising dataset from a client, and you’re determined to develop an outstanding machine learning model that could save them $300K per month. You have access to an enormous amount of data, and you’ve invested days in crafting an exceptionally efficient system. Everything appears to be going smoothly, and you’re ready for deployment. However, a sudden realization strikes you—some of the key variables you used to train your model are not accessible in the production environment!\n\n\n\nYes, wasted time!\n\n\nWhile this may sound trivial, it’s a mistake that many beginners fall into. They often concentrate intensely on the available data without considering how the model must perform in the real world. Yet, this oversight is a common and potentially severe pitfall in machine learning deployment. In fact, when a variable used in the training data is either unavailable or missing in the production environment, the entire system can face a catastrophic failure.\nUnderstanding the business use of a machine learning model in production is paramount to its success and effectiveness. Beyond just building a technically sound model, grasping how it aligns with the overarching business goals and requirements is essential.\n\n\n\nThe question was about three things but I feel the need of adding at least another one.\n\n\n\nIn the context of machine learning and data analysis, “spaces” typically refer to various types of mathematical or conceptual spaces that are fundamental for comprehending and solving problems. These spaces establish a framework for representing and analyzing data, features, parameters, and relationships.\nThe term “space” is ubiquitous throughout this field. You will often encounter terms such as Input Space, Output Space, Feature Space, Parameter Space, Embedding Space, Latent Space, and many more.\nIn this discussion, I’d like to take a step back and emphasize the utility of “transitioning from one space to another” in the realm of Machine Learning.\nFor instance, in the context of classic logistic regression, the log-odds space provides a linear interpretation of the effect of a variable \\(x\\) on a binary outcome \\(y\\). In neural networks, every time we apply a non-linear transformation using an activation function, we shift the output of our neuron from one space to another. This dynamic transition enables us to unearth new features and uncover fresh insights within our data.\n\n\n\nProbability space vs log-odds space\n\n\nIn summary, within the domain of machine learning and data analysis, the concept of “spaces” offers diverse perspectives and methodologies for representing and comprehending the underlying patterns and relationships within the data."
  },
  {
    "objectID": "posts/The 3 + 1 pillars of data science (part 1).html#derivatives",
    "href": "posts/The 3 + 1 pillars of data science (part 1).html#derivatives",
    "title": "The 3 + 1 pillars of data science",
    "section": "",
    "text": "Understanding derivatives both mathematically and conceptually serves as the bedrock of knowledge, unveiling the intricate relationships between variables. Derivatives, in essence, capture our way of perceiving the world. Whether we’re dealing with linear regression, logistic regression, or neural networks, derivatives are at the core. They signify the rate of change—a fundamental concept in data science.\nIn the realm of linear regression, we witness derivatives in action when determining the coefficients of the linear equation that best fits the data. In simple linear regression, the coefficient \\(\\beta\\) signifies the change in the dependent variable \\(y\\) for a one-unit change in the independent variable \\(x\\).\n\n\n\nα is the intercept and represents the expected value of the dependent variable when the independent variable is zero. β is the slope and indicates how much the expected value of the dependent variable changes for a one-unit change in the independent variable.\n\n\nFor multiple linear regression with multiple independent variables, we delve into the realm of partial derivatives, each \\(\\beta\\) portraying the change in the dependent variable while holding all others constant.\nAs soon as we take one step further with logistic regression, things become a bit more challenging. In the log-odds space, interpretations can still be linear, but if we aim to work with probabilities, we must acknowledge the non-linear nature of this space, characterized by the infamous sigmoid function. In this context, we rely on marginal effects to calculate the partial effects of each regressor on the probabilistic space. To be more precise, we can employ the method of finite differences (for further details, please refer to my previous blog post on the topic).\nIn neuronal networks is again all about derivatives. In the end Neuronal Networks are nothing more than multiple linear regressions stacked together with some fancy non-linear modification.\n\n\n\nA neural network with a single input layer, one output layer, and no hidden layers can essentially function as a linear regression model"
  },
  {
    "objectID": "posts/The 3 + 1 pillars of data science (part 1).html#probability-uncertainity",
    "href": "posts/The 3 + 1 pillars of data science (part 1).html#probability-uncertainity",
    "title": "The 3 + 1 pillars of data science",
    "section": "",
    "text": "Probability & Uncertainty play a fundamental role in decision-making. All my students get shocked when they understand the relation between the two.\n\n\n\n\n\nProbability is a measure of the likelihood or chance of an event occurring. It is a numerical value between 0 (indicating impossibility) and 1 (indicating certainty).\nUncertainty refers to a lack of certainty or the state of not knowing the outcome of a particular event or situation. It arises when there are multiple possible outcomes, and we may not have complete information to predict which outcome will occur.\nNow, probability is used to quantify uncertainty. When we assign probabilities to different outcomes or events, we are essentially expressing our uncertainty about which of those outcomes will occur. Higher probabilities indicate a higher degree of certainty or confidence, while lower probabilities represent greater uncertainty.\nIf we take these definitions further, we can say that a probability encapsulate what we do not know about reality. In essence, probabilities and probability theory allows us to navigate the inherent uncertainties and variability in the world by providing a rigorous framework for expressing, quantifying, and reasoning about what we do not know. It is a powerful tool for making informed decisions, conducting scientific investigations, and understanding complex systems, all while acknowledging the limits of our knowledge about reality."
  },
  {
    "objectID": "posts/The 3 + 1 pillars of data science (part 1).html#deployment",
    "href": "posts/The 3 + 1 pillars of data science (part 1).html#deployment",
    "title": "The 3 + 1 pillars of data science",
    "section": "",
    "text": "Let’s say you’ve received a promising dataset from a client, and you’re determined to develop an outstanding machine learning model that could save them $300K per month. You have access to an enormous amount of data, and you’ve invested days in crafting an exceptionally efficient system. Everything appears to be going smoothly, and you’re ready for deployment. However, a sudden realization strikes you—some of the key variables you used to train your model are not accessible in the production environment!\n\n\n\nYes, wasted time!\n\n\nWhile this may sound trivial, it’s a mistake that many beginners fall into. They often concentrate intensely on the available data without considering how the model must perform in the real world. Yet, this oversight is a common and potentially severe pitfall in machine learning deployment. In fact, when a variable used in the training data is either unavailable or missing in the production environment, the entire system can face a catastrophic failure.\nUnderstanding the business use of a machine learning model in production is paramount to its success and effectiveness. Beyond just building a technically sound model, grasping how it aligns with the overarching business goals and requirements is essential.\n\n\n\nThe question was about three things but I feel the need of adding at least another one."
  },
  {
    "objectID": "posts/The 3 + 1 pillars of data science (part 1).html#spaces",
    "href": "posts/The 3 + 1 pillars of data science (part 1).html#spaces",
    "title": "The 3 + 1 pillars of data science",
    "section": "",
    "text": "In the context of machine learning and data analysis, “spaces” typically refer to various types of mathematical or conceptual spaces that are fundamental for comprehending and solving problems. These spaces establish a framework for representing and analyzing data, features, parameters, and relationships.\nThe term “space” is ubiquitous throughout this field. You will often encounter terms such as Input Space, Output Space, Feature Space, Parameter Space, Embedding Space, Latent Space, and many more.\nIn this discussion, I’d like to take a step back and emphasize the utility of “transitioning from one space to another” in the realm of Machine Learning.\nFor instance, in the context of classic logistic regression, the log-odds space provides a linear interpretation of the effect of a variable \\(x\\) on a binary outcome \\(y\\). In neural networks, every time we apply a non-linear transformation using an activation function, we shift the output of our neuron from one space to another. This dynamic transition enables us to unearth new features and uncover fresh insights within our data.\n\n\n\nProbability space vs log-odds space\n\n\nIn summary, within the domain of machine learning and data analysis, the concept of “spaces” offers diverse perspectives and methodologies for representing and comprehending the underlying patterns and relationships within the data."
  },
  {
    "objectID": "posts/The 3 + 1 pillars of data science (part 1).html#probability-uncertainty",
    "href": "posts/The 3 + 1 pillars of data science (part 1).html#probability-uncertainty",
    "title": "The 3 + 1 pillars of data science",
    "section": "",
    "text": "Probability & Uncertainty play a fundamental role in decision-making. All my students get shocked when they understand the relation between the two.\n\n\n\n\n\nProbability is a measure of the likelihood or chance of an event occurring. It is a numerical value between 0 (indicating impossibility) and 1 (indicating certainty).\nUncertainty refers to a lack of certainty or the state of not knowing the outcome of a particular event or situation. It arises when there are multiple possible outcomes, and we may not have complete information to predict which outcome will occur.\nNow, probability is used to quantify uncertainty. When we assign probabilities to different outcomes or events, we are essentially expressing our uncertainty about which of those outcomes will occur. Higher probabilities indicate a higher degree of certainty or confidence, while lower probabilities represent greater uncertainty.\nIf we take these definitions further, we can say that a probability encapsulate what we do not know about reality. In essence, probabilities and probability theory allows us to navigate the inherent uncertainties and variability in the world by providing a rigorous framework for expressing, quantifying, and reasoning about what we do not know. It is a powerful tool for making informed decisions, conducting scientific investigations, and understanding complex systems, all while acknowledging the limits of our knowledge about reality."
  },
  {
    "objectID": "posts/The Savitzky–Golay Algorithm.html",
    "href": "posts/The Savitzky–Golay Algorithm.html",
    "title": "The Savitzky–Golay Algorithm",
    "section": "",
    "text": "Time series data often necessitates extensive processing and transformations to enhance its usability and interpretability. In this article, I’ll delve into one of my preferred smoothing techniques - the Savitzky–Golay algorithm. This remarkable method has consistently proven its effectiveness in a variety of applications, ranging from satellite data analysis to financial market forecasting. Initially, I will provide a comprehensive overview of the algorithm, delving into its mathematical underpinnings and various adjustable parameters. Subsequently, we’ll explore the concept of noise in time series data and demonstrate how smoothing, specifically using the Savitzky–Golay method, can mitigate these effects. Lastly, I’ll conduct a comparative analysis, showcasing how Savitzky–Golay outperforms the traditional Moving Average (MA) approach when applied to financial time series data, with a focus on Google stock prices.\nNow, let’s dive into each of these aspects, starting with a paragraph on the history and mathematical details of the Savitzky–Golay algorithm, accompanied by relevant Python code and plots.\n\n\nThe Savitzky–Golay algorithm, developed by Abraham Savitzky and Marcel J. E. Golay in the 1960s, is a robust method for smoothing noisy data while preserving essential features. It achieves this by fitting a low-degree polynomial to a small, moving window of data points and using polynomial coefficients for smoothing. Let’s explore its mathematical foundation and various parameters through Python code and illustrative plots.\n# Python code for Savitzky–Golay smoothing\nimport numpy as np\nfrom scipy.signal import savgol_filter\nimport matplotlib.pyplot as plt\n\n# Generate synthetic noisy data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.1, 100)\n\n# Apply Savitzky–Golay smoothing\nsmoothed = savgol_filter(y, window_length=11, polyorder=2)\n\n# Plot original and smoothed data\nplt.figure(figsize=(10, 5))\nplt.plot(x, y, label='Noisy Data', alpha=0.7)\nplt.plot(x, smoothed, label='Savitzky–Golay Smoothing', color='orange')\nplt.legend()\nplt.title('Savitzky–Golay Smoothing Example')\nplt.show()\nIn the above code, we generate noisy data, apply Savitzky–Golay smoothing with specified window length and polynomial order, and visualize the results.\nA well known complication is at the edges of the time series where fewer points are available. To handle this there are several strategy\nxxxxx\n\n\n\nTime series data often contains inherent noise, making it challenging to extract meaningful patterns. To illustrate the effectiveness of the Savitzky–Golay algorithm in mitigating noise, let’s consider satellite temperature data as an example. We’ll apply the algorithm to this data and visualize the noise reduction.\n(Include relevant Python code and plots for this section, demonstrating the noise reduction on satellite temperature data)\nFinally, let’s compare the Savitzky–Golay method with the classic Moving Average (MA) approach when applied to financial time series data, specifically Google stock prices.\n\n\n\nFinancial time series data, like stock prices, often exhibit complex fluctuations. In this section, we’ll compare the performance of the Savitzky–Golay algorithm against the conventional Moving Average method when smoothing Google stock prices. By doing so, we’ll highlight the advantages of using Savitzky–Golay for financial data analysis.\n(Include Python code for both methods, a comparison plot, and a brief discussion of the results)\nBy addressing these aspects, we’ll gain a comprehensive understanding of the Savitzky–Golay algorithm’s versatility and its practical applications in enhancing time series data analysis."
  },
  {
    "objectID": "posts/The Savitzky–Golay Algorithm.html#history-and-mathematical-details-of-savitzkygolay-algorithm",
    "href": "posts/The Savitzky–Golay Algorithm.html#history-and-mathematical-details-of-savitzkygolay-algorithm",
    "title": "The Savitzky–Golay Algorithm",
    "section": "",
    "text": "The Savitzky–Golay algorithm, developed by Abraham Savitzky and Marcel J. E. Golay in the 1960s, is a robust method for smoothing noisy data while preserving essential features. It achieves this by fitting a low-degree polynomial to a small, moving window of data points and using polynomial coefficients for smoothing. Let’s explore its mathematical foundation and various parameters through Python code and illustrative plots.\n# Python code for Savitzky–Golay smoothing\nimport numpy as np\nfrom scipy.signal import savgol_filter\nimport matplotlib.pyplot as plt\n\n# Generate synthetic noisy data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.1, 100)\n\n# Apply Savitzky–Golay smoothing\nsmoothed = savgol_filter(y, window_length=11, polyorder=2)\n\n# Plot original and smoothed data\nplt.figure(figsize=(10, 5))\nplt.plot(x, y, label='Noisy Data', alpha=0.7)\nplt.plot(x, smoothed, label='Savitzky–Golay Smoothing', color='orange')\nplt.legend()\nplt.title('Savitzky–Golay Smoothing Example')\nplt.show()\nIn the above code, we generate noisy data, apply Savitzky–Golay smoothing with specified window length and polynomial order, and visualize the results.\nA well known complication is at the edges of the time series where fewer points are available. To handle this there are several strategy\nxxxxx"
  },
  {
    "objectID": "posts/The Savitzky–Golay Algorithm.html#processsing-temperature-data-from-the-sky",
    "href": "posts/The Savitzky–Golay Algorithm.html#processsing-temperature-data-from-the-sky",
    "title": "The Savitzky–Golay Algorithm",
    "section": "",
    "text": "Time series data often contains inherent noise, making it challenging to extract meaningful patterns. To illustrate the effectiveness of the Savitzky–Golay algorithm in mitigating noise, let’s consider satellite temperature data as an example. We’ll apply the algorithm to this data and visualize the noise reduction.\n(Include relevant Python code and plots for this section, demonstrating the noise reduction on satellite temperature data)\nFinally, let’s compare the Savitzky–Golay method with the classic Moving Average (MA) approach when applied to financial time series data, specifically Google stock prices."
  },
  {
    "objectID": "index.html#languages-and-tools",
    "href": "index.html#languages-and-tools",
    "title": "Ahmed T. Hammad",
    "section": "",
    "text": "For curious visitors, the name of the website is just a tentative to mix the initial letter of my full name and statistics. By chance, I ended up with something that has an actual meaning in Arabic 🤷. Athsas (أتحسس) means “to feel something” or a sensation given by an object or material when touched."
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Ahmed T. Hammad",
    "section": "",
    "text": "With a focus on environmental analysis, I provide Monitoring and Evaluation (M&E) services tailored solutions for ongoing projects and policies, employing advanced machine learning algorithms designed for real-time stream data processing. From assessing the impact of cultivation approaches to evaluating the effectiveness of air pollution control programs, my M&E expertise provides valuable insights for informed decision-making. In forecasting, I utilize state-of-the-art algorithms to predict environmental events, adapting to changes over time and delivering accurate alerts. Additionally, my competencies extend to building adaptive and environmentally friendly learning systems, emphasizing portability, low resource consumption, and a serverless architecture. Whether it’s periodic data collection or continuous monitoring of environmental changes, my learning systems excel in extracting insights while prioritizing sustainability and versatility across various data sources. Get in touch if you want to know more\n\n\n\n\nFor curious visitors, the name of the website is just a tentative to mix the initial letter of my full name and statistics. By chance, I ended up with something that has an actual meaning in Arabic 🤷. Athsas (أتحسس) means “to feel something” or a sensation given by an object or material when touched."
  },
  {
    "objectID": "solutions.html",
    "href": "solutions.html",
    "title": "💡Solutions",
    "section": "",
    "text": "With a focus on environmental analysis, I provide Monitoring and Evaluation (M&E) tailored solutions for ongoing projects and policies, employing advanced machine learning algorithms designed for real-time stream data processing. From assessing the impact of cultivation approaches to evaluating the effectiveness of air pollution control programs, my M&E expertise provides valuable insights for informed decision-making. In forecasting, I utilize state-of-the-art algorithms to predict environmental events, adapting to changes over time and delivering accurate alerts. Additionally, my competencies extend to building adaptive and environmentally friendly learning systems, emphasizing portability, low resource consumption, and a serverless architecture."
  },
  {
    "objectID": "solutions.html#forecasting-services",
    "href": "solutions.html#forecasting-services",
    "title": "💡Solutions",
    "section": "Forecasting Services",
    "text": "Forecasting Services\n\n\nMy forecasting services provide a cutting-edge solution for predicting environmental events. Whether it’s anticipating floods or other critical occurrences, we utilize state-of-the-art machine learning algorithms to continuously analyze data from weather stations, sensors, and other sources. The adaptability of my forecasting approach allows to adjust to changes over time, ensuring accurate and timely alerts. By forecasting multiple future points with each data update, my services provide a dynamic and reliable solution to environmental challenges. I specialize in creating forecasting models that are not only accurate but also adaptable to evolving data relationships."
  },
  {
    "objectID": "solutions.html#building-adaptive-and-environmentally-friendly-learning-systems",
    "href": "solutions.html#building-adaptive-and-environmentally-friendly-learning-systems",
    "title": "💡Solutions",
    "section": "Building Adaptive and Environmentally Friendly Learning Systems:",
    "text": "Building Adaptive and Environmentally Friendly Learning Systems:\n\nMy expertise extends to the development of adaptive and environmentally friendly learning systems. I focus on creating systems that are portable, lightweight, and have a minimal environmental footprint. By integrating seamlessly with various data sources, including APIs, databases, and sensors, my systems are versatile and applicable to a range of scenarios. Whether it’s periodic data collection, like an Air Pollution Survey, or continuous monitoring of environmental changes, my learning systems excel in extracting valuable insights. My commitment to sustainability is reflected in the low resource consumption and serverless architecture of our systems, contributing to environmentally conscious practices in the field of machine learning."
  },
  {
    "objectID": "solutions.html#monitoring-and-evaluation-services",
    "href": "solutions.html#monitoring-and-evaluation-services",
    "title": "💡Solutions",
    "section": "Monitoring and Evaluation Services",
    "text": "Monitoring and Evaluation Services\n\n\nThe Monitoring and Evaluation (M&E) services offer a tailored approach to environmental analysis. With years of research and collaboration with diverse stakeholders such as NGOs, governmental bodies, universities, and program evaluators, I can provide comprehensive M&E solutions. My expertise lies in continuous monitoring of environmental projects and policies. I use advanced machine learning algorithms designed for real-time stream data processing, enabling the evaluation of ongoing initiatives. From assessing the impact of new cultivation approaches to the effectiveness of air pollution control programs, my M&E services deliver valuable insights for informed decision-making."
  },
  {
    "objectID": "solutions.html#adaptive-and-environmentally-friendly-learning-systems",
    "href": "solutions.html#adaptive-and-environmentally-friendly-learning-systems",
    "title": "💡Solutions",
    "section": "Adaptive and Environmentally Friendly Learning Systems",
    "text": "Adaptive and Environmentally Friendly Learning Systems\n\nMy expertise extends to the development of adaptive and environmentally friendly learning systems. I focus on creating systems that are portable, lightweight, and have a minimal environmental footprint. By integrating seamlessly with various data sources, including APIs, databases, and sensors, my systems are versatile and applicable to a range of scenarios. Whether it’s periodic data collection, like an Air Pollution Survey, or continuous monitoring of environmental changes, my learning systems excel in extracting valuable insights. My commitment to sustainability is reflected in the low resource consumption and serverless architecture of our systems, contributing to environmentally conscious practices in the field of machine learning."
  },
  {
    "objectID": "posts/Data Science Books.html#an-introduction-to-statistical-learning-by-gareth-james-daniela-witten-trevor-hastie-robert-tibshirani-jonathan-taylor",
    "href": "posts/Data Science Books.html#an-introduction-to-statistical-learning-by-gareth-james-daniela-witten-trevor-hastie-robert-tibshirani-jonathan-taylor",
    "title": "Data Science Books",
    "section": "",
    "text": "“An Introduction to Statistical Learning” by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor is a comprehensive textbook that provides an introduction to statistical learning methods. It covers key concepts and techniques in statistical learning, offering a balanced blend of theory and practical applications. Notably, the book is accessible to users of both R and Python programming languages, making it versatile for a wide range of readers.\nThe book begins by introducing fundamental concepts in statistical learning. It then covers topics such as linear regression, classification methods, resampling techniques, model selection, and regularization. Nonlinear models, tree-based methods, support vector machines, and unsupervised learning methods are also discussed.\nA distinctive feature of the book is its emphasis on practical applications and the statistical nature of Machine Learning, drawing important parallelisms between the classic vocabulary used in statistics and the the Machine Learning jargon. It includes case studies and examples throughout, illustrating how statistical learning methods can be applied to real-world problems. The inclusion of R and Python code examples further enhances its utility for readers using either programming language.\nIn summary, “An Introduction to Statistical Learning” is a valuable resource for students, researchers, and practitioners seeking a solid foundation in statistical learning. Its flexibility in catering to both R and Python users makes it an inclusive and versatile choice for individuals interested in applying statistical learning techniques to data analysis and modeling."
  },
  {
    "objectID": "posts/Probability Box with Kernel Density Estimation.html",
    "href": "posts/Probability Box with Kernel Density Estimation.html",
    "title": "Probability Box with Kernel Density Estimation",
    "section": "",
    "text": "all those numbers in the weather forecast got me thinking about a simple table with historical data for temperature, humidity, and rain. Then, my interest in playing with probabilities kicked in (it tends to do that a lot). I decided to turn this data into something fun, so I called it a “probability box.”\nNow, what’s a probability box? It’s just a fancy name for turning a bunch of data points into probabilities. Once I’ve done that, it’s like having a cool tool to play with. I can ask it about Cumulative Probabilities, which basically tells me the chances of two, three, or more things happening together.\nThere are lots of ways to make a Probability Box, but I like keeping it simple. So, I’m using something called Kernel Density Estimation.\nLet’s make it less complicated with an example:\n\nlibrary(ks)\nlibrary(data.table)\nlibrary(plotly)\n\n\n# Generating Random Data with Three Variables\nset.seed(1)\nn &lt;- 1000\ntemp &lt;- rnorm(n,mean=25,sd=2.47)\nhum &lt;- rnorm(n,mean=15,sd=0.2)\nrain &lt;- rnorm(n,mean=3.46,sd=4.07)\nxyz &lt;- cbind(temp, hum, rain)\n\nHere we have our fictional variables representing temperature in Celsius, rain in millimeters in and humidity as a percentage.\nKernel Density Estimation (KDE) is a non-parametric statistical method used for estimating the probability density function (PDF) of a continuous random variable. It provides a smooth representation of the underlying distribution of data points, helping to visualize and analyze the data’s probability distribution.\nDefining a grid for KDE is essential. The grid consists of points in the feature space where the estimated density is calculated, allowing for smooth visualization and integration to compute probabilities over specific regions. The grid’s resolution influences the precision of the KDE estimate, with a finer grid capturing more details but demanding greater computational resources. In essence, the grid serves as the canvas upon which the continuous probability density function is constructed, enabling a comprehensive exploration of the data’s distribution.\n\n# Setting up Grid for KDE\ntemp_min &lt;- min(temp)\ntemp_max &lt;- max(temp)\ncx &lt;- 0.1\n\nhum_min &lt;- min(hum)\nhum_max &lt;-max(hum)\ncy &lt;- 0.1\n\nrain_min &lt;- min(rain)\nrain_max &lt;- max(rain)\ncz &lt;- 0.1\n\npts.x &lt;- seq(temp_min, temp_max, cx)\npts.y &lt;- seq(hum_min, hum_max, cy)\npts.z &lt;- seq(rain_min, rain_max, cz)\n\npts &lt;- expand.grid(temp = pts.x, hum = pts.y, rain = pts.z)\n\nTime to run KDE!\n\n# Performing KDE\nf_kde &lt;- kde(xyz, eval.points = pts)\n\nBefore diving into the fun stuff, it’s important to make sure everything’s on track. Verifying KDE integration is a key step. This check ensures that the estimated probability density function follows the rules of probability theory. The integral of the KDE should add up to 1 across the entire variable range, reflecting a proper probability distribution where all possibilities together make a whole.\n\n# Assigning KDE Estimates to Data Frame\npts$est &lt;- f_kde$estimate\n\n# Checking KDE Integration\nintegration_check &lt;- sum(pts$est) * cx * cy * cz\n\n# Displaying the result of the integration check\ncat(\"Integration Check Result:\", integration_check, \"\\n\")\n\nIntegration Check Result: 0.9945768 \n\n\nGreat, we’re all set! Now, let’s have some fun with the data.\nTo find the cumulative probability over a specific area, we simply add up the estimated densities multiplied by the area of each corresponding grid cell. For instance, let’s say we want to know the probability of the temperature being above 17, humidity below 20%, and rain below 5 mm. Answering this is a breeze – just filter the data-frame based on the conditions temp &gt;17 & hum &lt;20 & rain &lt;5. Then, calculate the cumulative probability using the joint KDE estimates and grid spacing. This code snippet does the trick.\n\n# Querying Cumulative Probability in a Specific Area\n\nsetDT(pts)\ncumulative_prob_area &lt;- pts[temp &gt;17 & hum &lt;20  & rain &lt;5, .(pkde = sum(est) * cx * cy * cz)]\n\ncat(\"Cumulative Probability in Area:\", cumulative_prob_area$pkde, \"\\n\")\n\nCumulative Probability in Area: 0.6370082 \n\n\nWhat about the most probable combination of values? We just need find the point with the maximum density.\n\n# Find the index of the maximum density\nmax_density_index &lt;-which.max(f_kde$estimate)\n\n# Extract the corresponding combination of values from the grid\nprint(pts[max_density_index, c(\"temp\", \"hum\", \"rain\")], row.names = FALSE)\n\n     temp      hum     rain\n 24.47012 14.94936 3.453885\n\n\nLooks fun. We could even plot the cumulative probability over the full space and just look at the different regions of the 3D box.\nOur current method is basic, assuming that rain, temperature, and humidity are unrelated. If we wanted to consider their potential connections, we’d delve into more complex copula-based approaches. But here, our data was generated independently, so it’s not an issue with inter-variable dependencies.\nAlso kernel density estimators can perform poorly when estimating tail behavior, that is when we are looking at extreme values, and can over-emphasise bumps in the density for heavy tailed data. Again here copula-based aproaches could be of great help.\nKeeping it simple helps us explore the data without getting stuck in the box."
  },
  {
    "objectID": "slides.html#some-of-sample-slides-i-have-been-using-here-and-there-to-teach-different-topics",
    "href": "slides.html#some-of-sample-slides-i-have-been-using-here-and-there-to-teach-different-topics",
    "title": "📽️️Slides",
    "section": "",
    "text": "Data Management in Python. Here I go trough the XXX Data Management with Python and Pandas\n\n\nStatistics & Probabilities. All you need to know to get your started with Statistics & Probabilities\n\n\nIntro to ML. History and application of Machine Learning with Python\n\n\nLinear Regression. Running, intepreting and using the most famous algorithm ever!\n\n\nEnsemble Methods. Improve accuracy of results in models by combining multiple models instead of using a single model. All you need to know about Trees algorithms and more."
  },
  {
    "objectID": "slides.html#some-slides-i-have-been-using-here-and-there-to-teach-different-topics",
    "href": "slides.html#some-slides-i-have-been-using-here-and-there-to-teach-different-topics",
    "title": "📽️️ Slides",
    "section": "",
    "text": "Data Management in Python. Here I showcasing how Pandas empowers us to efficiently handle, analyze, and manipulate datasets\n\n\nStatistics & Probabilities. All you need to know to get your started with Statistics & Probabilities\n\n\nIntro to ML. History and application of Machine Learning with Python\n\n\nLinear Regression. Running, intepreting and using the most famous algorithm ever!\n\n\nEnsemble Methods. Improve accuracy of results in models by combining multiple models instead of using a single model. All you need to know about Trees algorithms and more.\n\nModel Tuning. Adjusting a machine learning model involves tweaking specific settings, like control knobs, to help it learn better."
  },
  {
    "objectID": "posts/Machine Learning, Copula and Synthetic Data.html",
    "href": "posts/Machine Learning, Copula and Synthetic Data.html",
    "title": "Machine Learning, Copula and Synthetic Data",
    "section": "",
    "text": "Copulas and synthetic data play pivotal roles in statistical modeling, offering innovative solutions for various challenges in Machine Learning. Here, I will focus into the use of copulas for synthetic data generation.\nCopulas are mathematical constructs used to model the dependence structure between random variables. Unlike traditional correlation measures, copulas separate the marginal distributions from the dependence structure, providing a more flexible and nuanced approach to capturing complex relationships. They are particularly useful in scenarios where traditional models might fail to capture the intricate dependencies between variables. In the specific case of syntetic data generation, what we need is to mimics the statistical properties of real-world data. But why do we need this “fake” data in the first place? Syntetic data are invaluable in scenarios where obtaining sufficient real data is challenging (small sample size) or when privacy concerns limit access to actual data. By creating synthetic datasets we can augment the available data, facilitating better model generalization and robustness.\nGoing back on the statistical properties we mentioned earlier, we are interested in the parameters governing the distribution of each variable separately (the marginals) and the dependency structure between them (the copula). Once these are known, we can generate new data from the same distribution and with the same correlation.\nTo give a simple example let’s take few variables from the classic Cars Dataset.\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom copulas.multivariate import GaussianMultivariate\nfrom copulas.univariate import ParametricType, Univariate\n\ndf = sns.load_dataset(\"mpg\")\ndf=df.drop(columns=['origin', 'name'])\ndf=df.dropna()\ndf.columns\n\ndf=df[['horsepower', 'weight','acceleration','mpg']]\ndf.describe()\n\n\n\n\n\n\n\n\nhorsepower\nweight\nacceleration\nmpg\n\n\n\n\ncount\n392.000000\n392.000000\n392.000000\n392.000000\n\n\nmean\n104.469388\n2977.584184\n15.541327\n23.445918\n\n\nstd\n38.491160\n849.402560\n2.758864\n7.805007\n\n\nmin\n46.000000\n1613.000000\n8.000000\n9.000000\n\n\n25%\n75.000000\n2225.250000\n13.775000\n17.000000\n\n\n50%\n93.500000\n2803.500000\n15.500000\n22.750000\n\n\n75%\n126.000000\n3614.750000\n17.025000\n29.000000\n\n\nmax\n230.000000\n5140.000000\n24.800000\n46.600000\n\n\n\n\n\n\n\nLet’s plot the kernel density distribution of 3 variables, the scatter plot of each pair and the corresponding correlation.\n\ndef corrdot(*args, **kwargs):\n    corr_r = args[0].corr(args[1], 'pearson')\n    corr_text = f\"{corr_r:2.2f}\".replace(\"0.\", \".\")\n    ax = plt.gca()\n    ax.set_axis_off()\n    marker_size = abs(corr_r) * 1000\n    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap=\"coolwarm\",\n               vmin=-1, vmax=1, transform=ax.transAxes)\n    font_size = abs(corr_r) * 10 + 5\n    ax.annotate(corr_text, [.5, .5,],  xycoords=\"axes fraction\",\n                ha='center', va='center', fontsize=font_size)\n\nsns.set(style='white', font_scale=1)\ng = sns.PairGrid(df[['horsepower', 'weight','acceleration']], aspect=1, diag_sharey=False)\ng.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})\ng.map_diag(sns.distplot, kde_kws={'color': 'black'})\ng.map_upper(corrdot)\nplt.show()\n\n\n\n\nWe can see all sorts of things here. Aside from the strong correlation among some of the variables, we see that they have different distribution. For example, acceleration is very normal distributed but the same cannot be said about the other two variables.\nBefore anything else, let’s try a simple model to predict mpg.\n\ny = df.pop('mpg')\nX = df\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,random_state=42)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nprint(model.score(X_test, y_test))\n\n0.6501833421053663\n\n\nNow, can we simulate something so similar to the actual data that we would get the same score? Yes, we can thanks to copulas!!! We ca generate a synthetic dataset with the same underlying structure.\n\n# Select the best PARAMETRIC univariate (no KDE)\nunivariate = Univariate(parametric=ParametricType.PARAMETRIC)\n\n\ndef create_synthetic(X, y):\n    \"\"\"\n    This function combines X and y into a single dataset D, models it\n    using a Gaussian copula, and generates a synthetic dataset S. It\n    returns the new, synthetic versions of X and y.\n    \"\"\"\n    dataset = np.concatenate([X, np.expand_dims(y, 1)], axis=1)\n\n    distribs =  GaussianMultivariate(distribution=univariate)\n    distribs.fit(dataset)\n\n    synthetic = distribs.sample(len(dataset))\n\n    X = synthetic.values[:, :-1]\n    y = synthetic.values[:, -1]\n\n    return X, y, distribs\n\nX_synthetic, y_synthetic, dist= create_synthetic(X_train, y_train)\n\nLet’s look at the individual distributions fitted by the algorithm.\n\nparameters = dist.to_dict()\nparameters['univariates']\n\n[{'a': 2.505456580509649,\n  'loc': 44.28600428264269,\n  'scale': 24.186079118156652,\n  'type': 'copulas.univariate.gamma.GammaUnivariate'},\n {'loc': 1604.6365783320787,\n  'scale': 3779.0567202878065,\n  'a': 1.4708615880361795,\n  'b': 2.5202670202239155,\n  'type': 'copulas.univariate.beta.BetaUnivariate'},\n {'loc': 1.0632403337723968,\n  'scale': 73.46060125357005,\n  'a': 20.470245065728314,\n  'b': 83.5439968070011,\n  'type': 'copulas.univariate.beta.BetaUnivariate'},\n {'loc': 9.84592394053172,\n  'scale': 40.487634662917245,\n  'a': 1.6832256330594189,\n  'b': 3.2317290395770817,\n  'type': 'copulas.univariate.beta.BetaUnivariate'}]\n\n\nWe see that the distributions (Gamma and Beta), and their corresponding parameters like location and scale. We can also take a look at the correlation that defines the join distribution.\n\nparameters['correlation']\n\n[[1.0, 0.8473079532001859, -0.7200908747599617, -0.8315209336313284],\n [0.8473079532001859, 1.0, -0.42047354940020193, -0.831115031494809],\n [-0.7200908747599617, -0.42047354940020193, 1.0, 0.43579734522625596],\n [-0.8315209336313284, -0.831115031494809, 0.43579734522625596, 1.0]]\n\n\nNow it is time to look at all the synthetic variables and compare them with the original one. Let’s look at the same things. A summary of the dataset and the plot of the 3 variables.\n\nsyntDF=pd.DataFrame(np.concatenate([X_synthetic, np.expand_dims(y_synthetic, 1)], axis=1),columns=['horsepower', 'weight', 'acceleration','mpg'])\n\nsyntDF.describe()\n\n\n\n\n\n\n\n\nhorsepower\nweight\nacceleration\nmpg\n\n\n\n\ncount\n274.000000\n274.000000\n274.000000\n274.000000\n\n\nmean\n103.393953\n3007.334850\n15.565402\n24.093801\n\n\nstd\n36.227720\n822.287724\n2.765889\n8.281429\n\n\nmin\n49.084379\n1650.853730\n8.469497\n10.183055\n\n\n25%\n76.679656\n2299.624685\n13.538932\n17.291204\n\n\n50%\n95.181303\n2933.743847\n15.416592\n22.956229\n\n\n75%\n119.586826\n3537.785385\n17.370376\n30.233671\n\n\nmax\n227.592858\n5142.078155\n23.603403\n45.263817\n\n\n\n\n\n\n\nThe descriptive statistics are remarkably similar, reflecting the statistical properties emphasized earlier. This holds significant importance in our analysis. However, a closer examination of individual variable distributions and their correlations reveals disparities. The Kernel distributions have evidently undergone changes, and although the correlation values remain within the same magnitude range and exhibit the same sign, they are not identical.\n\ng = sns.PairGrid(syntDF[['horsepower', 'weight','acceleration']], aspect=1, diag_sharey=False)\ng.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})\ng.map_diag(sns.distplot, kde_kws={'color': 'black'})\ng.map_upper(corrdot)\nplt.show()\n\n\n\n\nNow that we have seen similarities and differences, let’s try to run the same simple linear model on the synthetic data.\n\nmodel = LinearRegression()\nmodel.fit(X_synthetic, y_synthetic)\n\nprint(model.score(X_test, y_test))\n\n0.6068245805913557\n\n\nUpon scrutinizing the results, it is evident that they are highly comparable, even with the constraint of limiting the possible distributions to the simplest univariate forms and utilizing only three variables. This implies that our Gaussian copula has effectively captured the critical statistical characteristics of the dataset essential for addressing the regression problem."
  },
  {
    "objectID": "solutions.html#risk-and-environmental-analyses",
    "href": "solutions.html#risk-and-environmental-analyses",
    "title": "💡Solutions",
    "section": "Risk and Environmental analyses",
    "text": "Risk and Environmental analyses\n\n\nI conduct in-depth risk and environmental analyses, utilizing advanced modeling techniques. Through the application of probabilistic approaches, my goal is to pinpoint potential hazards and quantify their probabilities. This process enables to unveil intricate relationships among complex variables, fostering a comprehensive understanding of the risk landscape."
  },
  {
    "objectID": "Toolkits.html",
    "href": "Toolkits.html",
    "title": "Tolkits🧰",
    "section": "",
    "text": "SyntCF\n\n\nThe syntCF R package is a powerful tool designed for estimating the impact of programs or policies through a robust time series synthetic counterfactual approach, leveraging the double difference estimator within a Machine Learning framework. Inspired by contributions from various studies, the package employs a modified version of the robust random forest algorithm, considering the time dependency of data through block bootstrapping. To address uncertainty in predicted counterfactual time series, quantile regression forest is used.\n\n\n\nAn AI Geospatial-assisted decision support framework\n\n\n3Vs (Vulnerability, Vaccination, and Values), is a framework devised to optimize the allocation of COVID-19 vaccines in settings characterized by resource constraints. The framework integrates processes of data collection, AI-facilitated vulnerability assessment, and vaccination prioritization, all aligned with the principles delineated by the World Health Organization (WHO). Noteworthy components encompass the compilation of socio-demographic and environmental data, AI-driven expert elicitation for prioritization, and geospatially informed vaccine allocation with due regard to equity values. The framework’s empirical application in Kenya serves as a testament to its potential to surpass prevailing strategies, demonstrating adeptness in the judicious dispensation of vaccines to socially vulnerable demographics. Adaptable to diverse health emergencies, the 3Vs framework emerges as an invaluable instrument for decision-makers navigating challenges within resource-constrained environments.\n\n\n\npdata.table\n\npdata.table is a powerful tool designed for risk assessment and management. it is an advanced statistical toolkit that excels in exploring probability distributions within a given dataset. The tool offers a method to encapsulate and query the probability space effortlessly. Its distinctive feature lies in the ease with which users can navigate and analyze marginal, joint, and conditional probabilities while taking into account the underlying correlation structure inherent in the data. This unique capability empowers users to delve into intricate relationships and dependencies within datasets, furnishing a solid foundation for making well-informed decisions in the context of risk management scenarios."
  },
  {
    "objectID": "toolkits.html",
    "href": "toolkits.html",
    "title": "🧰Toolkits",
    "section": "",
    "text": "SyntCF\n\n\nThe syntCF R package is a powerful tool designed for estimating the impact of programs or policies through a robust time series synthetic counterfactual approach, leveraging the double difference estimator within a Machine Learning framework. Inspired by contributions from various studies, the package employs a modified version of the robust random forest algorithm, considering the time dependency of data through block bootstrapping. To address uncertainty in predicted counterfactual time series, quantile regression forest is used.\n\n\n\nPBOX\n\n\nThe pbox R package is designed for risk assessment and management. It is an advanced statistical library that excels in exploring probability distributions within a given dataset. The tool offers a method to encapsulate and query the probability space effortlessly. Its distinctive feature lies in the ease with which users can navigate and analyze marginal, joint, and conditional probabilities while taking into account the underlying correlation structure inherent in the data. This unique capability empowers users to delve into intricate relationships and dependencies within datasets, providing a solid foundation for making well-informed decisions in the context of risk management scenarios. With pbox is straightforward to answer questions like:\n\nWhat is the probability of experiencing extreme heat waves in Indonesia with temperatures above 32 degrees?\nWhat is the probability of simultaneous extreme heat waves in Vietnam with temperatures above than 31 degrees and the average regional temperature being above than 26 degrees?\nGiven that the average regional temperature is 26 degrees, what is the probability of experiencing extreme heat waves in both Vietnam and Indonesia with temperatures above 33 degrees?\n\n\n\n\nAn AI Geospatial-assisted decision support framework\n\n\n3Vs (Vulnerability, Vaccination, and Values), is a framework devised to optimize the allocation of COVID-19 vaccines in settings characterized by resource constraints. The framework integrates processes of data collection, AI-facilitated vulnerability assessment, and vaccination prioritization, all aligned with the principles delineated by the World Health Organization (WHO). Noteworthy components encompass the compilation of socio-demographic and environmental data, AI-driven expert elicitation for prioritization, and geospatially informed vaccine allocation with due regard to equity values. The framework’s empirical application in Kenya serves as a testament to its potential to surpass prevailing strategies, demonstrating adeptness in the judicious dispensation of vaccines to socially vulnerable demographics. Adaptable to diverse health emergencies, the 3Vs framework emerges as an invaluable instrument for decision-makers navigating challenges within resource-constrained environments."
  },
  {
    "objectID": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#video",
    "href": "posts/Embracing Change: Incremental vs. Batch Machine Learning.html#video",
    "title": "Embracing Change: Incremental vs. Batch Machine Learning",
    "section": "",
    "text": "If you’re interested in delving deeper into the subject, I’ve created a comprehensive one-hour video introduction on the topic."
  }
]